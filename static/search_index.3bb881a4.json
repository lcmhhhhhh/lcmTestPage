[{"id":0,"title":"Uninstalling the CLI","content":"#\n\nYour uninstall method depends on how you ran the CLI. Follow the instructions\nfor either npx or a global npm installation.\n\n\nMethod 1: Using npx#\n\nnpx runs packages from a temporary cache without a permanent installation. To\n\"uninstall\" the CLI, you must clear this cache, which will remove gemini-cli and\nany other packages previously executed with npx.\n\nThe npx cache is a directory named _npx inside your main npm cache folder. You\ncan find your npm cache path by running npm config get cache.\n\nFor macOS / Linux\n\n\n\nFor Windows\n\nCommand Prompt\n\n\n\nPowerShell\n\n\n\n\nMethod 2: Using npm (Global Install)#\n\nIf you installed the CLI globally (e.g., npm install -g @vecode-cli/vecode-cli),\nuse the npm uninstall command with the -g flag to remove it.\n\n\n\nThis command completely removes the package from your system.","routePath":"/Uninstall","lang":"","toc":[{"text":"Method 1: Using npx","id":"method-1-using-npx","depth":2,"charIndex":127},{"text":"Method 2: Using npm (Global Install)","id":"method-2-using-npm-global-install","depth":2,"charIndex":567}],"domain":"","frontmatter":{},"version":""},{"id":1,"title":"VeCLI Architecture Overview","content":"#\n\nThis document provides a high-level overview of the VeCLI's architecture.\n\n\nCore components#\n\nThe VeCLI is primarily composed of two main packages, along with a suite of\ntools that can be used by the system in the course of handling command-line\ninput:\n\n 1. CLI package (packages/cli):\n    \n    * Purpose: This contains the user-facing portion of the VeCLI, such as\n      handling the initial user input, presenting the final output, and managing\n      the overall user experience.\n    * Key functions contained in the package:\n      * Input processing\n      * History management\n      * Display rendering\n      * Theme and UI customization\n      * CLI configuration settings\n\n 2. Core package (packages/core):\n    \n    * Purpose: This acts as the backend for the VeCLI. It receives requests sent\n      from packages/cli, orchestrates interactions with the Volcano Engine API,\n      and manages the execution of available tools.\n    * Key functions contained in the package:\n      * API client for communicating with the Volcano Engine API\n      * Prompt construction and management\n      * Tool registration and execution logic\n      * State management for conversations or sessions\n      * Server-side configuration\n\n 3. Tools (packages/core/src/tools/):\n    \n    * Purpose: These are individual modules that extend the capabilities of the\n      Volcano Engine model, allowing it to interact with the local environment\n      (e.g., file system, shell commands, web fetching).\n    * Interaction: packages/core invokes these tools based on requests from the\n      Volcano Engine model.\n\n\nInteraction Flow#\n\nA typical interaction with the VeCLI follows this flow:\n\n 1. User input: The user types a prompt or command into the terminal, which is\n    managed by packages/cli.\n 2. Request to core: packages/cli sends the user's input to packages/core.\n 3. Request processed: The core package:\n    * Constructs an appropriate prompt for the Volcano Engine API, possibly\n      including conversation history and available tool definitions.\n    * Sends the prompt to the Volcano Engine API.\n 4. Volcano Engine API response: The Volcano Engine API processes the prompt and\n    returns a response. This response might be a direct answer or a request to\n    use one of the available tools.\n 5. Tool execution (if applicable):\n    * When the Volcano Engine API requests a tool, the core package prepares to\n      execute it.\n    * If the requested tool can modify the file system or execute shell\n      commands, the user is first given details of the tool and its arguments,\n      and the user must approve the execution.\n    * Read-only operations, such as reading files, might not require explicit\n      user confirmation to proceed.\n    * Once confirmed, or if confirmation is not required, the core package\n      executes the relevant action within the relevant tool, and the result is\n      sent back to the Volcano Engine API by the core package.\n    * The Volcano Engine API processes the tool result and generates a final\n      response.\n 6. Response to CLI: The core package sends the final response back to the CLI\n    package.\n 7. Display to user: The CLI package formats and displays the response to the\n    user in the terminal.\n\n\nKey Design Principles#\n\n * Modularity: Separating the CLI (frontend) from the Core (backend) allows for\n   independent development and potential future extensions (e.g., different\n   frontends for the same backend).\n * Extensibility: The tool system is designed to be extensible, allowing new\n   capabilities to be added.\n * User experience: The CLI focuses on providing a rich and interactive terminal\n   experience.","routePath":"/architecture","lang":"","toc":[{"text":"Core components","id":"core-components","depth":2,"charIndex":78},{"text":"Interaction Flow","id":"interaction-flow","depth":2,"charIndex":1590},{"text":"Key Design Principles","id":"key-design-principles","depth":2,"charIndex":3235}],"domain":"","frontmatter":{},"version":""},{"id":2,"title":"Checkpointing","content":"#\n\nThe VeCLI includes a Checkpointing feature that automatically saves a snapshot\nof your project's state before any file modifications are made by AI-powered\ntools. This allows you to safely experiment with and apply code changes, knowing\nyou can instantly revert back to the state before the tool was run.\n\n\nHow It Works#\n\nWhen you approve a tool that modifies the file system (like write_file or\nreplace), the CLI automatically creates a \"checkpoint.\" This checkpoint\nincludes:\n\n 1. A Git Snapshot: A commit is made in a special, shadow Git repository located\n    in your home directory (~/.ve/history/<project_hash>). This snapshot\n    captures the complete state of your project files at that moment. It does\n    not interfere with your own project's Git repository.\n 2. Conversation History: The entire conversation you've had with the agent up\n    to that point is saved.\n 3. The Tool Call: The specific tool call that was about to be executed is also\n    stored.\n\nIf you want to undo the change or simply go back, you can use the /restore\ncommand. Restoring a checkpoint will:\n\n * Revert all files in your project to the state captured in the snapshot.\n * Restore the conversation history in the CLI.\n * Re-propose the original tool call, allowing you to run it again, modify it,\n   or simply ignore it.\n\nAll checkpoint data, including the Git snapshot and conversation history, is\nstored locally on your machine. The Git snapshot is stored in the shadow\nrepository while the conversation history and tool calls are saved in a JSON\nfile in your project's temporary directory, typically located at\n~/.ve/tmp/<project_hash>/checkpoints.\n\n\nEnabling the Feature#\n\nThe Checkpointing feature is disabled by default. To enable it, you can either\nuse a command-line flag or edit your settings.json file.\n\n\nUsing the Command-Line Flag#\n\nYou can enable checkpointing for the current session by using the\n--checkpointing flag when starting the VeCLI:\n\n\n\n\nUsing the settings.json File#\n\nTo enable checkpointing by default for all sessions, you need to edit your\nsettings.json file.\n\nAdd the following key to your settings.json:\n\n\n\n\nUsing the /restore Command#\n\nOnce enabled, checkpoints are created automatically. To manage them, you use the\n/restore command.\n\n\nList Available Checkpoints#\n\nTo see a list of all saved checkpoints for the current project, simply run:\n\n\n\nThe CLI will display a list of available checkpoint files. These file names are\ntypically composed of a timestamp, the name of the file being modified, and the\nname of the tool that was about to be run (e.g.,\n2025-06-22T10-00-00_000Z-my-file.txt-write_file).\n\n\nRestore a Specific Checkpoint#\n\nTo restore your project to a specific checkpoint, use the checkpoint file from\nthe list:\n\n\n\nFor example:\n\n\n\nAfter running the command, your files and conversation will be immediately\nrestored to the state they were in when the checkpoint was created, and the\noriginal tool prompt will reappear.","routePath":"/checkpointing","lang":"","toc":[{"text":"How It Works","id":"how-it-works","depth":2,"charIndex":309},{"text":"Enabling the Feature","id":"enabling-the-feature","depth":2,"charIndex":1644},{"text":"Using the Command-Line Flag","id":"using-the-command-line-flag","depth":3,"charIndex":1805},{"text":"Using the `settings.json` File","id":"using-the-settingsjson-file","depth":3,"charIndex":-1},{"text":"Using the `/restore` Command","id":"using-the-restore-command","depth":2,"charIndex":-1},{"text":"List Available Checkpoints","id":"list-available-checkpoints","depth":3,"charIndex":2257},{"text":"Restore a Specific Checkpoint","id":"restore-a-specific-checkpoint","depth":3,"charIndex":2626}],"domain":"","frontmatter":{},"version":""},{"id":3,"title":"Authentication Setup","content":"#\n\nThe VeCLI requires you to authenticate with Google's AI services. On initial\nstartup you'll need to configure one of the following authentication methods:\n\n 1. Login with Google (Ve Code Assist):\n    \n    * Use this option to log in with your Google account.\n    \n    * During initial startup, VeCLI will direct you to a webpage for\n      authentication. Once authenticated, your credentials will be cached\n      locally so the web login can be skipped on subsequent runs.\n    \n    * Note that the web login must be done in a browser that can communicate\n      with the machine VeCLI is being run from. (Specifically, the browser will\n      be redirected to a localhost url that VeCLI will be listening on).\n    \n    * Users may have to specify a GOOGLE_CLOUD_PROJECT if:\n      \n      1. You have a Google Workspace account. Google Workspace is a paid service\n         for businesses and organizations that provides a suite of productivity\n         tools, including a custom email domain (e.g.\n         your-name@your-company.com), enhanced security features, and\n         administrative controls. These accounts are often managed by an\n         employer or school.\n      2. You have received a Ve Code Assist license through the Google Developer\n         Program (including qualified Google Developer Experts)\n      3. You have been assigned a license to a current Ve Code Assist standard\n         or enterprise subscription.\n      4. You are using the product outside the supported regions for free\n         individual usage.\n      5. You are a Google account holder under the age of 18\n      * If you fall into one of these categories, you must first configure a\n        Google Cloud Project ID to use, enable the Gemini for Cloud API and\n        configure access permissions.\n      \n      You can temporarily set the environment variable in your current shell\n      session using the following command:\n      \n      \n      \n      * For repeated use, you can add the environment variable to your .env file\n        or your shell's configuration file (like ~/.bashrc, ~/.zshrc, or\n        ~/.profile). For example, the following command adds the environment\n        variable to a ~/.bashrc file:\n      \n      \n\n 2. Gemini API key:\n    \n    * Obtain your API key from Google AI Studio:\n      https://aistudio.google.com/app/apikey\n    * Set the GEMINI_API_KEY environment variable. In the following methods,\n      replace YOUR_GEMINI_API_KEY with the API key you obtained from Google AI\n      Studio:\n      \n      * You can temporarily set the environment variable in your current shell\n        session using the following command:\n        \n        \n      \n      * For repeated use, you can add the environment variable to your .env\n        file.\n      \n      * Alternatively you can export the API key from your shell's configuration\n        file (like ~/.bashrc, ~/.zshrc, or ~/.profile). For example, the\n        following command adds the environment variable to a ~/.bashrc file:\n        \n        \n        \n        :warning: Be advised that when you export your API key inside your shell\n        configuration file, any other process executed from the shell can read\n        it.\n\n 3. Vertex AI:\n    \n    * API Key:\n      \n      * Obtain your Google Cloud API key: Get an API Key\n      * Set the GOOGLE_API_KEY environment variable. In the following methods,\n        replace YOUR_GOOGLE_API_KEY with your Vertex AI API key:\n        \n        * You can temporarily set the environment variable in your current shell\n          session using the following command:\n          \n          \n        \n        * For repeated use, you can add the environment variable to your .env\n          file or your shell's configuration file (like ~/.bashrc, ~/.zshrc, or\n          ~/.profile). For example, the following command adds the environment\n          variable to a ~/.bashrc file:\n          \n          \n          \n          :warning: Be advised that when you export your API key inside your\n          shell configuration file, any other process executed from the shell\n          can read it.\n          \n          > Note: If you encounter an error like \"API keys are not supported by\n          > this API - Expected OAuth2 access token or other authentication\n          > credentials that assert a principal\", it is likely that your\n          > organization has restricted the creation of service account API\n          > keys. In this case, please try the service account JSON key method\n          > described below.\n    \n    * Application Default Credentials (ADC):\n      \n      > Note: If you have previously set the GOOGLE_API_KEY or GEMINI_API_KEY\n      > environment variables, you must unset them to use Application Default\n      > Credentials.\n      \n      * Using gcloud (for local development):\n        \n        * Ensure you have a Google Cloud project and have enabled the Vertex AI\n          API.\n        * Log in with your user credentials:\n          \n          \n          \n          For more information, see Set up Application Default Credentials for\n          Google Cloud.\n      \n      * Using a Service Account (for applications or when service account API\n        keys are restricted):\n        \n        * If you are unable to create an API key due to organization policies,\n          or if you are running in a non-interactive environment, you can\n          authenticate using a service account key.\n        * Create a service account and key, and download the JSON key file. The\n          service account will need to be assigned the \"Vertex AI User\" role.\n        * Set the GOOGLE_APPLICATION_CREDENTIALS environment variable to the\n          absolute path of the JSON file.\n          * You can temporarily set the environment variable in your current\n            shell session:\n            \n            \n          \n          * For repeated use, you can add the command to your shell's\n            configuration file (e.g., ~/.bashrc).\n            \n            \n            \n            :warning: Be advised that when you export service account\n            credentials inside your shell configuration file, any other process\n            executed from the shell can read it.\n      \n      * Required Environment Variables for ADC:\n        \n        * When using ADC (either with gcloud or a service account), you must\n          also set the GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_LOCATION\n          environment variables. In the following methods, replace\n          YOUR_PROJECT_ID and YOUR_PROJECT_LOCATION with the relevant values for\n          your project:\n          * You can temporarily set these environment variables in your current\n            shell session using the following commands:\n            \n            \n          \n          * For repeated use, you can add the environment variables to your .env\n            file or your shell's configuration file (like ~/.bashrc, ~/.zshrc,\n            or ~/.profile). For example, the following commands add the\n            environment variables to a ~/.bashrc file:\n            \n            \n\n 4. Cloud Shell:\n    \n    * This option is only available when running in a Google Cloud Shell\n      environment.\n    \n    * It automatically uses the credentials of the logged-in user in the Cloud\n      Shell environment.\n    \n    * This is the default authentication method when running in Cloud Shell and\n      no other method is configured.\n      \n      \n\n\nPersisting Environment Variables with .env Files#\n\nYou can create a .ve/.env file in your project directory or in your home\ndirectory. Creating a plain .env file also works, but .ve/.env is recommended to\nkeep Gemini variables isolated from other tools.\n\nImportant: Some environment variables (like DEBUG and DEBUG_MODE) are\nautomatically excluded from project .env files to prevent interference with\ngemini-cli behavior. Use .ve/.env files for gemini-cli specific variables.\n\nVeCLI automatically loads environment variables from the first .env file it\nfinds, using the following search order:\n\n 1. Starting in the current directory and moving upward toward /, for each\n    directory it checks:\n    1. .ve/.env\n    2. .env\n 2. If no file is found, it falls back to your home directory:\n    * ~/.ve/.env\n    * ~/.env\n\n> Important: The search stops at the first file encountered—variables are not\n> merged across multiple files.\n\nExamples#\n\nProject-specific overrides (take precedence when you are inside the project):\n\n\n\nUser-wide settings (available in every directory):\n\n\n\n\nNon-Interactive Mode / Headless Environments#\n\nWhen running the VeCLI in a non-interactive environment, you cannot use the\ninteractive login flow. Instead, you must configure authentication using\nenvironment variables.\n\nThe CLI will automatically detect if it is running in a non-interactive terminal\nand will use one of the following authentication methods if available:\n\n 1. Gemini API Key:\n    \n    * Set the GEMINI_API_KEY environment variable.\n    * The CLI will use this key to authenticate with the Gemini API.\n\n 2. Vertex AI:\n    \n    * Set the GOOGLE_GENAI_USE_VERTEXAI=true environment variable.\n    * Using an API Key: Set the GOOGLE_API_KEY environment variable.\n    * Using Application Default Credentials (ADC):\n      * Run gcloud auth application-default login in your environment to\n        configure ADC.\n      * Ensure the GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_LOCATION environment\n        variables are set.\n\nIf none of these environment variables are set in a non-interactive session, the\nCLI will exit with an error.","routePath":"/cli/authentication","lang":"","toc":[{"text":"Persisting Environment Variables with `.env` Files","id":"persisting-environment-variables-with-env-files","depth":3,"charIndex":-1},{"text":"Examples","id":"examples","depth":4,"charIndex":8346},{"text":"Non-Interactive Mode / Headless Environments","id":"non-interactive-mode--headless-environments","depth":2,"charIndex":8493}],"domain":"","frontmatter":{},"version":""},{"id":4,"title":"CLI Commands","content":"#\n\nVeCLI supports several built-in commands to help you manage your session,\ncustomize the interface, and control its behavior. These commands are prefixed\nwith a forward slash (/), an at symbol (@), or an exclamation mark (!).\n\n\nSlash commands (/)#\n\nSlash commands provide meta-level control over the CLI itself.\n\n\nBuilt-in Commands#\n\n * /bug\n   \n   * Description: File an issue about VeCLI. By default, the issue is filed\n     within the GitHub repository for VeCLI. The string you enter after /bug\n     will become the headline for the bug being filed. The default /bug behavior\n     can be modified using the advanced.bugCommand setting in your\n     .ve/settings.json files.\n\n * /chat\n   \n   * Description: Save and resume conversation history for branching\n     conversation state interactively, or resuming a previous state from a later\n     session.\n   * Sub-commands:\n     * save\n       * Description: Saves the current conversation history. You must add a\n         <tag> for identifying the conversation state.\n       * Usage: /chat save <tag>\n       * Details on Checkpoint Location: The default locations for saved chat\n         checkpoints are:\n         * Linux/macOS: ~/.ve/tmp/<project_hash>/\n         * Windows: C:\\Users\\<YourUsername>\\.ve\\tmp\\<project_hash>\\\n         * When you run /chat list, the CLI only scans these specific\n           directories to find available checkpoints.\n         * Note: These checkpoints are for manually saving and resuming\n           conversation states. For automatic checkpoints created before file\n           modifications, see the Checkpointing documentation.\n     * resume\n       * Description: Resumes a conversation from a previous save.\n       * Usage: /chat resume <tag>\n     * list\n       * Description: Lists available tags for chat state resumption.\n     * delete\n       * Description: Deletes a saved conversation checkpoint.\n       * Usage: /chat delete <tag>\n\n * /clear\n   \n   * Description: Clear the terminal screen, including the visible session\n     history and scrollback within the CLI. The underlying session data (for\n     history recall) might be preserved depending on the exact implementation,\n     but the visual display is cleared.\n   * Keyboard shortcut: Press Ctrl+L at any time to perform a clear action.\n\n * /compress\n   \n   * Description: Replace the entire chat context with a summary. This saves on\n     tokens used for future tasks while retaining a high level summary of what\n     has happened.\n\n * /copy\n   \n   * Description: Copies the last output produced by VeCLI to your clipboard,\n     for easy sharing or reuse.\n   * Note: This command requires platform-specific clipboard tools to be\n     installed.\n     * On Linux, it requires xclip or xsel. You can typically install them using\n       your system's package manager.\n     * On macOS, it requires pbcopy, and on Windows, it requires clip. These\n       tools are typically pre-installed on their respective systems.\n\n * /directory (or /dir)\n   \n   * Description: Manage workspace directories for multi-directory support.\n   * Sub-commands:\n     * add:\n       * Description: Add a directory to the workspace. The path can be absolute\n         or relative to the current working directory. Moreover, the reference\n         from home directory is supported as well.\n       * Usage: /directory add <path1>,<path2>\n       * Note: Disabled in restrictive sandbox profiles. If you're using that,\n         use --include-directories when starting the session instead.\n     * show:\n       * Description: Display all directories added by /directory add and\n         --include-directories.\n       * Usage: /directory show\n\n * /editor\n   \n   * Description: Open a dialog for selecting supported editors.\n\n * /extensions\n   \n   * Description: Lists all active extensions in the current VeCLI session. See\n     VeCLI Extensions.\n\n * /help (or /?)\n   \n   * Description: Display help information about VeCLI, including available\n     commands and their usage.\n\n * /mcp\n   \n   * Description: List configured Model Context Protocol (MCP) servers, their\n     connection status, server details, and available tools.\n   * Sub-commands:\n     * desc or descriptions:\n       * Description: Show detailed descriptions for MCP servers and tools.\n     * nodesc or nodescriptions:\n       * Description: Hide tool descriptions, showing only the tool names.\n     * schema:\n       * Description: Show the full JSON schema for the tool's configured\n         parameters.\n   * Keyboard Shortcut: Press Ctrl+T at any time to toggle between showing and\n     hiding tool descriptions.\n\n * /memory\n   \n   * Description: Manage the AI's instructional context (hierarchical memory\n     loaded from VE.md files).\n   * Sub-commands:\n     * add:\n       * Description: Adds the following text to the AI's memory. Usage: /memory\n         add <text to remember>\n     * show:\n       * Description: Display the full, concatenated content of the current\n         hierarchical memory that has been loaded from all VE.md files. This\n         lets you inspect the instructional context being provided to the\n         Volcano Engine model.\n     * refresh:\n       * Description: Reload the hierarchical instructional memory from all\n         VE.md files found in the configured locations (global,\n         project/ancestors, and sub-directories). This command updates the model\n         with the latest VE.md content.\n     * Note: For more details on how VE.md files contribute to hierarchical\n       memory, see the CLI Configuration documentation.\n\n * /restore\n   \n   * Description: Restores the project files to the state they were in just\n     before a tool was executed. This is particularly useful for undoing file\n     edits made by a tool. If run without a tool call ID, it will list available\n     checkpoints to restore from.\n   * Usage: /restore [tool_call_id]\n   * Note: Only available if the CLI is invoked with the --checkpointing option\n     or configured via settings. See Checkpointing documentation for more\n     details.\n\n * /settings\n   \n   * Description: Open the settings editor to view and modify VeCLI settings.\n   * Details: This command provides a user-friendly interface for changing\n     settings that control the behavior and appearance of VeCLI. It is\n     equivalent to manually editing the .ve/settings.json file, but with\n     validation and guidance to prevent errors.\n   * Usage: Simply run /settings and the editor will open. You can then browse\n     or search for specific settings, view their current values, and modify them\n     as desired. Changes to some settings are applied immediately, while others\n     require a restart.\n\n * /stats\n   \n   * Description: Display detailed statistics for the current VeCLI session,\n     including token usage, cached token savings (when available), and session\n     duration. Note: Cached token information is only displayed when cached\n     tokens are being used, which occurs with API key authentication but not\n     with OAuth authentication at this time.\n\n * /theme\n   \n   * Description: Open a dialog that lets you change the visual theme of VeCLI.\n\n * /auth\n   \n   * Description: Open a dialog that lets you change the authentication method.\n\n * /about\n   \n   * Description: Show version info. Please share this information when filing\n     issues.\n\n * /tools\n   \n   * Description: Display a list of tools that are currently available within\n     VeCLI.\n   * Usage: /tools [desc]\n   * Sub-commands:\n     * desc or descriptions:\n       * Description: Show detailed descriptions of each tool, including each\n         tool's name with its full description as provided to the model.\n     * nodesc or nodescriptions:\n       * Description: Hide tool descriptions, showing only the tool names.\n\n * /privacy\n   \n   * Description: Display the Privacy Notice and allow users to select whether\n     they consent to the collection of their data for service improvement\n     purposes.\n\n * /quit (or /exit)\n   \n   * Description: Exit VeCLI.\n\n * /vim\n   \n   * Description: Toggle vim mode on or off. When vim mode is enabled, the input\n     area supports vim-style navigation and editing commands in both NORMAL and\n     INSERT modes.\n   * Features:\n     * NORMAL mode: Navigate with h, j, k, l; jump by words with w, b, e; go to\n       line start/end with 0, $, ^; go to specific lines with G (or gg for first\n       line)\n     * INSERT mode: Standard text input with escape to return to NORMAL mode\n     * Editing commands: Delete with x, change with c, insert with i, a, o, O;\n       complex operations like dd, cc, dw, cw\n     * Count support: Prefix commands with numbers (e.g., 3h, 5w, 10G)\n     * Repeat last command: Use . to repeat the last editing operation\n     * Persistent setting: Vim mode preference is saved to ~/.ve/settings.json\n       and restored between sessions\n   * Status indicator: When enabled, shows [NORMAL] or [INSERT] in the footer\n\n * /init\n   \n   * Description: To help users easily create a VE.md file, this command\n     analyzes the current directory and generates a tailored context file,\n     making it simpler for them to provide project-specific instructions to the\n     Volcano Engine agent.\n\n\nCustom Commands#\n\nFor a quick start, see the example below.\n\nCustom commands allow you to save and reuse your favorite or most frequently\nused prompts as personal shortcuts within VeCLI. You can create commands that\nare specific to a single project or commands that are available globally across\nall your projects, streamlining your workflow and ensuring consistency.\n\nFile Locations & Precedence#\n\nVeCLI discovers commands from two locations, loaded in a specific order:\n\n 1. User Commands (Global): Located in ~/.ve/commands/. These commands are\n    available in any project you are working on.\n 2. Project Commands (Local): Located in ~/.ve/commands/. These commands are\n    specific to the current project and can be checked into version control to\n    be shared with your team.\n\nIf a command in the project directory has the same name as a command in the user\ndirectory, the project command will always be used. This allows projects to\noverride global commands with project-specific versions.\n\nNaming and Namespacing#\n\nThe name of a command is determined by its file path relative to its commands\ndirectory. Subdirectories are used to create namespaced commands, with the path\nseparator (/ or \\) being converted to a colon (:).\n\n * A file at ~/.ve/commands/test.toml becomes the command /test.\n * A file at <project>/.ve/commands/git/commit.toml becomes the namespaced\n   command /git:commit.\n\nTOML File Format (v1)#\n\nYour command definition files must be written in the TOML format and use the\n.toml file extension.\n\nRequired Fields#\n\n * prompt (String): The prompt that will be sent to the Volcano Engine model\n   when the command is executed. This can be a single-line or multi-line string.\n\nOptional Fields#\n\n * description (String): A brief, one-line description of what the command does.\n   This text will be displayed next to your command in the /help menu. If you\n   omit this field, a generic description will be generated from the filename.\n\nHandling Arguments#\n\nCustom commands support two powerful methods for handling arguments. The CLI\nautomatically chooses the correct method based on the content of your command's\nprompt.\n\n1. Context-Aware Injection with {{args}}#\n\nIf your prompt contains the special placeholder {{args}}, the CLI will replace\nthat placeholder with the text the user typed after the command name.\n\nThe behavior of this injection depends on where it is used:\n\nA. Raw Injection (Outside Shell Commands)\n\nWhen used in the main body of the prompt, the arguments are injected exactly as\nthe user typed them.\n\nExample (git/fix.toml):\n\n\n\nThe model receives: Please provide a code fix for the issue described here:\n\"Button is misaligned\".\n\nB. Using Arguments in Shell Commands (Inside !{...} Blocks)\n\nWhen you use {{args}} inside a shell injection block (!{...}), the arguments are\nautomatically shell-escaped before replacement. This allows you to safely pass\narguments to shell commands, ensuring the resulting command is syntactically\ncorrect and secure while preventing command injection vulnerabilities.\n\nExample (/grep-code.toml):\n\n\n\nWhen you run /grep-code It's complicated:\n\n 1. The CLI sees {{args}} used both outside and inside !{...}.\n 2. Outside: The first {{args}} is replaced raw with It's complicated.\n 3. Inside: The second {{args}} is replaced with the escaped version (e.g., on\n    Linux: \"It's complicated\").\n 4. The command executed is grep -r \"It's complicated\" ..\n 5. The CLI prompts you to confirm this exact, secure command before execution.\n 6. The final prompt is sent.\n\n2. Default Argument Handling#\n\nIf your prompt does not contain the special placeholder {{args}}, the CLI uses a\ndefault behavior for handling arguments.\n\nIf you provide arguments to the command (e.g., /mycommand arg1), the CLI will\nappend the full command you typed to the end of the prompt, separated by two\nnewlines. This allows the model to see both the original instructions and the\nspecific arguments you just provided.\n\nIf you do not provide any arguments (e.g., /mycommand), the prompt is sent to\nthe model exactly as it is, with nothing appended.\n\nExample (changelog.toml):\n\nThis example shows how to create a robust command by defining a role for the\nmodel, explaining where to find the user's input, and specifying the expected\nformat and behavior.\n\n\n\nWhen you run /changelog 1.2.0 added \"New feature\", the final text sent to the\nmodel will be the original prompt followed by two newlines and the command you\ntyped.\n\n3. Executing Shell Commands with !{...}#\n\nYou can make your commands dynamic by executing shell commands directly within\nyour prompt and injecting their output. This is ideal for gathering context from\nyour local environment, like reading file content or checking the status of Git.\n\nWhen a custom command attempts to execute a shell command, VeCLI will now prompt\nyou for confirmation before proceeding. This is a security measure to ensure\nthat only intended commands can be run.\n\nHow It Works:\n\n 1. Inject Commands: Use the !{...} syntax.\n 2. Argument Substitution: If {{args}} is present inside the block, it is\n    automatically shell-escaped (see Context-Aware Injection above).\n 3. Robust Parsing: The parser correctly handles complex shell commands that\n    include nested braces, such as JSON payloads. Note: The content inside\n    !{...} must have balanced braces ({ and }). If you need to execute a command\n    containing unbalanced braces, consider wrapping it in an external script\n    file and calling the script within the !{...} block.\n 4. Security Check and Confirmation: The CLI performs a security check on the\n    final, resolved command (after arguments are escaped and substituted). A\n    dialog will appear showing the exact command(s) to be executed.\n 5. Execution and Error Reporting: The command is executed. If the command\n    fails, the output injected into the prompt will include the error messages\n    (stderr) followed by a status line, e.g., [Shell command exited with code\n    1]. This helps the model understand the context of the failure.\n\nExample (git/commit.toml):\n\nThis command gets the staged git diff and uses it to ask the model to write a\ncommit message.\n\n\n\nWhen you run /git:commit, the CLI first executes git diff --staged, then\nreplaces !{git diff --staged} with the output of that command before sending the\nfinal, complete prompt to the model.\n\n4. Injecting File Content with @{...}#\n\nYou can directly embed the content of a file or a directory listing into your\nprompt using the @{...} syntax. This is useful for creating commands that\noperate on specific files.\n\nHow It Works:\n\n * File Injection: @{path/to/file.txt} is replaced by the content of file.txt.\n * Multimodal Support: If the path points to a supported image (e.g., PNG,\n   JPEG), PDF, audio, or video file, it will be correctly encoded and injected\n   as multimodal input. Other binary files are handled gracefully and skipped.\n * Directory Listing: @{path/to/dir} is traversed and each file present within\n   the directory and all subdirectories are inserted into the prompt. This\n   respects .gitignore and .veignore if enabled.\n * Workspace-Aware: The command searches for the path in the current directory\n   and any other workspace directories. Absolute paths are allowed if they are\n   within the workspace.\n * Processing Order: File content injection with @{...} is processed before\n   shell commands (!{...}) and argument substitution ({{args}}).\n * Parsing: The parser requires the content inside @{...} (the path) to have\n   balanced braces ({ and }).\n\nExample (review.toml):\n\nThis command injects the content of a fixed best practices file\n(docs/best-practices.md) and uses the user's arguments to provide context for\nthe review.\n\n\n\nWhen you run /review FileCommandLoader.ts, the @{docs/best-practices.md}\nplaceholder is replaced by the content of that file, and {{args}} is replaced by\nthe text you provided, before the final prompt is sent to the model.\n\n--------------------------------------------------------------------------------\n\nExample: A \"Pure Function\" Refactoring Command#\n\nLet's create a global command that asks the model to refactor a piece of code.\n\n1. Create the file and directories:\n\nFirst, ensure the user commands directory exists, then create a refactor\nsubdirectory for organization and the final TOML file.\n\n\n\n2. Add the content to the file:\n\nOpen ~/.ve/commands/refactor/pure.toml in your editor and add the following\ncontent. We are including the optional description for best practice.\n\n\n\n3. Run the Command:\n\nThat's it! You can now run your command in the CLI. First, you might add a file\nto the context, and then invoke your command:\n\n\n\nVeCLI will then execute the multi-line prompt defined in your TOML file.\n\n\nAt commands (@)#\n\nAt commands are used to include the content of files or directories as part of\nyour prompt to Volcano Engine. These commands include git-aware filtering.\n\n * @<path_to_file_or_directory>\n   \n   * Description: Inject the content of the specified file or files into your\n     current prompt. This is useful for asking questions about specific code,\n     text, or collections of files.\n   * Examples:\n     * @path/to/your/file.txt Explain this text.\n     * @src/my_project/ Summarize the code in this directory.\n     * What is this file about? @README.md\n   * Details:\n     * If a path to a single file is provided, the content of that file is read.\n     * If a path to a directory is provided, the command attempts to read the\n       content of files within that directory and any subdirectories.\n     * Spaces in paths should be escaped with a backslash (e.g., @My\\\n       Documents/file.txt).\n     * The command uses the read_many_files tool internally. The content is\n       fetched and then inserted into your query before being sent to the model.\n     * Git-aware filtering: By default, git-ignored files (like node_modules/,\n       dist/, .env, .git/) are excluded. This behavior can be changed via the\n       context.fileFiltering settings.\n     * File types: The command is intended for text-based files. While it might\n       attempt to read any file, binary files or very large files might be\n       skipped or truncated by the underlying read_many_files tool to ensure\n       performance and relevance. The tool indicates if files were skipped.\n   * Output: The CLI will show a tool call message indicating that\n     read_many_files was used, along with a message detailing the status and the\n     path(s) that were processed.\n\n * @ (Lone at symbol)\n   \n   * Description: If you type a lone @ symbol without a path, the query is\n     passed as-is to the Volcano Engine model. This might be useful if you are\n     specifically talking about the @ symbol in your prompt.\n\n\nError handling for @ commands#\n\n * If the path specified after @ is not found or is invalid, an error message\n   will be displayed, and the query might not be sent to the model, or it will\n   be sent without the file content.\n * If the read_many_files tool encounters an error (e.g., permission issues),\n   this will also be reported.\n\n\nShell mode & passthrough commands (!)#\n\nThe ! prefix lets you interact with your system's shell directly from within\nVeCLI.\n\n * !<shell_command>\n   \n   * Description: Execute the given <shell_command> using bash on Linux/macOS or\n     cmd.exe on Windows. Any output or errors from the command are displayed in\n     the terminal.\n   * Examples:\n     * !ls -la (executes ls -la and returns to VeCLI)\n     * !git status (executes git status and returns to VeCLI)\n\n * ! (Toggle shell mode)\n   \n   * Description: Typing ! on its own toggles shell mode.\n     * Entering shell mode:\n       * When active, shell mode uses a different coloring and a \"Shell Mode\n         Indicator\".\n       * While in shell mode, text you type is interpreted directly as a shell\n         command.\n     * Exiting shell mode:\n       * When exited, the UI reverts to its standard appearance and normal VeCLI\n         behavior resumes.\n\n * Caution for all ! usage: Commands you execute in shell mode have the same\n   permissions and impact as if you ran them directly in your terminal.\n\n * Environment Variable: When a command is executed via ! or in shell mode, the\n   GEMINI_CLI=1 environment variable is set in the subprocess's environment.\n   This allows scripts or tools to detect if they are being run from within the\n   VeCLI.","routePath":"/cli/commands","lang":"","toc":[{"text":"Slash commands (`/`)","id":"slash-commands-","depth":2,"charIndex":-1},{"text":"Built-in Commands","id":"built-in-commands","depth":3,"charIndex":315},{"text":"Custom Commands","id":"custom-commands","depth":3,"charIndex":9188},{"text":"File Locations & Precedence","id":"file-locations--precedence","depth":4,"charIndex":9557},{"text":"Naming and Namespacing","id":"naming-and-namespacing","depth":4,"charIndex":10187},{"text":"TOML File Format (v1)","id":"toml-file-format-v1","depth":4,"charIndex":10587},{"text":"Handling Arguments","id":"handling-arguments","depth":4,"charIndex":11145},{"text":"Example: A \"Pure Function\" Refactoring Command","id":"example-a-pure-function-refactoring-command","depth":4,"charIndex":17205},{"text":"At commands (`@`)","id":"at-commands-","depth":2,"charIndex":-1},{"text":"Error handling for `@` commands","id":"error-handling-for--commands","depth":3,"charIndex":-1},{"text":"Shell mode & passthrough commands (`!`)","id":"shell-mode--passthrough-commands-","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":5,"title":"VeCLI Configuration","content":"#\n\nNote on Deprecated Configuration Format\n\nThis document describes the legacy v1 format for the settings.json file. This\nformat is now deprecated.\n\n * The new format will be supported in the stable release starting [09/10/25].\n * Automatic migration from the old format to the new format will begin on\n   [09/17/25].\n\nFor details on the new, recommended format, please see the current Configuration\ndocumentation.\n\nVeCLI offers several ways to configure its behavior, including environment\nvariables, command-line arguments, and settings files. This document outlines\nthe different configuration methods and available settings.\n\n\nConfiguration layers#\n\nConfiguration is applied in the following order of precedence (lower numbers are\noverridden by higher numbers):\n\n 1. Default values: Hardcoded defaults within the application.\n 2. System defaults file: System-wide default settings that can be overridden by\n    other settings files.\n 3. User settings file: Global settings for the current user.\n 4. Project settings file: Project-specific settings.\n 5. System settings file: System-wide settings that override all other settings\n    files.\n 6. Environment variables: System-wide or session-specific variables,\n    potentially loaded from .env files.\n 7. Command-line arguments: Values passed when launching the CLI.\n\n\nSettings files#\n\nVeCLI uses JSON settings files for persistent configuration. There are four\nlocations for these files:\n\n * System defaults file:\n   * Location: /etc/gemini-cli/system-defaults.json (Linux),\n     C:\\ProgramData\\gemini-cli\\system-defaults.json (Windows) or\n     /Library/Application Support/GeminiCli/system-defaults.json (macOS). The\n     path can be overridden using the GEMINI_CLI_SYSTEM_DEFAULTS_PATH\n     environment variable.\n   * Scope: Provides a base layer of system-wide default settings. These\n     settings have the lowest precedence and are intended to be overridden by\n     user, project, or system override settings.\n * User settings file:\n   * Location: ~/.ve/settings.json (where ~ is your home directory).\n   * Scope: Applies to all VeCLI sessions for the current user. User settings\n     override system defaults.\n * Project settings file:\n   * Location: .ve/settings.json within your project's root directory.\n   * Scope: Applies only when running VeCLI from that specific project. Project\n     settings override user settings and system defaults.\n * System settings file:\n   * Location: /etc/gemini-cli/settings.json (Linux),\n     C:\\ProgramData\\gemini-cli\\settings.json (Windows) or /Library/Application\n     Support/GeminiCli/settings.json (macOS). The path can be overridden using\n     the GEMINI_CLI_SYSTEM_SETTINGS_PATH environment variable.\n   * Scope: Applies to all VeCLI sessions on the system, for all users. System\n     settings act as overrides, taking precedence over all other settings files.\n     May be useful for system administrators at enterprises to have controls\n     over users' VeCLI setups.\n\nNote on environment variables in settings: String values within your\nsettings.json files can reference environment variables using either $VAR_NAME\nor ${VAR_NAME} syntax. These variables will be automatically resolved when the\nsettings are loaded. For example, if you have an environment variable\nMY_API_TOKEN, you could use it in settings.json like this: \"apiKey\":\n\"$MY_API_TOKEN\".\n\n> Note for Enterprise Users: For guidance on deploying and managing VeCLI in a\n> corporate environment, please see the Enterprise Configuration documentation.\n\n\nThe .ve directory in your project#\n\nIn addition to a project settings file, a project's .ve directory can contain\nother project-specific files related to VeCLI's operation, such as:\n\n * Custom sandbox profiles (e.g., .ve/sandbox-macos-custom.sb,\n   .ve/sandbox.Dockerfile).\n\n\nAvailable settings in settings.json:#\n\n * contextFileName (string or array of strings):\n   \n   * Description: Specifies the filename for context files (e.g., VE.md,\n     AGENTS.md). Can be a single filename or a list of accepted filenames.\n   * Default: VE.md\n   * Example: \"contextFileName\": \"AGENTS.md\"\n\n * bugCommand (object):\n   \n   * Description: Overrides the default URL for the /bug command.\n   * Default: \"urlTemplate\":\n     \"https://github.com/google-gemini/gemini-cli/issues/new?template=bug_report\n     .yml&title={title}&info={info}\"\n   * Properties:\n     * urlTemplate (string): A URL that can contain {title} and {info}\n       placeholders.\n   * Example:\n     \n     \n\n * fileFiltering (object):\n   \n   * Description: Controls git-aware file filtering behavior for @ commands and\n     file discovery tools.\n   * Default: \"respectGitIgnore\": true, \"enableRecursiveFileSearch\": true\n   * Properties:\n     * respectGitIgnore (boolean): Whether to respect .gitignore patterns when\n       discovering files. When set to true, git-ignored files (like\n       node_modules/, dist/, .env) are automatically excluded from @ commands\n       and file listing operations.\n     * enableRecursiveFileSearch (boolean): Whether to enable searching\n       recursively for filenames under the current tree when completing @\n       prefixes in the prompt.\n     * disableFuzzySearch (boolean): When true, disables the fuzzy search\n       capabilities when searching for files, which can improve performance on\n       projects with a large number of files.\n   * Example:\n     \n     \n\n\nTroubleshooting File Search Performance#\n\nIf you are experiencing performance issues with file searching (e.g., with @\ncompletions), especially in projects with a very large number of files, here are\na few things you can try in order of recommendation:\n\n 1. Use .veignore: Create a .veignore file in your project root to exclude\n    directories that contain a large number of files that you don't need to\n    reference (e.g., build artifacts, logs, node_modules). Reducing the total\n    number of files crawled is the most effective way to improve performance.\n\n 2. Disable Fuzzy Search: If ignoring files is not enough, you can disable fuzzy\n    search by setting disableFuzzySearch to true in your settings.json file.\n    This will use a simpler, non-fuzzy matching algorithm, which can be faster.\n\n 3. Disable Recursive File Search: As a last resort, you can disable recursive\n    file search entirely by setting enableRecursiveFileSearch to false. This\n    will be the fastest option as it avoids a recursive crawl of your project.\n    However, it means you will need to type the full path to files when using @\n    completions.\n\n * coreTools (array of strings):\n   \n   * Description: Allows you to specify a list of core tool names that should be\n     made available to the model. This can be used to restrict the set of\n     built-in tools. See Built-in Tools for a list of core tools. You can also\n     specify command-specific restrictions for tools that support it, like the\n     ShellTool. For example, \"coreTools\": [\"ShellTool(ls -l)\"] will only allow\n     the ls -l command to be executed.\n   * Default: All tools available for use by the Gemini model.\n   * Example: \"coreTools\": [\"ReadFileTool\", \"GlobTool\", \"ShellTool(ls)\"].\n\n * allowedTools (array of strings):\n   \n   * Default: undefined\n   * Description: A list of tool names that will bypass the confirmation dialog.\n     This is useful for tools that you trust and use frequently. The match\n     semantics are the same as coreTools.\n   * Example: \"allowedTools\": [\"ShellTool(git status)\"].\n\n * excludeTools (array of strings):\n   \n   * Description: Allows you to specify a list of core tool names that should be\n     excluded from the model. A tool listed in both excludeTools and coreTools\n     is excluded. You can also specify command-specific restrictions for tools\n     that support it, like the ShellTool. For example, \"excludeTools\":\n     [\"ShellTool(rm -rf)\"] will block the rm -rf command.\n   * Default: No tools excluded.\n   * Example: \"excludeTools\": [\"run_shell_command\", \"findFiles\"].\n   * Security Note: Command-specific restrictions in excludeTools for\n     run_shell_command are based on simple string matching and can be easily\n     bypassed. This feature is not a security mechanism and should not be relied\n     upon to safely execute untrusted code. It is recommended to use coreTools\n     to explicitly select commands that can be executed.\n\n * allowMCPServers (array of strings):\n   \n   * Description: Allows you to specify a list of MCP server names that should\n     be made available to the model. This can be used to restrict the set of MCP\n     servers to connect to. Note that this will be ignored if\n     --allowed-mcp-server-names is set.\n   * Default: All MCP servers are available for use by the Gemini model.\n   * Example: \"allowMCPServers\": [\"myPythonServer\"].\n   * Security Note: This uses simple string matching on MCP server names, which\n     can be modified. If you're a system administrator looking to prevent users\n     from bypassing this, consider configuring the mcpServers at the system\n     settings level such that the user will not be able to configure any MCP\n     servers of their own. This should not be used as an airtight security\n     mechanism.\n\n * excludeMCPServers (array of strings):\n   \n   * Description: Allows you to specify a list of MCP server names that should\n     be excluded from the model. A server listed in both excludeMCPServers and\n     allowMCPServers is excluded. Note that this will be ignored if\n     --allowed-mcp-server-names is set.\n   * Default: No MCP servers excluded.\n   * Example: \"excludeMCPServers\": [\"myNodeServer\"].\n   * Security Note: This uses simple string matching on MCP server names, which\n     can be modified. If you're a system administrator looking to prevent users\n     from bypassing this, consider configuring the mcpServers at the system\n     settings level such that the user will not be able to configure any MCP\n     servers of their own. This should not be used as an airtight security\n     mechanism.\n\n * autoAccept (boolean):\n   \n   * Description: Controls whether the CLI automatically accepts and executes\n     tool calls that are considered safe (e.g., read-only operations) without\n     explicit user confirmation. If set to true, the CLI will bypass the\n     confirmation prompt for tools deemed safe.\n   * Default: false\n   * Example: \"autoAccept\": true\n\n * theme (string):\n   \n   * Description: Sets the visual theme for VeCLI.\n   * Default: \"Default\"\n   * Example: \"theme\": \"GitHub\"\n\n * vimMode (boolean):\n   \n   * Description: Enables or disables vim mode for input editing. When enabled,\n     the input area supports vim-style navigation and editing commands with\n     NORMAL and INSERT modes. The vim mode status is displayed in the footer and\n     persists between sessions.\n   * Default: false\n   * Example: \"vimMode\": true\n\n * sandbox (boolean or string):\n   \n   * Description: Controls whether and how to use sandboxing for tool execution.\n     If set to true, VeCLI uses a pre-built gemini-cli-sandbox Docker image. For\n     more information, see Sandboxing.\n   * Default: false\n   * Example: \"sandbox\": \"docker\"\n\n * toolDiscoveryCommand (string):\n   \n   * Description: Defines a custom shell command for discovering tools from your\n     project. The shell command must return on stdout a JSON array of function\n     declarations. Tool wrappers are optional.\n   * Default: Empty\n   * Example: \"toolDiscoveryCommand\": \"bin/get_tools\"\n\n * toolCallCommand (string):\n   \n   * Description: Defines a custom shell command for calling a specific tool\n     that was discovered using toolDiscoveryCommand. The shell command must meet\n     the following criteria:\n     * It must take function name (exactly as in function declaration) as first\n       command line argument.\n     * It must read function arguments as JSON on stdin, analogous to\n       functionCall.args.\n     * It must return function output as JSON on stdout, analogous to\n       functionResponse.response.content.\n   * Default: Empty\n   * Example: \"toolCallCommand\": \"bin/call_tool\"\n\n * mcpServers (object):\n   \n   * Description: Configures connections to one or more Model-Context Protocol\n     (MCP) servers for discovering and using custom tools. VeCLI attempts to\n     connect to each configured MCP server to discover available tools. If\n     multiple MCP servers expose a tool with the same name, the tool names will\n     be prefixed with the server alias you defined in the configuration (e.g.,\n     serverAlias__actualToolName) to avoid conflicts. Note that the system might\n     strip certain schema properties from MCP tool definitions for\n     compatibility. At least one of command, url, or httpUrl must be provided.\n     If multiple are specified, the order of precedence is httpUrl, then url,\n     then command.\n   * Default: Empty\n   * Properties:\n     * <SERVER_NAME> (object): The server parameters for the named server.\n       * command (string, optional): The command to execute to start the MCP\n         server via standard I/O.\n       * args (array of strings, optional): Arguments to pass to the command.\n       * env (object, optional): Environment variables to set for the server\n         process.\n       * cwd (string, optional): The working directory in which to start the\n         server.\n       * url (string, optional): The URL of an MCP server that uses Server-Sent\n         Events (SSE) for communication.\n       * httpUrl (string, optional): The URL of an MCP server that uses\n         streamable HTTP for communication.\n       * headers (object, optional): A map of HTTP headers to send with requests\n         to url or httpUrl.\n       * timeout (number, optional): Timeout in milliseconds for requests to\n         this MCP server.\n       * trust (boolean, optional): Trust this server and bypass all tool call\n         confirmations.\n       * description (string, optional): A brief description of the server,\n         which may be used for display purposes.\n       * includeTools (array of strings, optional): List of tool names to\n         include from this MCP server. When specified, only the tools listed\n         here will be available from this server (allowlist behavior). If not\n         specified, all tools from the server are enabled by default.\n       * excludeTools (array of strings, optional): List of tool names to\n         exclude from this MCP server. Tools listed here will not be available\n         to the model, even if they are exposed by the server. Note:\n         excludeTools takes precedence over includeTools - if a tool is in both\n         lists, it will be excluded.\n   * Example:\n     \n     \n\n * checkpointing (object):\n   \n   * Description: Configures the checkpointing feature, which allows you to save\n     and restore conversation and file states. See the Checkpointing\n     documentation for more details.\n   * Default: {\"enabled\": false}\n   * Properties:\n     * enabled (boolean): When true, the /restore command is available.\n\n * preferredEditor (string):\n   \n   * Description: Specifies the preferred editor to use for viewing diffs.\n   * Default: vscode\n   * Example: \"preferredEditor\": \"vscode\"\n\n * telemetry (object)\n   \n   * Description: Configures logging and metrics collection for VeCLI. For more\n     information, see Telemetry.\n   * Default: {\"enabled\": false, \"target\": \"local\", \"otlpEndpoint\":\n     \"http://localhost:4317\", \"logPrompts\": true}\n   * Properties:\n     * enabled (boolean): Whether or not telemetry is enabled.\n     * target (string): The destination for collected telemetry. Supported\n       values are local and gcp.\n     * otlpEndpoint (string): The endpoint for the OTLP Exporter.\n     * logPrompts (boolean): Whether or not to include the content of user\n       prompts in the logs.\n   * Example:\n     \n     \n\n * usageStatisticsEnabled (boolean):\n   \n   * Description: Enables or disables the collection of usage statistics. See\n     Usage Statistics for more information.\n   * Default: true\n   * Example:\n     \n     \n\n * hideTips (boolean):\n   \n   * Description: Enables or disables helpful tips in the CLI interface.\n   \n   * Default: false\n   \n   * Example:\n     \n     \n\n * hideBanner (boolean):\n   \n   * Description: Enables or disables the startup banner (ASCII art logo) in the\n     CLI interface.\n   \n   * Default: false\n   \n   * Example:\n     \n     \n\n * maxSessionTurns (number):\n   \n   * Description: Sets the maximum number of turns for a session. If the session\n     exceeds this limit, the CLI will stop processing and start a new chat.\n   * Default: -1 (unlimited)\n   * Example:\n     \n     \n\n * summarizeToolOutput (object):\n   \n   * Description: Enables or disables the summarization of tool output. You can\n     specify the token budget for the summarization using the tokenBudget\n     setting.\n   * Note: Currently only the run_shell_command tool is supported.\n   * Default: {} (Disabled by default)\n   * Example:\n     \n     \n\n * excludedProjectEnvVars (array of strings):\n   \n   * Description: Specifies environment variables that should be excluded from\n     being loaded from project .env files. This prevents project-specific\n     environment variables (like DEBUG=true) from interfering with gemini-cli\n     behavior. Variables from .ve/.env files are never excluded.\n   * Default: [\"DEBUG\", \"DEBUG_MODE\"]\n   * Example:\n     \n     \n\n * includeDirectories (array of strings):\n   \n   * Description: Specifies an array of additional absolute or relative paths to\n     include in the workspace context. Missing directories will be skipped with\n     a warning by default. Paths can use ~ to refer to the user's home\n     directory. This setting can be combined with the --include-directories\n     command-line flag.\n   * Default: []\n   * Example:\n     \n     \n\n * loadMemoryFromIncludeDirectories (boolean):\n   \n   * Description: Controls the behavior of the /memory refresh command. If set\n     to true, VE.md files should be loaded from all directories that are added.\n     If set to false, VE.md should only be loaded from the current directory.\n   * Default: false\n   * Example:\n     \n     \n\n * chatCompression (object):\n   \n   * Description: Controls the settings for chat history compression, both\n     automatic and when manually invoked through the /compress command.\n   * Properties:\n     * contextPercentageThreshold (number): A value between 0 and 1 that\n       specifies the token threshold for compression as a percentage of the\n       model's total token limit. For example, a value of 0.6 will trigger\n       compression when the chat history exceeds 60% of the token limit.\n   * Example:\n     \n     \n\n * showLineNumbers (boolean):\n   \n   * Description: Controls whether line numbers are displayed in code blocks in\n     the CLI output.\n   * Default: true\n   * Example:\n     \n     \n\n * accessibility (object):\n   \n   * Description: Configures accessibility features for the CLI.\n   * Properties:\n     * screenReader (boolean): Enables screen reader mode, which adjusts the TUI\n       for better compatibility with screen readers. This can also be enabled\n       with the --screen-reader command-line flag, which will take precedence\n       over the setting.\n     * disableLoadingPhrases (boolean): Disables the display of loading phrases\n       during operations.\n   * Default: {\"screenReader\": false, \"disableLoadingPhrases\": false}\n   * Example:\n     \n     \n\n\nExample settings.json:#\n\n\n\n\nShell History#\n\nThe CLI keeps a history of shell commands you run. To avoid conflicts between\ndifferent projects, this history is stored in a project-specific directory\nwithin your user's home folder.\n\n * Location: ~/.ve/tmp/<project_hash>/shell_history\n   * <project_hash> is a unique identifier generated from your project's root\n     path.\n   * The history is stored in a file named shell_history.\n\n\nEnvironment Variables & .env Files#\n\nEnvironment variables are a common way to configure applications, especially for\nsensitive information like API keys or for settings that might change between\nenvironments. For authentication setup, see the Authentication documentation\nwhich covers all available authentication methods.\n\nThe CLI automatically loads environment variables from an .env file. The loading\norder is:\n\n 1. .env file in the current working directory.\n 2. If not found, it searches upwards in parent directories until it finds an\n    .env file or reaches the project root (identified by a .git folder) or the\n    home directory.\n 3. If still not found, it looks for ~/.env (in the user's home directory).\n\nEnvironment Variable Exclusion: Some environment variables (like DEBUG and\nDEBUG_MODE) are automatically excluded from being loaded from project .env files\nto prevent interference with gemini-cli behavior. Variables from .ve/.env files\nare never excluded. You can customize this behavior using the\nexcludedProjectEnvVars setting in your settings.json file.\n\n * GEMINI_API_KEY:\n   * Your API key for the Gemini API.\n   * One of several available authentication methods.\n   * Set this in your shell profile (e.g., ~/.bashrc, ~/.zshrc) or an .env file.\n * GEMINI_MODEL:\n   * Specifies the default Gemini model to use.\n   * Overrides the hardcoded default\n   * Example: export GEMINI_MODEL=\"gemini-2.5-flash\"\n * GOOGLE_API_KEY:\n   * Your Google Cloud API key.\n   * Required for using Vertex AI in express mode.\n   * Ensure you have the necessary permissions.\n   * Example: export GOOGLE_API_KEY=\"YOUR_GOOGLE_API_KEY\".\n * GOOGLE_CLOUD_PROJECT:\n   * Your Google Cloud Project ID.\n   * Required for using Code Assist or Vertex AI.\n   * If using Vertex AI, ensure you have the necessary permissions in this\n     project.\n   * Cloud Shell Note: When running in a Cloud Shell environment, this variable\n     defaults to a special project allocated for Cloud Shell users. If you have\n     GOOGLE_CLOUD_PROJECT set in your global environment in Cloud Shell, it will\n     be overridden by this default. To use a different project in Cloud Shell,\n     you must define GOOGLE_CLOUD_PROJECT in a .env file.\n   * Example: export GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_ID\".\n * GOOGLE_APPLICATION_CREDENTIALS (string):\n   * Description: The path to your Google Application Credentials JSON file.\n   * Example: export\n     GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your/credentials.json\"\n * OTLP_GOOGLE_CLOUD_PROJECT:\n   * Your Google Cloud Project ID for Telemetry in Google Cloud\n   * Example: export OTLP_GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_ID\".\n * GOOGLE_CLOUD_LOCATION:\n   * Your Google Cloud Project Location (e.g., us-central1).\n   * Required for using Vertex AI in non express mode.\n   * Example: export GOOGLE_CLOUD_LOCATION=\"YOUR_PROJECT_LOCATION\".\n * GEMINI_SANDBOX:\n   * Alternative to the sandbox setting in settings.json.\n   * Accepts true, false, docker, podman, or a custom command string.\n * SEATBELT_PROFILE (macOS specific):\n   * Switches the Seatbelt (sandbox-exec) profile on macOS.\n   * permissive-open: (Default) Restricts writes to the project folder (and a\n     few other folders, see\n     packages/cli/src/utils/sandbox-macos-permissive-open.sb) but allows other\n     operations.\n   * strict: Uses a strict profile that declines operations by default.\n   * <profile_name>: Uses a custom profile. To define a custom profile, create a\n     file named sandbox-macos-<profile_name>.sb in your project's .ve/ directory\n     (e.g., my-project/.ve/sandbox-macos-custom.sb).\n * DEBUG or DEBUG_MODE (often used by underlying libraries or the CLI itself):\n   * Set to true or 1 to enable verbose debug logging, which can be helpful for\n     troubleshooting.\n   * Note: These variables are automatically excluded from project .env files by\n     default to prevent interference with gemini-cli behavior. Use .ve/.env\n     files if you need to set these for gemini-cli specifically.\n * NO_COLOR:\n   * Set to any value to disable all color output in the CLI.\n * CLI_TITLE:\n   * Set to a string to customize the title of the CLI.\n * CODE_ASSIST_ENDPOINT:\n   * Specifies the endpoint for the code assist server.\n   * This is useful for development and testing.\n\n\nCommand-Line Arguments#\n\nArguments passed directly when running the CLI can override other configurations\nfor that specific session.\n\n * --model <model_name> (-m <model_name>):\n   * Specifies the Gemini model to use for this session.\n   * Example: npm start -- --model gemini-1.5-pro-latest\n * --prompt <your_prompt> (-p <your_prompt>):\n   * Used to pass a prompt directly to the command. This invokes VeCLI in a\n     non-interactive mode.\n * --prompt-interactive <your_prompt> (-i <your_prompt>):\n   * Starts an interactive session with the provided prompt as the initial\n     input.\n   * The prompt is processed within the interactive session, not before it.\n   * Cannot be used when piping input from stdin.\n   * Example: gemini -i \"explain this code\"\n * --sandbox (-s):\n   * Enables sandbox mode for this session.\n * --sandbox-image:\n   * Sets the sandbox image URI.\n * --debug (-d):\n   * Enables debug mode for this session, providing more verbose output.\n * --all-files (-a):\n   * If set, recursively includes all files within the current directory as\n     context for the prompt.\n * --help (or -h):\n   * Displays help information about command-line arguments.\n * --show-memory-usage:\n   * Displays the current memory usage.\n * --yolo:\n   * Enables YOLO mode, which automatically approves all tool calls.\n * --approval-mode <mode>:\n   * Sets the approval mode for tool calls. Available modes:\n     * default: Prompt for approval on each tool call (default behavior)\n     * auto_edit: Automatically approve edit tools (replace, write_file) while\n       prompting for others\n     * yolo: Automatically approve all tool calls (equivalent to --yolo)\n   * Cannot be used together with --yolo. Use --approval-mode=yolo instead of\n     --yolo for the new unified approach.\n   * Example: gemini --approval-mode auto_edit\n * --allowed-tools <tool1,tool2,...>:\n   * A comma-separated list of tool names that will bypass the confirmation\n     dialog.\n   * Example: gemini --allowed-tools \"ShellTool(git status)\"\n * --telemetry:\n   * Enables telemetry.\n * --telemetry-target:\n   * Sets the telemetry target. See telemetry for more information.\n * --telemetry-otlp-endpoint:\n   * Sets the OTLP endpoint for telemetry. See telemetry for more information.\n * --telemetry-otlp-protocol:\n   * Sets the OTLP protocol for telemetry (grpc or http). Defaults to grpc. See\n     telemetry for more information.\n * --telemetry-log-prompts:\n   * Enables logging of prompts for telemetry. See telemetry for more\n     information.\n * --checkpointing:\n   * Enables checkpointing.\n * --extensions <extension_name ...> (-e <extension_name ...>):\n   * Specifies a list of extensions to use for the session. If not provided, all\n     available extensions are used.\n   * Use the special term gemini -e none to disable all extensions.\n   * Example: gemini -e my-extension -e my-other-extension\n * --list-extensions (-l):\n   * Lists all available extensions and exits.\n * --proxy:\n   * Sets the proxy for the CLI.\n   * Example: --proxy http://localhost:7890.\n * --include-directories <dir1,dir2,...>:\n   * Includes additional directories in the workspace for multi-directory\n     support.\n   * Can be specified multiple times or as comma-separated values.\n   * 5 directories can be added at maximum.\n   * Example: --include-directories /path/to/project1,/path/to/project2 or\n     --include-directories /path/to/project1 --include-directories\n     /path/to/project2\n * --screen-reader:\n   * Enables screen reader mode for accessibility.\n * --version:\n   * Displays the version of the CLI.\n\n\nContext Files (Hierarchical Instructional Context)#\n\nWhile not strictly configuration for the CLI's behavior, context files\n(defaulting to GEMINI.md but configurable via the contextFileName setting) are\ncrucial for configuring the instructional context (also referred to as \"memory\")\nprovided to the Gemini model. This powerful feature allows you to give\nproject-specific instructions, coding style guides, or any relevant background\ninformation to the AI, making its responses more tailored and accurate to your\nneeds. The CLI includes UI elements, such as an indicator in the footer showing\nthe number of loaded context files, to keep you informed about the active\ncontext.\n\n * Purpose: These Markdown files contain instructions, guidelines, or context\n   that you want the Gemini model to be aware of during your interactions. The\n   system is designed to manage this instructional context hierarchically.\n\n\nExample Context File Content (e.g., GEMINI.md)#\n\nHere's a conceptual example of what a context file at the root of a TypeScript\nproject might contain:\n\n\n\nThis example demonstrates how you can provide general project context, specific\ncoding conventions, and even notes about particular files or components. The\nmore relevant and precise your context files are, the better the AI can assist\nyou. Project-specific context files are highly encouraged to establish\nconventions and context.\n\n * Hierarchical Loading and Precedence: The CLI implements a sophisticated\n   hierarchical memory system by loading context files (e.g., GEMINI.md) from\n   several locations. Content from files lower in this list (more specific)\n   typically overrides or supplements content from files higher up (more\n   general). The exact concatenation order and final context can be inspected\n   using the /memory show command. The typical loading order is:\n   1. Global Context File:\n      * Location: ~/.ve/<contextFileName> (e.g., ~/.ve/GEMINI.md in your user\n        home directory).\n      * Scope: Provides default instructions for all your projects.\n   2. Project Root & Ancestors Context Files:\n      * Location: The CLI searches for the configured context file in the\n        current working directory and then in each parent directory up to either\n        the project root (identified by a .git folder) or your home directory.\n      * Scope: Provides context relevant to the entire project or a significant\n        portion of it.\n   3. Sub-directory Context Files (Contextual/Local):\n      * Location: The CLI also scans for the configured context file in\n        subdirectories below the current working directory (respecting common\n        ignore patterns like node_modules, .git, etc.). The breadth of this\n        search is limited to 200 directories by default, but can be configured\n        with a memoryDiscoveryMaxDirs field in your settings.json file.\n      * Scope: Allows for highly specific instructions relevant to a particular\n        component, module, or subsection of your project.\n * Concatenation & UI Indication: The contents of all found context files are\n   concatenated (with separators indicating their origin and path) and provided\n   as part of the system prompt to the Gemini model. The CLI footer displays the\n   count of loaded context files, giving you a quick visual cue about the active\n   instructional context.\n * Importing Content: You can modularize your context files by importing other\n   Markdown files using the @path/to/file.md syntax. For more details, see the\n   Memory Import Processor documentation.\n * Commands for Memory Management:\n   * Use /memory refresh to force a re-scan and reload of all context files from\n     all configured locations. This updates the AI's instructional context.\n   * Use /memory show to display the combined instructional context currently\n     loaded, allowing you to verify the hierarchy and content being used by the\n     AI.\n   * See the Commands documentation for full details on the /memory command and\n     its sub-commands (show and refresh).\n\nBy understanding and utilizing these configuration layers and the hierarchical\nnature of context files, you can effectively manage the AI's memory and tailor\nthe VeCLI's responses to your specific needs and projects.\n\n\nSandboxing#\n\nThe VeCLI can execute potentially unsafe operations (like shell commands and\nfile modifications) within a sandboxed environment to protect your system.\n\nSandboxing is disabled by default, but you can enable it in a few ways:\n\n * Using --sandbox or -s flag.\n * Setting GEMINI_SANDBOX environment variable.\n * Sandbox is enabled when using --yolo or --approval-mode=yolo by default.\n\nBy default, it uses a pre-built gemini-cli-sandbox Docker image.\n\nFor project-specific sandboxing needs, you can create a custom Dockerfile at\n.ve/sandbox.Dockerfile in your project's root directory. This Dockerfile can be\nbased on the base sandbox image:\n\n\n\nWhen .ve/sandbox.Dockerfile exists, you can use BUILD_SANDBOX environment\nvariable when running VeCLI to automatically build the custom sandbox image:\n\n\n\n\nUsage Statistics#\n\nTo help us improve the VeCLI, we collect anonymized usage statistics. This data\nhelps us understand how the CLI is used, identify common issues, and prioritize\nnew features.\n\nWhat we collect:\n\n * Tool Calls: We log the names of the tools that are called, whether they\n   succeed or fail, and how long they take to execute. We do not collect the\n   arguments passed to the tools or any data returned by them.\n * API Requests: We log the Gemini model used for each request, the duration of\n   the request, and whether it was successful. We do not collect the content of\n   the prompts or responses.\n * Session Information: We collect information about the configuration of the\n   CLI, such as the enabled tools and the approval mode.\n\nWhat we DON'T collect:\n\n * Personally Identifiable Information (PII): We do not collect any personal\n   information, such as your name, email address, or API keys.\n * Prompt and Response Content: We do not log the content of your prompts or the\n   responses from the Gemini model.\n * File Content: We do not log the content of any files that are read or written\n   by the CLI.\n\nHow to opt out:\n\nYou can opt out of usage statistics collection at any time by setting the\nusageStatisticsEnabled property to false in your settings.json file:\n\n","routePath":"/cli/configuration-v1","lang":"","toc":[{"text":"Configuration layers","id":"configuration-layers","depth":2,"charIndex":630},{"text":"Settings files","id":"settings-files","depth":2,"charIndex":1321},{"text":"The `.ve` directory in your project","id":"the-ve-directory-in-your-project","depth":3,"charIndex":-1},{"text":"Available settings in `settings.json`:","id":"available-settings-in-settingsjson","depth":3,"charIndex":-1},{"text":"Troubleshooting File Search Performance","id":"troubleshooting-file-search-performance","depth":3,"charIndex":5371},{"text":"Example `settings.json`:","id":"example-settingsjson","depth":3,"charIndex":-1},{"text":"Shell History","id":"shell-history","depth":2,"charIndex":19342},{"text":"Environment Variables & `.env` Files","id":"environment-variables--env-files","depth":2,"charIndex":-1},{"text":"Command-Line Arguments","id":"command-line-arguments","depth":2,"charIndex":24011},{"text":"Context Files (Hierarchical Instructional Context)","id":"context-files-hierarchical-instructional-context","depth":2,"charIndex":27572},{"text":"Example Context File Content (e.g., `GEMINI.md`)","id":"example-context-file-content-eg-geminimd","depth":3,"charIndex":-1},{"text":"Sandboxing","id":"sandboxing","depth":2,"charIndex":31812},{"text":"Usage Statistics","id":"usage-statistics","depth":2,"charIndex":32621}],"domain":"","frontmatter":{},"version":""},{"id":6,"title":"VeCLI Configuration","content":"#\n\nNote on New Configuration Format\n\nThe format of the settings.json file has been updated to a new, more organized\nstructure.\n\n * The new format will be supported in the stable release starting [09/10/25].\n * Automatic migration from the old format to the new format will begin on\n   [09/17/25].\n\nFor details on the previous format, please see the v1 Configuration\ndocumentation.\n\nVeCLI offers several ways to configure its behavior, including environment\nvariables, command-line arguments, and settings files. This document outlines\nthe different configuration methods and available settings.\n\n\nConfiguration layers#\n\nConfiguration is applied in the following order of precedence (lower numbers are\noverridden by higher numbers):\n\n 1. Default values: Hardcoded defaults within the application.\n 2. System defaults file: System-wide default settings that can be overridden by\n    other settings files.\n 3. User settings file: Global settings for the current user.\n 4. Project settings file: Project-specific settings.\n 5. System settings file: System-wide settings that override all other settings\n    files.\n 6. Environment variables: System-wide or session-specific variables,\n    potentially loaded from .env files.\n 7. Command-line arguments: Values passed when launching the CLI.\n\n\nSettings files#\n\nVeCLI uses JSON settings files for persistent configuration. There are four\nlocations for these files:\n\n * System defaults file:\n   * Location: /etc/vecli/system-defaults.json (Linux),\n     C:\\ProgramData\\vecli\\system-defaults.json (Windows) or /Library/Application\n     Support/GeminiCli/system-defaults.json (macOS). The path can be overridden\n     using the VE_CLI_SYSTEM_DEFAULTS_PATH environment variable.\n   * Scope: Provides a base layer of system-wide default settings. These\n     settings have the lowest precedence and are intended to be overridden by\n     user, project, or system override settings.\n * User settings file:\n   * Location: ~/.ve/settings.json (where ~ is your home directory).\n   * Scope: Applies to all VeCLI sessions for the current user. User settings\n     override system defaults.\n * Project settings file:\n   * Location: .ve/settings.json within your project's root directory.\n   * Scope: Applies only when running VeCLI from that specific project. Project\n     settings override user settings and system defaults.\n * System settings file:\n   * Location: /etc/vecli/settings.json (Linux),\n     C:\\ProgramData\\vecli\\settings.json (Windows) or /Library/Application\n     Support/VeCli/settings.json (macOS). The path can be overridden using the\n     VE_CLI_SYSTEM_SETTINGS_PATH environment variable.\n   * Scope: Applies to all VeCLI sessions on the system, for all users. System\n     settings act as overrides, taking precedence over all other settings files.\n     May be useful for system administrators at enterprises to have controls\n     over users' VeCLI setups.\n\nNote on environment variables in settings: String values within your\nsettings.json files can reference environment variables using either $VAR_NAME\nor ${VAR_NAME} syntax. These variables will be automatically resolved when the\nsettings are loaded. For example, if you have an environment variable\nMY_API_TOKEN, you could use it in settings.json like this: \"apiKey\":\n\"$MY_API_TOKEN\".\n\n> Note for Enterprise Users: For guidance on deploying and managing VeCLI in a\n> corporate environment, please see the Enterprise Configuration documentation.\n\n\nThe .ve directory in your project#\n\nIn addition to a project settings file, a project's .ve directory can contain\nother project-specific files related to VeCLI's operation, such as:\n\n * Custom sandbox profiles (e.g., .ve/sandbox-macos-custom.sb,\n   .ve/sandbox.Dockerfile).\n\n\nAvailable settings in settings.json#\n\nSettings are organized into categories. All settings should be placed within\ntheir corresponding top-level category object in your settings.json file.\n\ngeneral#\n\n * general.preferredEditor (string):\n   \n   * Description: The preferred editor to open files in.\n   * Default: undefined\n\n * general.vimMode (boolean):\n   \n   * Description: Enable Vim keybindings.\n   * Default: false\n\n * general.disableAutoUpdate (boolean):\n   \n   * Description: Disable automatic updates.\n   * Default: false\n\n * general.disableUpdateNag (boolean):\n   \n   * Description: Disable update notification prompts.\n   * Default: false\n\n * general.checkpointing.enabled (boolean):\n   \n   * Description: Enable session checkpointing for recovery.\n   * Default: false\n\nui#\n\n * ui.theme (string):\n   \n   * Description: The color theme for the UI. See Themes for available options.\n   * Default: undefined\n\n * ui.customThemes (object):\n   \n   * Description: Custom theme definitions.\n   * Default: {}\n\n * ui.hideWindowTitle (boolean):\n   \n   * Description: Hide the window title bar.\n   * Default: false\n\n * ui.hideTips (boolean):\n   \n   * Description: Hide helpful tips in the UI.\n   * Default: false\n\n * ui.hideBanner (boolean):\n   \n   * Description: Hide the application banner.\n   * Default: false\n\n * ui.hideFooter (boolean):\n   \n   * Description: Hide the footer from the UI.\n   * Default: false\n\n * ui.showMemoryUsage (boolean):\n   \n   * Description: Display memory usage information in the UI.\n   * Default: false\n\n * ui.showLineNumbers (boolean):\n   \n   * Description: Show line numbers in the chat.\n   * Default: false\n\n * ui.showCitations (boolean):\n   \n   * Description: Show citations for generated text in the chat.\n   * Default: false\n\n * ui.accessibility.disableLoadingPhrases (boolean):\n   \n   * Description: Disable loading phrases for accessibility.\n   * Default: false\n\nide#\n\n * ide.enabled (boolean):\n   \n   * Description: Enable IDE integration mode.\n   * Default: false\n\n * ide.hasSeenNudge (boolean):\n   \n   * Description: Whether the user has seen the IDE integration nudge.\n   * Default: false\n\nprivacy#\n\n * privacy.usageStatisticsEnabled (boolean):\n   * Description: Enable collection of usage statistics.\n   * Default: true\n\nmodel#\n\n * model.name (string):\n   \n   * Description: The model to use for conversations.\n   * Default: undefined\n\n * model.maxSessionTurns (number):\n   \n   * Description: Maximum number of user/model/tool turns to keep in a session.\n     -1 means unlimited.\n   * Default: -1\n\n * model.summarizeToolOutput (object):\n   \n   * Description: Enables or disables the summarization of tool output. You can\n     specify the token budget for the summarization using the tokenBudget\n     setting. Note: Currently only the run_shell_command tool is supported. For\n     example {\"run_shell_command\": {\"tokenBudget\": 2000}}\n   * Default: undefined\n\n * model.chatCompression.contextPercentageThreshold (number):\n   \n   * Description: Sets the threshold for chat history compression as a\n     percentage of the model's total token limit. This is a value between 0 and\n     1 that applies to both automatic compression and the manual /compress\n     command. For example, a value of 0.6 will trigger compression when the chat\n     history exceeds 60% of the token limit.\n   * Default: 0.7\n\n * model.skipNextSpeakerCheck (boolean):\n   \n   * Description: Skip the next speaker check.\n   * Default: false\n\ncontext#\n\n * context.fileName (string or array of strings):\n   \n   * Description: The name of the context file(s).\n   * Default: undefined\n\n * context.importFormat (string):\n   \n   * Description: The format to use when importing memory.\n   * Default: undefined\n\n * context.discoveryMaxDirs (number):\n   \n   * Description: Maximum number of directories to search for memory.\n   * Default: 200\n\n * context.includeDirectories (array):\n   \n   * Description: Additional directories to include in the workspace context.\n     Missing directories will be skipped with a warning.\n   * Default: []\n\n * context.loadFromIncludeDirectories (boolean):\n   \n   * Description: Controls the behavior of the /memory refresh command. If set\n     to true, VE.md files should be loaded from all directories that are added.\n     If set to false, VE.md should only be loaded from the current directory.\n   * Default: false\n\n * context.fileFiltering.respectGitIgnore (boolean):\n   \n   * Description: Respect .gitignore files when searching.\n   * Default: true\n\n * context.fileFiltering.respectGeminiIgnore (boolean):\n   \n   * Description: Respect .veignore files when searching.\n   * Default: true\n\n * context.fileFiltering.enableRecursiveFileSearch (boolean):\n   \n   * Description: Whether to enable searching recursively for filenames under\n     the current tree when completing @ prefixes in the prompt.\n   * Default: true\n\ntools#\n\n * tools.sandbox (boolean or string):\n   \n   * Description: Sandbox execution environment (can be a boolean or a path\n     string).\n   * Default: undefined\n\n * tools.usePty (boolean):\n   \n   * Description: Use node-pty for shell command execution. Fallback to\n     child_process still applies.\n   * Default: false\n\n * tools.core (array of strings):\n   \n   * Description: This can be used to restrict the set of built-in tools with an\n     allowlist. See Built-in Tools for a list of core tools. The match semantics\n     are the same as tools.allowed.\n   * Default: undefined\n\n * tools.exclude (array of strings):\n   \n   * Description: Tool names to exclude from discovery.\n   * Default: undefined\n\n * tools.allowed (array of strings):\n   \n   * Description: A list of tool names that will bypass the confirmation dialog.\n     This is useful for tools that you trust and use frequently. For example,\n     [\"run_shell_command(git)\", \"run_shell_command(npm test)\"] will skip the\n     confirmation dialog to run any git and npm test commands. See Shell Tool\n     command restrictions for details on prefix matching, command chaining, etc.\n   * Default: undefined\n\n * tools.discoveryCommand (string):\n   \n   * Description: Command to run for tool discovery.\n   * Default: undefined\n\n * tools.callCommand (string):\n   \n   * Description: Defines a custom shell command for calling a specific tool\n     that was discovered using tools.discoveryCommand. The shell command must\n     meet the following criteria:\n     * It must take function name (exactly as in function declaration) as first\n       command line argument.\n     * It must read function arguments as JSON on stdin, analogous to\n       functionCall.args.\n     * It must return function output as JSON on stdout, analogous to\n       functionResponse.response.content.\n   * Default: undefined\n\nmcp#\n\n * mcp.serverCommand (string):\n   \n   * Description: Command to start an MCP server.\n   * Default: undefined\n\n * mcp.allowed (array of strings):\n   \n   * Description: An allowlist of MCP servers to allow.\n   * Default: undefined\n\n * mcp.excluded (array of strings):\n   \n   * Description: A denylist of MCP servers to exclude.\n   * Default: undefined\n\nsecurity#\n\n * security.folderTrust.enabled (boolean):\n   \n   * Description: Setting to track whether Folder trust is enabled.\n   * Default: false\n\n * security.auth.selectedType (string):\n   \n   * Description: The currently selected authentication type.\n   * Default: undefined\n\n * security.auth.enforcedType (string):\n   \n   * Description: The required auth type (useful for enterprises).\n   * Default: undefined\n\n * security.auth.useExternal (boolean):\n   \n   * Description: Whether to use an external authentication flow.\n   * Default: undefined\n\nadvanced#\n\n * advanced.autoConfigureMemory (boolean):\n   \n   * Description: Automatically configure Node.js memory limits.\n   * Default: false\n\n * advanced.dnsResolutionOrder (string):\n   \n   * Description: The DNS resolution order.\n   * Default: undefined\n\n * advanced.excludedEnvVars (array of strings):\n   \n   * Description: Environment variables to exclude from project context.\n   * Default: [\"DEBUG\",\"DEBUG_MODE\"]\n\n * advanced.bugCommand (object):\n   \n   * Description: Configuration for the bug report command.\n   * Default: undefined\n\nmcpServers#\n\nConfigures connections to one or more Model-Context Protocol (MCP) servers for\ndiscovering and using custom tools. VeCLI attempts to connect to each configured\nMCP server to discover available tools. If multiple MCP servers expose a tool\nwith the same name, the tool names will be prefixed with the server alias you\ndefined in the configuration (e.g., serverAlias__actualToolName) to avoid\nconflicts. Note that the system might strip certain schema properties from MCP\ntool definitions for compatibility. At least one of command, url, or httpUrl\nmust be provided. If multiple are specified, the order of precedence is httpUrl,\nthen url, then command.\n\n * mcpServers.<SERVER_NAME> (object): The server parameters for the named\n   server.\n   * command (string, optional): The command to execute to start the MCP server\n     via standard I/O.\n   * args (array of strings, optional): Arguments to pass to the command.\n   * env (object, optional): Environment variables to set for the server\n     process.\n   * cwd (string, optional): The working directory in which to start the server.\n   * url (string, optional): The URL of an MCP server that uses Server-Sent\n     Events (SSE) for communication.\n   * httpUrl (string, optional): The URL of an MCP server that uses streamable\n     HTTP for communication.\n   * headers (object, optional): A map of HTTP headers to send with requests to\n     url or httpUrl.\n   * timeout (number, optional): Timeout in milliseconds for requests to this\n     MCP server.\n   * trust (boolean, optional): Trust this server and bypass all tool call\n     confirmations.\n   * description (string, optional): A brief description of the server, which\n     may be used for display purposes.\n   * includeTools (array of strings, optional): List of tool names to include\n     from this MCP server. When specified, only the tools listed here will be\n     available from this server (allowlist behavior). If not specified, all\n     tools from the server are enabled by default.\n   * excludeTools (array of strings, optional): List of tool names to exclude\n     from this MCP server. Tools listed here will not be available to the model,\n     even if they are exposed by the server. Note: excludeTools takes precedence\n     over includeTools - if a tool is in both lists, it will be excluded.\n\ntelemetry#\n\nConfigures logging and metrics collection for VeCLI. For more information, see\nTelemetry.\n\n * Properties:\n   * enabled (boolean): Whether or not telemetry is enabled.\n   * target (string): The destination for collected telemetry. Supported values\n     are local and gcp.\n   * otlpEndpoint (string): The endpoint for the OTLP Exporter.\n   * otlpProtocol (string): The protocol for the OTLP Exporter (grpc or http).\n   * logPrompts (boolean): Whether or not to include the content of user prompts\n     in the logs.\n   * outfile (string): The file to write telemetry to when target is local.\n\n\nExample settings.json#\n\nHere is an example of a settings.json file with the nested structure, new as of\nv0.3.0:\n\n\n\n\nShell History#\n\nThe CLI keeps a history of shell commands you run. To avoid conflicts between\ndifferent projects, this history is stored in a project-specific directory\nwithin your user's home folder.\n\n * Location: ~/.ve/tmp/<project_hash>/shell_history\n   * <project_hash> is a unique identifier generated from your project's root\n     path.\n   * The history is stored in a file named shell_history.\n\n\nEnvironment Variables & .env Files#\n\nEnvironment variables are a common way to configure applications, especially for\nsensitive information like API keys or for settings that might change between\nenvironments. For authentication setup, see the Authentication documentation\nwhich covers all available authentication methods.\n\nThe CLI automatically loads environment variables from an .env file. The loading\norder is:\n\n 1. .env file in the current working directory.\n 2. If not found, it searches upwards in parent directories until it finds an\n    .env file or reaches the project root (identified by a .git folder) or the\n    home directory.\n 3. If still not found, it looks for ~/.env (in the user's home directory).\n\nEnvironment Variable Exclusion: Some environment variables (like DEBUG and\nDEBUG_MODE) are automatically excluded from being loaded from project .env files\nto prevent interference with vecli behavior. Variables from .ve/.env files are\nnever excluded. You can customize this behavior using the\nadvanced.excludedEnvVars setting in your settings.json file.\n\n * GEMINI_API_KEY:\n   * Your API key for the Volcano Engine API.\n   * One of several available authentication methods.\n   * Set this in your shell profile (e.g., ~/.bashrc, ~/.zshrc) or an .env file.\n * GEMINI_MODEL:\n   * Specifies the default Volcano Engine model to use.\n   * Overrides the hardcoded default\n   * Example: export VECLI_MODEL=\"deepseek-v3-1\"\n * GOOGLE_API_KEY:\n   * Your Volcano Engine API key.\n   * Required for using Vertex AI in express mode.\n   * Ensure you have the necessary permissions.\n   * Example: export ARK_API_KEY=\"YOUR_Ark_API_KEY\".\n * GOOGLE_CLOUD_PROJECT:\n   * Your Volcano Engine Project ID.\n   * Required for using Code Assist or Vertex AI.\n   * If using Vertex AI, ensure you have the necessary permissions in this\n     project.\n   * Cloud Shell Note: When running in a Cloud Shell environment, this variable\n     defaults to a special project allocated for Cloud Shell users. If you have\n     VOLCANO_ENGINE_PROJECT set in your global environment in Cloud Shell, it\n     will be overridden by this default. To use a different project in Cloud\n     Shell, you must define VOLCANO_ENGINE_PROJECT in a .env file.\n   * Example: export VOLCANO_ENGINE_PROJECT=\"YOUR_PROJECT_ID\".\n * GOOGLE_APPLICATION_CREDENTIALS (string):\n   * Description: The path to your Volcano Engine Application Credentials JSON\n     file.\n   * Example: export\n     VOLCANO_ENGINE_APPLICATION_CREDENTIALS=\"/path/to/your/credentials.json\"\n * OTLP_VOLCANO_ENGINE_CLOUD_PROJECT:\n   * Your Volcano Engine Project ID for Telemetry in Volcano Engine\n   * Example: export OTLP_VOLCANO_ENGINE_PROJECT=\"YOUR_PROJECT_ID\".\n * VOLCANO_ENGINE_LOCATION:\n   * Your Volcano Engine Project Location (e.g., us-central1).\n   * Required for using Vertex AI in non express mode.\n   * Example: export VOLCANO_ENGINE_LOCATION=\"YOUR_PROJECT_LOCATION\".\n * GEMINI_SANDBOX:\n   * Alternative to the sandbox setting in settings.json.\n   * Accepts true, false, docker, podman, or a custom command string.\n * SEATBELT_PROFILE (macOS specific):\n   * Switches the Seatbelt (sandbox-exec) profile on macOS.\n   * permissive-open: (Default) Restricts writes to the project folder (and a\n     few other folders, see\n     packages/cli/src/utils/sandbox-macos-permissive-open.sb) but allows other\n     operations.\n   * strict: Uses a strict profile that declines operations by default.\n   * <profile_name>: Uses a custom profile. To define a custom profile, create a\n     file named sandbox-macos-<profile_name>.sb in your project's .ve/ directory\n     (e.g., my-project/.ve/sandbox-macos-custom.sb).\n * DEBUG or DEBUG_MODE (often used by underlying libraries or the CLI itself):\n   * Set to true or 1 to enable verbose debug logging, which can be helpful for\n     troubleshooting.\n   * Note: These variables are automatically excluded from project .env files by\n     default to prevent interference with vecli behavior. Use .ve/.env files if\n     you need to set these for vecli specifically.\n * NO_COLOR:\n   * Set to any value to disable all color output in the CLI.\n * CLI_TITLE:\n   * Set to a string to customize the title of the CLI.\n * CODE_ASSIST_ENDPOINT:\n   * Specifies the endpoint for the code assist server.\n   * This is useful for development and testing.\n\n\nCommand-Line Arguments#\n\nArguments passed directly when running the CLI can override other configurations\nfor that specific session.\n\n * --model <model_name> (-m <model_name>):\n   * Specifies the Volcano Engine model to use for this session.\n   * Example: npm start -- --model deepseek-v3-1\n * --prompt <your_prompt> (-p <your_prompt>):\n   * Used to pass a prompt directly to the command. This invokes VeCLI in a\n     non-interactive mode.\n * --prompt-interactive <your_prompt> (-i <your_prompt>):\n   * Starts an interactive session with the provided prompt as the initial\n     input.\n   * The prompt is processed within the interactive session, not before it.\n   * Cannot be used when piping input from stdin.\n   * Example: vecli -i \"explain this code\"\n * --sandbox (-s):\n   * Enables sandbox mode for this session.\n * --sandbox-image:\n   * Sets the sandbox image URI.\n * --debug (-d):\n   * Enables debug mode for this session, providing more verbose output.\n * --all-files (-a):\n   * If set, recursively includes all files within the current directory as\n     context for the prompt.\n * --help (or -h):\n   * Displays help information about command-line arguments.\n * --show-memory-usage:\n   * Displays the current memory usage.\n * --yolo:\n   * Enables YOLO mode, which automatically approves all tool calls.\n * --approval-mode <mode>:\n   * Sets the approval mode for tool calls. Available modes:\n     * default: Prompt for approval on each tool call (default behavior)\n     * auto_edit: Automatically approve edit tools (replace, write_file) while\n       prompting for others\n     * yolo: Automatically approve all tool calls (equivalent to --yolo)\n   * Cannot be used together with --yolo. Use --approval-mode=yolo instead of\n     --yolo for the new unified approach.\n   * Example: vecli --approval-mode auto_edit\n * --allowed-tools <tool1,tool2,...>:\n   * A comma-separated list of tool names that will bypass the confirmation\n     dialog.\n   * Example: vecli --allowed-tools \"ShellTool(git status)\"\n * --telemetry:\n   * Enables telemetry.\n * --telemetry-target:\n   * Sets the telemetry target. See telemetry for more information.\n * --telemetry-otlp-endpoint:\n   * Sets the OTLP endpoint for telemetry. See telemetry for more information.\n * --telemetry-otlp-protocol:\n   * Sets the OTLP protocol for telemetry (grpc or http). Defaults to grpc. See\n     telemetry for more information.\n * --telemetry-log-prompts:\n   * Enables logging of prompts for telemetry. See telemetry for more\n     information.\n * --checkpointing:\n   * Enables checkpointing.\n * --extensions <extension_name ...> (-e <extension_name ...>):\n   * Specifies a list of extensions to use for the session. If not provided, all\n     available extensions are used.\n   * Use the special term vecli -e none to disable all extensions.\n   * Example: vecli -e my-extension -e my-other-extension\n * --list-extensions (-l):\n   * Lists all available extensions and exits.\n * --proxy:\n   * Sets the proxy for the CLI.\n   * Example: --proxy http://localhost:7890.\n * --include-directories <dir1,dir2,...>:\n   * Includes additional directories in the workspace for multi-directory\n     support.\n   * Can be specified multiple times or as comma-separated values.\n   * 5 directories can be added at maximum.\n   * Example: --include-directories /path/to/project1,/path/to/project2 or\n     --include-directories /path/to/project1 --include-directories\n     /path/to/project2\n * --screen-reader:\n   * Enables screen reader mode, which adjusts the TUI for better compatibility\n     with screen readers.\n * --version:\n   * Displays the version of the CLI.\n\n\nContext Files (Hierarchical Instructional Context)#\n\nWhile not strictly configuration for the CLI's behavior, context files\n(defaulting to vecli.md but configurable via the context.fileName setting) are\ncrucial for configuring the instructional context (also referred to as \"memory\")\nprovided to the Volcano Engine model. This powerful feature allows you to give\nproject-specific instructions, coding style guides, or any relevant background\ninformation to the AI, making its responses more tailored and accurate to your\nneeds. The CLI includes UI elements, such as an indicator in the footer showing\nthe number of loaded context files, to keep you informed about the active\ncontext.\n\n * Purpose: These Markdown files contain instructions, guidelines, or context\n   that you want the Volcano Engine model to be aware of during your\n   interactions. The system is designed to manage this instructional context\n   hierarchically.\n\n\nExample Context File Content (e.g., VE.md)#\n\nHere's a conceptual example of what a context file at the root of a TypeScript\nproject might contain:\n\n\n\nThis example demonstrates how you can provide general project context, specific\ncoding conventions, and even notes about particular files or components. The\nmore relevant and precise your context files are, the better the AI can assist\nyou. Project-specific context files are highly encouraged to establish\nconventions and context.\n\n * Hierarchical Loading and Precedence: The CLI implements a sophisticated\n   hierarchical memory system by loading context files (e.g., VE.md) from\n   several locations. Content from files lower in this list (more specific)\n   typically overrides or supplements content from files higher up (more\n   general). The exact concatenation order and final context can be inspected\n   using the /memory show command. The typical loading order is:\n   1. Global Context File:\n      * Location: ~/.ve/<configured-context-filename> (e.g., ~/.ve/VE.md in your\n        user home directory).\n      * Scope: Provides default instructions for all your projects.\n   2. Project Root & Ancestors Context Files:\n      * Location: The CLI searches for the configured context file in the\n        current working directory and then in each parent directory up to either\n        the project root (identified by a .git folder) or your home directory.\n      * Scope: Provides context relevant to the entire project or a significant\n        portion of it.\n   3. Sub-directory Context Files (Contextual/Local):\n      * Location: The CLI also scans for the configured context file in\n        subdirectories below the current working directory (respecting common\n        ignore patterns like node_modules, .git, etc.). The breadth of this\n        search is limited to 200 directories by default, but can be configured\n        with the context.discoveryMaxDirs setting in your settings.json file.\n      * Scope: Allows for highly specific instructions relevant to a particular\n        component, module, or subsection of your project.\n * Concatenation & UI Indication: The contents of all found context files are\n   concatenated (with separators indicating their origin and path) and provided\n   as part of the system prompt to the Volcano Engine model. The CLI footer\n   displays the count of loaded context files, giving you a quick visual cue\n   about the active instructional context.\n * Importing Content: You can modularize your context files by importing other\n   Markdown files using the @path/to/file.md syntax. For more details, see the\n   Memory Import Processor documentation.\n * Commands for Memory Management:\n   * Use /memory refresh to force a re-scan and reload of all context files from\n     all configured locations. This updates the AI's instructional context.\n   * Use /memory show to display the combined instructional context currently\n     loaded, allowing you to verify the hierarchy and content being used by the\n     AI.\n   * See the Commands documentation for full details on the /memory command and\n     its sub-commands (show and refresh).\n\nBy understanding and utilizing these configuration layers and the hierarchical\nnature of context files, you can effectively manage the AI's memory and tailor\nthe VeCLI's responses to your specific needs and projects.\n\n\nSandboxing#\n\nThe VeCLI can execute potentially unsafe operations (like shell commands and\nfile modifications) within a sandboxed environment to protect your system.\n\nSandboxing is disabled by default, but you can enable it in a few ways:\n\n * Using --sandbox or -s flag.\n * Setting VECLI_SANDBOX environment variable.\n * Sandbox is enabled when using --yolo or --approval-mode=yolo by default.\n\nBy default, it uses a pre-built vecli-sandbox Docker image.\n\nFor project-specific sandboxing needs, you can create a custom Dockerfile at\n.ve/sandbox.Dockerfile in your project's root directory. This Dockerfile can be\nbased on the base sandbox image:\n\n\n\nWhen .ve/sandbox.Dockerfile exists, you can use BUILD_SANDBOX environment\nvariable when running VeCLI to automatically build the custom sandbox image:\n\n\n\n\nUsage Statistics#\n\nTo help us improve the VeCLI, we collect anonymized usage statistics. This data\nhelps us understand how the CLI is used, identify common issues, and prioritize\nnew features.\n\nWhat we collect:\n\n * Tool Calls: We log the names of the tools that are called, whether they\n   succeed or fail, and how long they take to execute. We do not collect the\n   arguments passed to the tools or any data returned by them.\n * API Requests: We log the Volcano Engine model used for each request, the\n   duration of the request, and whether it was successful. We do not collect the\n   content of the prompts or responses.\n * Session Information: We collect information about the configuration of the\n   CLI, such as the enabled tools and the approval mode.\n\nWhat we DON'T collect:\n\n * Personally Identifiable Information (PII): We do not collect any personal\n   information, such as your name, email address, or API keys.\n * Prompt and Response Content: We do not log the content of your prompts or the\n   responses from the Volcano Engine model.\n * File Content: We do not log the content of any files that are read or written\n   by the CLI.\n\nHow to opt out:\n\nYou can opt out of usage statistics collection at any time by setting the\nusageStatisticsEnabled property to false under the privacy category in your\nsettings.json file:\n\n","routePath":"/cli/configuration","lang":"","toc":[{"text":"Configuration layers","id":"configuration-layers","depth":2,"charIndex":596},{"text":"Settings files","id":"settings-files","depth":2,"charIndex":1287},{"text":"The `.ve` directory in your project","id":"the-ve-directory-in-your-project","depth":3,"charIndex":-1},{"text":"Available settings in `settings.json`","id":"available-settings-in-settingsjson","depth":3,"charIndex":-1},{"text":"`general`","id":"general","depth":4,"charIndex":-1},{"text":"`ui`","id":"ui","depth":4,"charIndex":-1},{"text":"`ide`","id":"ide","depth":4,"charIndex":-1},{"text":"`privacy`","id":"privacy","depth":4,"charIndex":-1},{"text":"`model`","id":"model","depth":4,"charIndex":-1},{"text":"`context`","id":"context","depth":4,"charIndex":-1},{"text":"`tools`","id":"tools","depth":4,"charIndex":-1},{"text":"`mcp`","id":"mcp","depth":4,"charIndex":-1},{"text":"`security`","id":"security","depth":4,"charIndex":-1},{"text":"`advanced`","id":"advanced","depth":4,"charIndex":-1},{"text":"`mcpServers`","id":"mcpservers","depth":4,"charIndex":-1},{"text":"`telemetry`","id":"telemetry","depth":4,"charIndex":-1},{"text":"Example `settings.json`","id":"example-settingsjson","depth":3,"charIndex":-1},{"text":"Shell History","id":"shell-history","depth":2,"charIndex":14915},{"text":"Environment Variables & `.env` Files","id":"environment-variables--env-files","depth":2,"charIndex":-1},{"text":"Command-Line Arguments","id":"command-line-arguments","depth":2,"charIndex":19628},{"text":"Context Files (Hierarchical Instructional Context)","id":"context-files-hierarchical-instructional-context","depth":2,"charIndex":23239},{"text":"Example Context File Content (e.g., `VE.md`)","id":"example-context-file-content-eg-vemd","depth":3,"charIndex":-1},{"text":"Sandboxing","id":"sandboxing","depth":2,"charIndex":27512},{"text":"Usage Statistics","id":"usage-statistics","depth":2,"charIndex":28315}],"domain":"","frontmatter":{},"version":""},{"id":7,"title":"VeCLI for the Enterprise","content":"#\n\nThis document outlines configuration patterns and best practices for deploying\nand managing VeCLI in an enterprise environment. By leveraging system-level\nsettings, administrators can enforce security policies, manage tool access, and\nensure a consistent experience for all users.\n\n> A Note on Security: The patterns described in this document are intended to\n> help administrators create a more controlled and secure environment for using\n> VeCLI. However, they should not be considered a foolproof security boundary. A\n> determined user with sufficient privileges on their local machine may still be\n> able to circumvent these configurations. These measures are designed to\n> prevent accidental misuse and enforce corporate policy in a managed\n> environment, not to defend against a malicious actor with local administrative\n> rights.\n\n\nCentralized Configuration: The System Settings File#\n\nThe most powerful tools for enterprise administration are the system-wide\nsettings files. These files allow you to define a baseline configuration\n(system-defaults.json) and a set of overrides (settings.json) that apply to all\nusers on a machine. For a complete overview of configuration options, see the\nConfiguration documentation.\n\nSettings are merged from four files. The precedence order for single-value\nsettings (like theme) is:\n\n 1. System Defaults (system-defaults.json)\n 2. User Settings (~/.ve/settings.json)\n 3. Workspace Settings (<project>/.ve/settings.json)\n 4. System Overrides (settings.json)\n\nThis means the System Overrides file has the final say. For settings that are\narrays (includeDirectories) or objects (mcpServers), the values are merged.\n\nExample of Merging and Precedence:\n\nHere is how settings from different levels are combined.\n\n * System Defaults system-defaults.json:\n   \n   \n\n * User settings.json (~/.ve/settings.json):\n   \n   \n\n * Workspace settings.json (<project>/.ve/settings.json):\n   \n   \n\n * System Overrides settings.json:\n   \n   \n\nThis results in the following merged configuration:\n\n * Final Merged Configuration:\n   \n   \n\nWhy:\n\n * theme: The value from the system overrides (system-enforced-theme) is used,\n   as it has the highest precedence.\n\n * mcpServers: The objects are merged. The corp-server definition from the\n   system overrides takes precedence over the user's definition. The unique\n   user-tool and project-tool are included.\n\n * includeDirectories: The arrays are concatenated in the order of System\n   Defaults, User, Workspace, and then System Overrides.\n\n * Location:\n   \n   * Linux: /etc/gemini-cli/settings.json\n   * Windows: C:\\ProgramData\\gemini-cli\\settings.json\n   * macOS: /Library/Application Support/GeminiCli/settings.json\n   * The path can be overridden using the GEMINI_CLI_SYSTEM_SETTINGS_PATH\n     environment variable.\n\n * Control: This file should be managed by system administrators and protected\n   with appropriate file permissions to prevent unauthorized modification by\n   users.\n\nBy using the system settings file, you can enforce the security and\nconfiguration patterns described below.\n\n\nRestricting Tool Access#\n\nYou can significantly enhance security by controlling which tools the Gemini\nmodel can use. This is achieved through the tools.core and tools.exclude\nsettings. For a list of available tools, see the Tools documentation.\n\n\nAllowlisting with coreTools#\n\nThe most secure approach is to explicitly add the tools and commands that users\nare permitted to execute to an allowlist. This prevents the use of any tool not\non the approved list.\n\nExample: Allow only safe, read-only file operations and listing files.\n\n\n\n\nBlocklisting with excludeTools#\n\nAlternatively, you can add specific tools that are considered dangerous in your\nenvironment to a blocklist.\n\nExample: Prevent the use of the shell tool for removing files.\n\n\n\nSecurity Note: Blocklisting with excludeTools is less secure than allowlisting\nwith coreTools, as it relies on blocking known-bad commands, and clever users\nmay find ways to bypass simple string-based blocks. Allowlisting is the\nrecommended approach.\n\n\nManaging Custom Tools (MCP Servers)#\n\nIf your organization uses custom tools via Model-Context Protocol (MCP) servers,\nit is crucial to understand how server configurations are managed to apply\nsecurity policies effectively.\n\n\nHow MCP Server Configurations are Merged#\n\nVeCLI loads settings.json files from three levels: System, Workspace, and User.\nWhen it comes to the mcpServers object, these configurations are merged:\n\n 1. Merging: The lists of servers from all three levels are combined into a\n    single list.\n 2. Precedence: If a server with the same name is defined at multiple levels\n    (e.g., a server named corp-api exists in both system and user settings), the\n    definition from the highest-precedence level is used. The order of\n    precedence is: System > Workspace > User.\n\nThis means a user cannot override the definition of a server that is already\ndefined in the system-level settings. However, they can add new servers with\nunique names.\n\n\nEnforcing a Catalog of Tools#\n\nThe security of your MCP tool ecosystem depends on a combination of defining the\ncanonical servers and adding their names to an allowlist.\n\n\nRestricting Tools Within an MCP Server#\n\nFor even greater security, especially when dealing with third-party MCP servers,\nyou can restrict which specific tools from a server are exposed to the model.\nThis is done using the includeTools and excludeTools properties within a\nserver's definition. This allows you to use a subset of tools from a server\nwithout allowing potentially dangerous ones.\n\nFollowing the principle of least privilege, it is highly recommended to use\nincludeTools to create an allowlist of only the necessary tools.\n\nExample: Only allow the code-search and get-ticket-details tools from a\nthird-party MCP server, even if the server offers other tools like\ndelete-ticket.\n\n\n\nMore Secure Pattern: Define and Add to Allowlist in System Settings#\n\nTo create a secure, centrally-managed catalog of tools, the system administrator\nmust do both of the following in the system-level settings.json file:\n\n 1. Define the full configuration for every approved server in the mcpServers\n    object. This ensures that even if a user defines a server with the same\n    name, the secure system-level definition will take precedence.\n 2. Add the names of those servers to an allowlist using the mcp.allowed\n    setting. This is a critical security step that prevents users from running\n    any servers that are not on this list. If this setting is omitted, the CLI\n    will merge and allow any server defined by the user.\n\nExample System settings.json:\n\n 1. Add the names of all approved servers to an allowlist. This will prevent\n    users from adding their own servers.\n\n 2. Provide the canonical definition for each server on the allowlist.\n\n\n\nThis pattern is more secure because it uses both definition and an allowlist.\nAny server a user defines will either be overridden by the system definition (if\nit has the same name) or blocked because its name is not in the mcp.allowed\nlist.\n\n\nLess Secure Pattern: Omitting the Allowlist#\n\nIf the administrator defines the mcpServers object but fails to also specify the\nmcp.allowed allowlist, users may add their own servers.\n\nExample System settings.json:\n\nThis configuration defines servers but does not enforce the allowlist. The\nadministrator has NOT included the \"mcp.allowed\" setting.\n\n\n\nIn this scenario, a user can add their own server in their local settings.json.\nBecause there is no mcp.allowed list to filter the merged results, the user's\nserver will be added to the list of available tools and allowed to run.\n\n\nEnforcing Sandboxing for Security#\n\nTo mitigate the risk of potentially harmful operations, you can enforce the use\nof sandboxing for all tool execution. The sandbox isolates tool execution in a\ncontainerized environment.\n\nExample: Force all tool execution to happen within a Docker sandbox.\n\n\n\nYou can also specify a custom, hardened Docker image for the sandbox using the\n--sandbox-image command-line argument or by building a custom sandbox.Dockerfile\nas described in the Sandboxing documentation.\n\n\nControlling Network Access via Proxy#\n\nIn corporate environments with strict network policies, you can configure VeCLI\nto route all outbound traffic through a corporate proxy. This can be set via an\nenvironment variable, but it can also be enforced for custom tools via the\nmcpServers configuration.\n\nExample (for an MCP Server):\n\n\n\n\nTelemetry and Auditing#\n\nFor auditing and monitoring purposes, you can configure VeCLI to send telemetry\ndata to a central location. This allows you to track tool usage and other\nevents. For more information, see the telemetry documentation.\n\nExample: Enable telemetry and send it to a local OTLP collector. If otlpEndpoint\nis not specified, it defaults to http://localhost:4317.\n\n\n\nNote: Ensure that logPrompts is set to false in an enterprise setting to avoid\ncollecting potentially sensitive information from user prompts.\n\n\nAuthentication#\n\nYou can enforce a specific authentication method for all users by setting the\nenforcedAuthType in the system-level settings.json file. This prevents users\nfrom choosing a different authentication method. See the Authentication docs for\nmore details.\n\nExample: Enforce the use of Google login for all users.\n\n\n\nIf a user has a different authentication method configured, they will be\nprompted to switch to the enforced method. In non-interactive mode, the CLI will\nexit with an error if the configured authentication method does not match the\nenforced one.\n\n\nPutting It All Together: Example System settings.json#\n\nHere is an example of a system settings.json file that combines several of the\npatterns discussed above to create a secure, controlled environment for VeCLI.\n\n\n\nThis configuration:\n\n * Forces all tool execution into a Docker sandbox.\n * Strictly uses an allowlist for a small set of safe shell commands and file\n   tools.\n * Defines and allows a single corporate MCP server for custom tools.\n * Enables telemetry for auditing, without logging prompt content.\n * Redirects the /bug command to an internal ticketing system.\n * Disables general usage statistics collection.","routePath":"/cli/enterprise","lang":"","toc":[{"text":"Centralized Configuration: The System Settings File","id":"centralized-configuration-the-system-settings-file","depth":2,"charIndex":841},{"text":"Restricting Tool Access","id":"restricting-tool-access","depth":2,"charIndex":3071},{"text":"Allowlisting with `coreTools`","id":"allowlisting-with-coretools","depth":3,"charIndex":-1},{"text":"Blocklisting with `excludeTools`","id":"blocklisting-with-excludetools","depth":3,"charIndex":-1},{"text":"Managing Custom Tools (MCP Servers)","id":"managing-custom-tools-mcp-servers","depth":2,"charIndex":4068},{"text":"How MCP Server Configurations are Merged","id":"how-mcp-server-configurations-are-merged","depth":3,"charIndex":4295},{"text":"Enforcing a Catalog of Tools","id":"enforcing-a-catalog-of-tools","depth":3,"charIndex":5031},{"text":"Restricting Tools Within an MCP Server","id":"restricting-tools-within-an-mcp-server","depth":3,"charIndex":5203},{"text":"More Secure Pattern: Define and Add to Allowlist in System Settings","id":"more-secure-pattern-define-and-add-to-allowlist-in-system-settings","depth":4,"charIndex":5897},{"text":"Less Secure Pattern: Omitting the Allowlist","id":"less-secure-pattern-omitting-the-allowlist","depth":3,"charIndex":7096},{"text":"Enforcing Sandboxing for Security","id":"enforcing-sandboxing-for-security","depth":2,"charIndex":7679},{"text":"Controlling Network Access via Proxy","id":"controlling-network-access-via-proxy","depth":2,"charIndex":8182},{"text":"Telemetry and Auditing","id":"telemetry-and-auditing","depth":2,"charIndex":8516},{"text":"Authentication","id":"authentication","depth":2,"charIndex":9044},{"text":"Putting It All Together: Example System `settings.json`","id":"putting-it-all-together-example-system-settingsjson","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":8,"title":"VeCLI","content":"#\n\nWithin VeCLI, packages/cli is the frontend for users to send and receive prompts\nwith the Gemini AI model and its associated tools. For a general overview of\nVeCLI, see the main documentation page.\n\n\nNavigating this section#\n\n * Authentication: A guide to setting up authentication with Google's AI\n   services.\n * Commands: A reference for VeCLI commands (e.g., /help, /tools, /theme).\n * Configuration: A guide to tailoring VeCLI behavior using configuration files.\n * Enterprise: A guide to enterprise configuration.\n * Token Caching: Optimize API costs through token caching.\n * Themes: A guide to customizing the CLI's appearance with different themes.\n * Tutorials: A tutorial showing how to use VeCLI to automate a development\n   task.\n\n\nNon-interactive mode#\n\nVeCLI can be run in a non-interactive mode, which is useful for scripting and\nautomation. In this mode, you pipe input to the CLI, it executes the command,\nand then it exits.\n\nThe following example pipes a command to VeCLI from your terminal:\n\n\n\nVeCLI executes the command and prints the output to your terminal. Note that you\ncan achieve the same behavior by using the --prompt or -p flag. For example:\n\n","routePath":"/cli/","lang":"","toc":[{"text":"Navigating this section","id":"navigating-this-section","depth":2,"charIndex":202},{"text":"Non-interactive mode","id":"non-interactive-mode","depth":2,"charIndex":747}],"domain":"","frontmatter":{},"version":""},{"id":9,"title":"Themes","content":"#\n\nVeCLI supports a variety of themes to customize its color scheme and appearance.\nYou can change the theme to suit your preferences via the /theme command or\n\"theme\": configuration setting.\n\n\nAvailable Themes#\n\nVeCLI comes with a selection of pre-defined themes, which you can list using the\n/theme command within VeCLI:\n\n * Dark Themes:\n   * ANSI\n   * Atom One\n   * Ayu\n   * Default\n   * Dracula\n   * GitHub\n * Light Themes:\n   * ANSI Light\n   * Ayu Light\n   * Default Light\n   * GitHub Light\n   * Google Code\n   * Xcode\n\n\nChanging Themes#\n\n 1. Enter /theme into VeCLI.\n 2. A dialog or selection prompt appears, listing the available themes.\n 3. Using the arrow keys, select a theme. Some interfaces might offer a live\n    preview or highlight as you select.\n 4. Confirm your selection to apply the theme.\n\nNote: If a theme is defined in your settings.json file (either by name or by a\nfile path), you must remove the \"theme\" setting from the file before you can\nchange the theme using the /theme command.\n\n\nTheme Persistence#\n\nSelected themes are saved in VeCLI's configuration so your preference is\nremembered across sessions.\n\n--------------------------------------------------------------------------------\n\n\nCustom Color Themes#\n\nVeCLI allows you to create your own custom color themes by specifying them in\nyour settings.json file. This gives you full control over the color palette used\nin the CLI.\n\n\nHow to Define a Custom Theme#\n\nAdd a customThemes block to your user, project, or system settings.json file.\nEach custom theme is defined as an object with a unique name and a set of color\nkeys. For example:\n\n\n\nColor keys:\n\n * Background\n * Foreground\n * LightBlue\n * AccentBlue\n * AccentPurple\n * AccentCyan\n * AccentGreen\n * AccentYellow\n * AccentRed\n * Comment\n * Gray\n * DiffAdded (optional, for added lines in diffs)\n * DiffRemoved (optional, for removed lines in diffs)\n * DiffModified (optional, for modified lines in diffs)\n\nRequired Properties:\n\n * name (must match the key in the customThemes object and be a string)\n * type (must be the string \"custom\")\n * Background\n * Foreground\n * LightBlue\n * AccentBlue\n * AccentPurple\n * AccentCyan\n * AccentGreen\n * AccentYellow\n * AccentRed\n * Comment\n * Gray\n\nYou can use either hex codes (e.g., #FF0000) or standard CSS color names (e.g.,\ncoral, teal, blue) for any color value. See CSS color names for a full list of\nsupported names.\n\nYou can define multiple custom themes by adding more entries to the customThemes\nobject.\n\n\nLoading Themes from a File#\n\nIn addition to defining custom themes in settings.json, you can also load a\ntheme directly from a JSON file by specifying the file path in your\nsettings.json. This is useful for sharing themes or keeping them separate from\nyour main configuration.\n\nTo load a theme from a file, set the theme property in your settings.json to the\npath of your theme file:\n\n\n\nThe theme file must be a valid JSON file that follows the same structure as a\ncustom theme defined in settings.json.\n\nExample my-theme.json:\n\n\n\nSecurity Note: For your safety, VeCLI will only load theme files that are\nlocated within your home directory. If you attempt to load a theme from outside\nyour home directory, a warning will be displayed and the theme will not be\nloaded. This is to prevent loading potentially malicious theme files from\nuntrusted sources.\n\n\nExample Custom Theme#\n\n\nUsing Your Custom Theme#\n\n * Select your custom theme using the /theme command in VeCLI. Your custom theme\n   will appear in the theme selection dialog.\n * Or, set it as the default by adding \"theme\": \"MyCustomTheme\" to the ui object\n   in your settings.json.\n * Custom themes can be set at the user, project, or system level, and follow\n   the same configuration precedence as other settings.\n\n--------------------------------------------------------------------------------\n\n\nDark Themes#\n\n\nANSI#\n\n\nAtom OneDark#\n\n\nAyu#\n\n\nDefault#\n\n\nDracula#\n\n\nGitHub#\n\n\nLight Themes#\n\n\nANSI Light#\n\n\nAyu Light#\n\n\nDefault Light#\n\n\nGitHub Light#\n\n\nGoogle Code#\n\n\nXcode#","routePath":"/cli/themes","lang":"","toc":[{"text":"Available Themes","id":"available-themes","depth":2,"charIndex":193},{"text":"Changing Themes","id":"changing-themes","depth":3,"charIndex":525},{"text":"Theme Persistence","id":"theme-persistence","depth":3,"charIndex":1010},{"text":"Custom Color Themes","id":"custom-color-themes","depth":2,"charIndex":1215},{"text":"How to Define a Custom Theme","id":"how-to-define-a-custom-theme","depth":3,"charIndex":1410},{"text":"Loading Themes from a File","id":"loading-themes-from-a-file","depth":3,"charIndex":2492},{"text":"Example Custom Theme","id":"example-custom-theme","depth":3,"charIndex":3347},{"text":"Using Your Custom Theme","id":"using-your-custom-theme","depth":3,"charIndex":3371},{"text":"Dark Themes","id":"dark-themes","depth":2,"charIndex":3849},{"text":"ANSI","id":"ansi","depth":3,"charIndex":3864},{"text":"Atom OneDark","id":"atom-onedark","depth":3,"charIndex":3872},{"text":"Ayu","id":"ayu","depth":3,"charIndex":3888},{"text":"Default","id":"default","depth":3,"charIndex":3895},{"text":"Dracula","id":"dracula","depth":3,"charIndex":3906},{"text":"GitHub","id":"github","depth":3,"charIndex":3917},{"text":"Light Themes","id":"light-themes","depth":2,"charIndex":3927},{"text":"ANSI Light","id":"ansi-light","depth":3,"charIndex":3943},{"text":"Ayu Light","id":"ayu-light","depth":3,"charIndex":3957},{"text":"Default Light","id":"default-light","depth":3,"charIndex":3970},{"text":"GitHub Light","id":"github-light","depth":3,"charIndex":3987},{"text":"Google Code","id":"google-code","depth":3,"charIndex":4003},{"text":"Xcode","id":"xcode","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":10,"title":"Token Caching and Cost Optimization","content":"#\n\nVeCLI automatically optimizes API costs through token caching when using API key\nauthentication (Gemini API key or Vertex AI). This feature reuses previous\nsystem instructions and context to reduce the number of tokens processed in\nsubsequent requests.\n\nToken caching is available for:\n\n * API key users (Gemini API key)\n * Vertex AI users (with project and location setup)\n\nToken caching is not available for:\n\n * OAuth users (Google Personal/Enterprise accounts) - the Code Assist API does\n   not support cached content creation at this time\n\nYou can view your token usage and cached token savings using the /stats command.\nWhen cached tokens are available, they will be displayed in the stats output.","routePath":"/cli/token-caching","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":11,"title":"Tutorials","content":"#\n\nThis page contains tutorials for interacting with VeCLI.\n\n\nSetting up a Model Context Protocol (MCP) server#\n\nCAUTION\n\nBefore using a third-party MCP server, ensure you trust its source and\nunderstand the tools it provides. Your use of third-party servers is at your own\nrisk.\n\nThis tutorial demonstrates how to set up a MCP server, using the GitHub MCP\nserver as an example. The GitHub MCP server provides tools for interacting with\nGitHub repositories, such as creating issues and commenting on pull requests.\n\n\nPrerequisites#\n\nBefore you begin, ensure you have the following installed and configured:\n\n * Docker: Install and run Docker.\n * GitHub Personal Access Token (PAT): Create a new classic or fine-grained PAT\n   with the necessary scopes.\n\n\nGuide#\n\nConfigure the MCP server in settings.json#\n\nIn your project's root directory, create or open the .ve/settings.json file.\nWithin the file, add the mcpServers configuration block, which provides\ninstructions for how to launch the GitHub MCP server.\n\n\n\nSet your GitHub token#\n\nCAUTION\n\nUsing a broadly scoped personal access token that has access to personal and\nprivate repositories can lead to information from the private repository being\nleaked into the public repository. We recommend using a fine-grained access\ntoken that doesn't share access to both public and private repositories.\n\nUse an environment variable to store your GitHub PAT:\n\n\n\nVeCLI uses this value in the mcpServers configuration that you defined in the\nsettings.json file.\n\nLaunch VeCLI and verify the connection#\n\nWhen you launch VeCLI, it automatically reads your configuration and launches\nthe GitHub MCP server in the background. You can then use natural language\nprompts to ask VeCLI to perform GitHub actions. For example:\n\n","routePath":"/cli/tutorials","lang":"","toc":[{"text":"Setting up a Model Context Protocol (MCP) server","id":"setting-up-a-model-context-protocol-mcp-server","depth":2,"charIndex":61},{"text":"Prerequisites","id":"prerequisites","depth":3,"charIndex":516},{"text":"Guide","id":"guide","depth":3,"charIndex":754},{"text":"Configure the MCP server in `settings.json`","id":"configure-the-mcp-server-in-settingsjson","depth":4,"charIndex":-1},{"text":"Set your GitHub token","id":"set-your-github-token","depth":4,"charIndex":1012},{"text":"Launch VeCLI and verify the connection","id":"launch-vecli-and-verify-the-connection","depth":4,"charIndex":1507}],"domain":"","frontmatter":{},"version":""},{"id":12,"title":"VeCLI Core","content":"#\n\nVeCLI's core package (packages/core) is the backend portion of VeCLI, handling\ncommunication with the Volcano Engine API, managing tools, and processing\nrequests sent from packages/cli. For a general overview of VeCLI, see the main\ndocumentation page.\n\n\nNavigating this section#\n\n * Core tools API: Information on how tools are defined, registered, and used by\n   the core.\n * Memory Import Processor: Documentation for the modular VE.md import feature\n   using @file.md syntax.\n\n\nRole of the core#\n\nWhile the packages/cli portion of VeCLI provides the user interface,\npackages/core is responsible for:\n\n * Volcano Engine API interaction: Securely communicating with the Volcano\n   Engine API, sending user prompts, and receiving model responses.\n * Prompt engineering: Constructing effective prompts for the Volcano Engine\n   model, potentially incorporating conversation history, tool definitions, and\n   instructional context from VE.md files.\n * Tool management & orchestration:\n   * Registering available tools (e.g., file system tools, shell command\n     execution).\n   * Interpreting tool use requests from the Volcano Engine model.\n   * Executing the requested tools with the provided arguments.\n   * Returning tool execution results to the Volcano Engine model for further\n     processing.\n * Session and state management: Keeping track of the conversation state,\n   including history and any relevant context required for coherent\n   interactions.\n * Configuration: Managing core-specific configurations, such as API key access,\n   model selection, and tool settings.\n\n\nSecurity considerations#\n\nThe core plays a vital role in security:\n\n * API key management: It handles the VOLCENGINE_API_KEY and ensures it's used\n   securely when communicating with the Volcano Engine API.\n * Tool execution: When tools interact with the local system (e.g.,\n   run_shell_command), the core (and its underlying tool implementations) must\n   do so with appropriate caution, often involving sandboxing mechanisms to\n   prevent unintended modifications.\n\n\nChat history compression#\n\nTo ensure that long conversations don't exceed the token limits of the Volcano\nEngine model, the core includes a chat history compression feature.\n\nWhen a conversation approaches the token limit for the configured model, the\ncore automatically compresses the conversation history before sending it to the\nmodel. This compression is designed to be lossless in terms of the information\nconveyed, but it reduces the overall number of tokens used.\n\nYou can find the token limits for each model in the Volcano Engine AI\ndocumentation.\n\n\nModel fallback#\n\nVeCLI includes a model fallback mechanism to ensure that you can continue to use\nthe CLI even if the default \"pro\" model is rate-limited.\n\nIf you are using the default \"pro\" model and the CLI detects that you are being\nrate-limited, it automatically switches to the \"flash\" model for the current\nsession. This allows you to continue working without interruption.\n\n\nFile discovery service#\n\nThe file discovery service is responsible for finding files in the project that\nare relevant to the current context. It is used by the @ command and other tools\nthat need to access files.\n\n\nMemory discovery service#\n\nThe memory discovery service is responsible for finding and loading the VE.md\nfiles that provide context to the model. It searches for these files in a\nhierarchical manner, starting from the current working directory and moving up\nto the project root and the user's home directory. It also searches in\nsubdirectories.\n\nThis allows you to have global, project-level, and component-level context\nfiles, which are all combined to provide the model with the most relevant\ninformation.\n\nYou can use the /memory command to show, add, and refresh the content of loaded\nVE.md files.","routePath":"/core/","lang":"","toc":[{"text":"Navigating this section","id":"navigating-this-section","depth":2,"charIndex":256},{"text":"Role of the core","id":"role-of-the-core","depth":2,"charIndex":483},{"text":"Security considerations","id":"security-considerations","depth":2,"charIndex":1582},{"text":"Chat history compression","id":"chat-history-compression","depth":2,"charIndex":2051},{"text":"Model fallback","id":"model-fallback","depth":2,"charIndex":2610},{"text":"File discovery service","id":"file-discovery-service","depth":2,"charIndex":2992},{"text":"Memory discovery service","id":"memory-discovery-service","depth":2,"charIndex":3207}],"domain":"","frontmatter":{},"version":""},{"id":13,"title":"Memory Import Processor","content":"#\n\nThe Memory Import Processor is a feature that allows you to modularize your\nVE.md files by importing content from other files using the @file.md syntax.\n\n\nOverview#\n\nThis feature enables you to break down large VE.md files into smaller, more\nmanageable components that can be reused across different contexts. The import\nprocessor supports both relative and absolute paths, with built-in safety\nfeatures to prevent circular imports and ensure file access security.\n\n\nSyntax#\n\nUse the @ symbol followed by the path to the file you want to import:\n\n\n\n\nSupported Path Formats#\n\n\nRelative Paths#\n\n * @./file.md - Import from the same directory\n * @../file.md - Import from parent directory\n * @./components/file.md - Import from subdirectory\n\n\nAbsolute Paths#\n\n * @/absolute/path/to/file.md - Import using absolute path\n\n\nExamples#\n\n\nBasic Import#\n\n\n\n\nNested Imports#\n\nThe imported files can themselves contain imports, creating a nested structure:\n\n\n\n\n\n\nSafety Features#\n\n\nCircular Import Detection#\n\nThe processor automatically detects and prevents circular imports:\n\n\n\n\nFile Access Security#\n\nThe validateImportPath function ensures that imports are only allowed from\nspecified directories, preventing access to sensitive files outside the allowed\nscope.\n\n\nMaximum Import Depth#\n\nTo prevent infinite recursion, there's a configurable maximum import depth\n(default: 5 levels).\n\n\nError Handling#\n\n\nMissing Files#\n\nIf a referenced file doesn't exist, the import will fail gracefully with an\nerror comment in the output.\n\n\nFile Access Errors#\n\nPermission issues or other file system errors are handled gracefully with\nappropriate error messages.\n\n\nCode Region Detection#\n\nThe import processor uses the marked library to detect code blocks and inline\ncode spans, ensuring that @ imports inside these regions are properly ignored.\nThis provides robust handling of nested code blocks and complex Markdown\nstructures.\n\n\nImport Tree Structure#\n\nThe processor returns an import tree that shows the hierarchy of imported files,\nsimilar to Claude's /memory feature. This helps users debug problems with their\nVE.md files by showing which files were read and their import relationships.\n\nExample tree structure:\n\n\n\nThe tree preserves the order that files were imported and shows the complete\nimport chain for debugging purposes.\n\n\nComparison to Claude Code's /memory (claude.md) Approach#\n\nClaude Code's /memory feature (as seen in claude.md) produces a flat, linear\ndocument by concatenating all included files, always marking file boundaries\nwith clear comments and path names. It does not explicitly present the import\nhierarchy, but the LLM receives all file contents and paths, which is sufficient\nfor reconstructing the hierarchy if needed.\n\nNote: The import tree is mainly for clarity during development and has limited\nrelevance to LLM consumption.\n\n\nAPI Reference#\n\n\nprocessImports(content, basePath, debugMode?, importState?)#\n\nProcesses import statements in VE.md content.\n\nParameters:\n\n * content (string): The content to process for imports\n * basePath (string): The directory path where the current file is located\n * debugMode (boolean, optional): Whether to enable debug logging (default:\n   false)\n * importState (ImportState, optional): State tracking for circular import\n   prevention\n\nReturns: Promise - Object containing processed content and import tree\n\n\nProcessImportsResult#\n\n\n\n\nMemoryFile#\n\n\n\n\nvalidateImportPath(importPath, basePath, allowedDirectories)#\n\nValidates import paths to ensure they are safe and within allowed directories.\n\nParameters:\n\n * importPath (string): The import path to validate\n * basePath (string): The base directory for resolving relative paths\n * allowedDirectories (string[]): Array of allowed directory paths\n\nReturns: boolean - Whether the import path is valid\n\n\nfindProjectRoot(startDir)#\n\nFinds the project root by searching for a .git directory upwards from the given\nstart directory. Implemented as an async function using non-blocking file system\nAPIs to avoid blocking the Node.js event loop.\n\nParameters:\n\n * startDir (string): The directory to start searching from\n\nReturns: Promise - The project root directory (or the start directory if no .git\nis found)\n\n\nBest Practices#\n\n 1. Use descriptive file names for imported components\n 2. Keep imports shallow - avoid deeply nested import chains\n 3. Document your structure - maintain a clear hierarchy of imported files\n 4. Test your imports - ensure all referenced files exist and are accessible\n 5. Use relative paths when possible for better portability\n\n\nTroubleshooting#\n\n\nCommon Issues#\n\n 1. Import not working: Check that the file exists and the path is correct\n 2. Circular import warnings: Review your import structure for circular\n    references\n 3. Permission errors: Ensure the files are readable and within allowed\n    directories\n 4. Path resolution issues: Use absolute paths if relative paths aren't\n    resolving correctly\n\n\nDebug Mode#\n\nEnable debug mode to see detailed logging of the import process:\n\n","routePath":"/core/memport","lang":"","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":157},{"text":"Syntax","id":"syntax","depth":2,"charIndex":469},{"text":"Supported Path Formats","id":"supported-path-formats","depth":2,"charIndex":552},{"text":"Relative Paths","id":"relative-paths","depth":3,"charIndex":578},{"text":"Absolute Paths","id":"absolute-paths","depth":3,"charIndex":742},{"text":"Examples","id":"examples","depth":2,"charIndex":820},{"text":"Basic Import","id":"basic-import","depth":3,"charIndex":832},{"text":"Nested Imports","id":"nested-imports","depth":3,"charIndex":850},{"text":"Safety Features","id":"safety-features","depth":2,"charIndex":953},{"text":"Circular Import Detection","id":"circular-import-detection","depth":3,"charIndex":972},{"text":"File Access Security","id":"file-access-security","depth":3,"charIndex":1071},{"text":"Maximum Import Depth","id":"maximum-import-depth","depth":3,"charIndex":1258},{"text":"Error Handling","id":"error-handling","depth":2,"charIndex":1379},{"text":"Missing Files","id":"missing-files","depth":3,"charIndex":1397},{"text":"File Access Errors","id":"file-access-errors","depth":3,"charIndex":1520},{"text":"Code Region Detection","id":"code-region-detection","depth":2,"charIndex":1645},{"text":"Import Tree Structure","id":"import-tree-structure","depth":2,"charIndex":1913},{"text":"Comparison to Claude Code's `/memory` (`claude.md`) Approach","id":"comparison-to-claude-codes-memory-claudemd-approach","depth":2,"charIndex":-1},{"text":"API Reference","id":"api-reference","depth":2,"charIndex":2847},{"text":"`processImports(content, basePath, debugMode?, importState?)`","id":"processimportscontent-basepath-debugmode-importstate","depth":3,"charIndex":-1},{"text":"`ProcessImportsResult`","id":"processimportsresult","depth":3,"charIndex":-1},{"text":"`MemoryFile`","id":"memoryfile","depth":3,"charIndex":-1},{"text":"`validateImportPath(importPath, basePath, allowedDirectories)`","id":"validateimportpathimportpath-basepath-alloweddirectories","depth":3,"charIndex":-1},{"text":"`findProjectRoot(startDir)`","id":"findprojectrootstartdir","depth":3,"charIndex":-1},{"text":"Best Practices","id":"best-practices","depth":2,"charIndex":4212},{"text":"Troubleshooting","id":"troubleshooting","depth":2,"charIndex":4559},{"text":"Common Issues","id":"common-issues","depth":3,"charIndex":4578},{"text":"Debug Mode","id":"debug-mode","depth":3,"charIndex":4942}],"domain":"","frontmatter":{},"version":""},{"id":14,"title":"VeCLI Core: Tools API","content":"#\n\nThe VeCLI core (packages/core) features a robust system for defining,\nregistering, and executing tools. These tools extend the capabilities of the\nVolcano Engine model, allowing it to interact with the local environment, fetch\nweb content, and perform various actions beyond simple text generation.\n\n\nCore Concepts#\n\n * Tool (tools.ts): An interface and base class (BaseTool) that defines the\n   contract for all tools. Each tool must have:\n   \n   * name: A unique internal name (used in API calls to Volcano Engine).\n   * displayName: A user-friendly name.\n   * description: A clear explanation of what the tool does, which is provided\n     to the Volcano Engine model.\n   * parameterSchema: A JSON schema defining the parameters that the tool\n     accepts. This is crucial for the Volcano Engine model to understand how to\n     call the tool correctly.\n   * validateToolParams(): A method to validate incoming parameters.\n   * getDescription(): A method to provide a human-readable description of what\n     the tool will do with specific parameters before execution.\n   * shouldConfirmExecute(): A method to determine if user confirmation is\n     required before execution (e.g., for potentially destructive operations).\n   * execute(): The core method that performs the tool's action and returns a\n     ToolResult.\n\n * ToolResult (tools.ts): An interface defining the structure of a tool's\n   execution outcome:\n   \n   * llmContent: The factual content to be included in the history sent back to\n     the LLM for context. This can be a simple string or a PartListUnion (an\n     array of Part objects and strings) for rich content.\n   * returnDisplay: A user-friendly string (often Markdown) or a special object\n     (like FileDiff) for display in the CLI.\n\n * Returning Rich Content: Tools are not limited to returning simple text. The\n   llmContent can be a PartListUnion, which is an array that can contain a mix\n   of Part objects (for images, audio, etc.) and strings. This allows a single\n   tool execution to return multiple pieces of rich content.\n\n * Tool Registry (tool-registry.ts): A class (ToolRegistry) responsible for:\n   \n   * Registering Tools: Holding a collection of all available built-in tools\n     (e.g., ReadFileTool, ShellTool).\n   * Discovering Tools: It can also discover tools dynamically:\n     * Command-based Discovery: If tools.discoveryCommand is configured in\n       settings, this command is executed. It's expected to output JSON\n       describing custom tools, which are then registered as DiscoveredTool\n       instances.\n     * MCP-based Discovery: If mcp.serverCommand is configured, the registry can\n       connect to a Model Context Protocol (MCP) server to list and register\n       tools (DiscoveredMCPTool).\n   * Providing Schemas: Exposing the FunctionDeclaration schemas of all\n     registered tools to the Volcano Engine model, so it knows what tools are\n     available and how to use them.\n   * Retrieving Tools: Allowing the core to get a specific tool by name for\n     execution.\n\n\nBuilt-in Tools#\n\nThe core comes with a suite of pre-defined tools, typically found in\npackages/core/src/tools/. These include:\n\n * File System Tools:\n   * LSTool (ls.ts): Lists directory contents.\n   * ReadFileTool (read-file.ts): Reads the content of a single file. It takes\n     an absolute_path parameter, which must be an absolute path.\n   * WriteFileTool (write-file.ts): Writes content to a file.\n   * GrepTool (grep.ts): Searches for patterns in files.\n   * GlobTool (glob.ts): Finds files matching glob patterns.\n   * EditTool (edit.ts): Performs in-place modifications to files (often\n     requiring confirmation).\n   * ReadManyFilesTool (read-many-files.ts): Reads and concatenates content from\n     multiple files or glob patterns (used by the @ command in CLI).\n * Execution Tools:\n   * ShellTool (shell.ts): Executes arbitrary shell commands (requires careful\n     sandboxing and user confirmation).\n * Web Tools:\n   * WebFetchTool (web-fetch.ts): Fetches content from a URL.\n   * WebSearchTool (web-search.ts): Performs a web search.\n * Memory Tools:\n   * MemoryTool (memoryTool.ts): Interacts with the AI's memory.\n\nEach of these tools extends BaseTool and implements the required methods for its\nspecific functionality.\n\n\nTool Execution Flow#\n\n 1. Model Request: The Volcano Engine model, based on the user's prompt and the\n    provided tool schemas, decides to use a tool and returns a FunctionCall part\n    in its response, specifying the tool name and arguments.\n 2. Core Receives Request: The core parses this FunctionCall.\n 3. Tool Retrieval: It looks up the requested tool in the ToolRegistry.\n 4. Parameter Validation: The tool's validateToolParams() method is called.\n 5. Confirmation (if needed):\n    * The tool's shouldConfirmExecute() method is called.\n    * If it returns details for confirmation, the core communicates this back to\n      the CLI, which prompts the user.\n    * The user's decision (e.g., proceed, cancel) is sent back to the core.\n 6. Execution: If validated and confirmed (or if no confirmation is needed), the\n    core calls the tool's execute() method with the provided arguments and an\n    AbortSignal (for potential cancellation).\n 7. Result Processing: The ToolResult from execute() is received by the core.\n 8. Response to Model: The llmContent from the ToolResult is packaged as a\n    FunctionResponse and sent back to the Volcano Engine model so it can\n    continue generating a user-facing response.\n 9. Display to User: The returnDisplay from the ToolResult is sent to the CLI to\n    show the user what the tool did.\n\n\nExtending with Custom Tools#\n\nWhile direct programmatic registration of new tools by users isn't explicitly\ndetailed as a primary workflow in the provided files for typical end-users, the\narchitecture supports extension through:\n\n * Command-based Discovery: Advanced users or project administrators can define\n   a tools.discoveryCommand in settings.json. This command, when run by the\n   VeCLI core, should output a JSON array of FunctionDeclaration objects. The\n   core will then make these available as DiscoveredTool instances. The\n   corresponding tools.callCommand would then be responsible for actually\n   executing these custom tools.\n * MCP Server(s): For more complex scenarios, one or more MCP servers can be set\n   up and configured via the mcpServers setting in settings.json. The VeCLI core\n   can then discover and use tools exposed by these servers. As mentioned, if\n   you have multiple MCP servers, the tool names will be prefixed with the\n   server name from your configuration (e.g., serverAlias__actualToolName).\n\nThis tool system provides a flexible and powerful way to augment the Volcano\nEngine model's capabilities, making the VeCLI a versatile assistant for a wide\nrange of tasks.","routePath":"/core/tools-api","lang":"","toc":[{"text":"Core Concepts","id":"core-concepts","depth":2,"charIndex":303},{"text":"Built-in Tools","id":"built-in-tools","depth":2,"charIndex":3034},{"text":"Tool Execution Flow","id":"tool-execution-flow","depth":2,"charIndex":4272},{"text":"Extending with Custom Tools","id":"extending-with-custom-tools","depth":2,"charIndex":5609}],"domain":"","frontmatter":{},"version":""},{"id":15,"title":"VeCLI Execution and Deployment","content":"#\n\nThis document describes how to run VeCLI and explains the deployment\narchitecture that VeCLI uses.\n\n\nRunning VeCLI#\n\nThere are several ways to run VeCLI. The option you choose depends on how you\nintend to use VeCLI.\n\n--------------------------------------------------------------------------------\n\n\n1. Standard installation (Recommended for typical users)#\n\nThis is the recommended way for end-users to install VeCLI. It involves\ndownloading the VeCLI package from the NPM registry.\n\n * Global install:\n   \n   \n   \n   Then, run the CLI from anywhere:\n   \n   \n\n * NPX execution:\n   \n   \n\n--------------------------------------------------------------------------------\n\n\n2. Running in a sandbox (Docker/Podman)#\n\nFor security and isolation, VeCLI can be run inside a container. This is the\ndefault way that the CLI executes tools that might have side effects.\n\n * Directly from the Registry: You can run the published sandbox image directly.\n   This is useful for environments where you only have Docker and want to run\n   the CLI.\n   \n   \n\n * Using the --sandbox flag: If you have VeCLI installed locally (using the\n   standard installation described above), you can instruct it to run inside the\n   sandbox container.\n   \n   \n\n--------------------------------------------------------------------------------\n\n\n3. Running from source (Recommended for VeCLI contributors)#\n\nContributors to the project will want to run the CLI directly from the source\ncode.\n\n * Development Mode: This method provides hot-reloading and is useful for active\n   development.\n   \n   \n\n * Production-like mode (Linked package): This method simulates a global\n   installation by linking your local package. It's useful for testing a local\n   build in a production workflow.\n   \n   \n\n--------------------------------------------------------------------------------\n\n\n4. Running the latest VeCLI commit from GitHub#\n\nYou can run the most recently committed version of VeCLI directly from the\nGitHub repository. This is useful for testing features still in development.\n\n\n\n\nDeployment architecture#\n\nThe execution methods described above are made possible by the following\narchitectural components and processes:\n\nNPM packages\n\nVeCLI project is a monorepo that publishes two core packages to the NPM\nregistry:\n\n * @vecli/vecli-core: The backend, handling logic and tool execution.\n * @vecli/vecli: The user-facing frontend.\n\nThese packages are used when performing the standard installation and when\nrunning VeCLI from the source.\n\nBuild and packaging processes\n\nThere are two distinct build processes used, depending on the distribution\nchannel:\n\n * NPM publication: For publishing to the NPM registry, the TypeScript source\n   code in @vecli/vecli-core and @vecli/vecli is transpiled into standard\n   JavaScript using the TypeScript Compiler (tsc). The resulting dist/ directory\n   is what gets published in the NPM package. This is a standard approach for\n   TypeScript libraries.\n\n * GitHub npx execution: When running the latest version of VeCLI directly from\n   GitHub, a different process is triggered by the prepare script in\n   package.json. This script uses esbuild to bundle the entire application and\n   its dependencies into a single, self-contained JavaScript file. This bundle\n   is created on-the-fly on the user's machine and is not checked into the\n   repository.\n\nDocker sandbox image\n\nThe Docker-based execution method is supported by the vecli-sandbox container\nimage. This image is published to a container registry and contains a\npre-installed, global version of VeCLI.\n\n\nRelease process#\n\nThe release process is automated through GitHub Actions. The release workflow\nperforms the following actions:\n\n 1. Build the NPM packages using tsc.\n 2. Publish the NPM packages to the artifact registry.\n 3. Create GitHub releases with bundled assets.","routePath":"/deployment","lang":"","toc":[{"text":"Running VeCLI","id":"running-vecli","depth":2,"charIndex":103},{"text":"1. Standard installation (Recommended for typical users)","id":"1-standard-installation-recommended-for-typical-users","depth":3,"charIndex":302},{"text":"2. Running in a sandbox (Docker/Podman)","id":"2-running-in-a-sandbox-dockerpodman","depth":3,"charIndex":673},{"text":"3. Running from source (Recommended for VeCLI contributors)","id":"3-running-from-source-recommended-for-vecli-contributors","depth":3,"charIndex":1314},{"text":"4. Running the latest VeCLI commit from GitHub","id":"4-running-the-latest-vecli-commit-from-github","depth":3,"charIndex":1846},{"text":"Deployment architecture","id":"deployment-architecture","depth":2,"charIndex":2051},{"text":"Release process","id":"release-process","depth":2,"charIndex":3572}],"domain":"","frontmatter":{},"version":""},{"id":16,"title":"Uninstalling the CLI","content":"#\n\nYour uninstall method depends on how you ran the CLI. Follow the instructions\nfor either npx or a global npm installation.\n\n\nMethod 1: Using npx#\n\nnpx runs packages from a temporary cache without a permanent installation. To\n\"uninstall\" the CLI, you must clear this cache, which will remove gemini-cli and\nany other packages previously executed with npx.\n\nThe npx cache is a directory named _npx inside your main npm cache folder. You\ncan find your npm cache path by running npm config get cache.\n\nFor macOS / Linux\n\n\n\nFor Windows\n\nCommand Prompt\n\n\n\nPowerShell\n\n\n\n\nMethod 2: Using npm (Global Install)#\n\nIf you installed the CLI globally (e.g., npm install -g @vecode-cli/vecode-cli),\nuse the npm uninstall command with the -g flag to remove it.\n\n\n\nThis command completely removes the package from your system.","routePath":"/en/Uninstall","lang":"","toc":[{"text":"Method 1: Using npx","id":"method-1-using-npx","depth":2,"charIndex":127},{"text":"Method 2: Using npm (Global Install)","id":"method-2-using-npm-global-install","depth":2,"charIndex":567}],"domain":"","frontmatter":{},"version":""},{"id":17,"title":"Checkpointing","content":"#\n\nThe VeCLI includes a Checkpointing feature that automatically saves a snapshot\nof your project's state before any file modifications are made by AI-powered\ntools. This allows you to safely experiment with and apply code changes, knowing\nyou can instantly revert back to the state before the tool was run.\n\n\nHow It Works#\n\nWhen you approve a tool that modifies the file system (like write_file or\nreplace), the CLI automatically creates a \"checkpoint.\" This checkpoint\nincludes:\n\n 1. A Git Snapshot: A commit is made in a special, shadow Git repository located\n    in your home directory (~/.ve/history/<project_hash>). This snapshot\n    captures the complete state of your project files at that moment. It does\n    not interfere with your own project's Git repository.\n 2. Conversation History: The entire conversation you've had with the agent up\n    to that point is saved.\n 3. The Tool Call: The specific tool call that was about to be executed is also\n    stored.\n\nIf you want to undo the change or simply go back, you can use the /restore\ncommand. Restoring a checkpoint will:\n\n * Revert all files in your project to the state captured in the snapshot.\n * Restore the conversation history in the CLI.\n * Re-propose the original tool call, allowing you to run it again, modify it,\n   or simply ignore it.\n\nAll checkpoint data, including the Git snapshot and conversation history, is\nstored locally on your machine. The Git snapshot is stored in the shadow\nrepository while the conversation history and tool calls are saved in a JSON\nfile in your project's temporary directory, typically located at\n~/.ve/tmp/<project_hash>/checkpoints.\n\n\nEnabling the Feature#\n\nThe Checkpointing feature is disabled by default. To enable it, you can either\nuse a command-line flag or edit your settings.json file.\n\n\nUsing the Command-Line Flag#\n\nYou can enable checkpointing for the current session by using the\n--checkpointing flag when starting the VeCLI:\n\n\n\n\nUsing the settings.json File#\n\nTo enable checkpointing by default for all sessions, you need to edit your\nsettings.json file.\n\nAdd the following key to your settings.json:\n\n\n\n\nUsing the /restore Command#\n\nOnce enabled, checkpoints are created automatically. To manage them, you use the\n/restore command.\n\n\nList Available Checkpoints#\n\nTo see a list of all saved checkpoints for the current project, simply run:\n\n\n\nThe CLI will display a list of available checkpoint files. These file names are\ntypically composed of a timestamp, the name of the file being modified, and the\nname of the tool that was about to be run (e.g.,\n2025-06-22T10-00-00_000Z-my-file.txt-write_file).\n\n\nRestore a Specific Checkpoint#\n\nTo restore your project to a specific checkpoint, use the checkpoint file from\nthe list:\n\n\n\nFor example:\n\n\n\nAfter running the command, your files and conversation will be immediately\nrestored to the state they were in when the checkpoint was created, and the\noriginal tool prompt will reappear.","routePath":"/en/checkpointing","lang":"","toc":[{"text":"How It Works","id":"how-it-works","depth":2,"charIndex":309},{"text":"Enabling the Feature","id":"enabling-the-feature","depth":2,"charIndex":1644},{"text":"Using the Command-Line Flag","id":"using-the-command-line-flag","depth":3,"charIndex":1805},{"text":"Using the `settings.json` File","id":"using-the-settingsjson-file","depth":3,"charIndex":-1},{"text":"Using the `/restore` Command","id":"using-the-restore-command","depth":2,"charIndex":-1},{"text":"List Available Checkpoints","id":"list-available-checkpoints","depth":3,"charIndex":2257},{"text":"Restore a Specific Checkpoint","id":"restore-a-specific-checkpoint","depth":3,"charIndex":2626}],"domain":"","frontmatter":{},"version":""},{"id":18,"title":"Authentication Setup","content":"#\n\nThe VeCLI requires you to authenticate with Google's AI services. On initial\nstartup you'll need to configure one of the following authentication methods:\n\n 1. Login with Volcengine AK/SK:\n    \n    * Use this option to authenticate through Volcengine's Access Key (AK) and\n      Secret Key (SK).\n    * This authentication method is suitable for scenarios where VeCLI is used\n      in server environments or automated scripts.\n    * Authentication information will be securely cached locally, eliminating\n      the need for repeated configuration in subsequent uses.\n    \n    Obtaining AK/SK:\n    \n    1. Log in to Volcengine Console\n    2. Navigate to \"Access Control\" → \"Access Keys\" page\n    3. Click \"Create Access Key\" button\n    4. The system will generate a pair of AK/SK, please save them securely\n    \n    Configuration Methods:\n    \n    Method 1: Environment Variable Configuration\n    \n    \n    \n    Method 2: Configuration File Create a configuration file\n    ~/.vecli/config.json in the user's home directory:\n    \n    \n    \n    Method 3: Command Line Parameters\n    \n    \n    \n    Verifying Configuration:\n    \n    \n\n 2. Login with Volcengine API key:\n    \n    * Obtain your API key from Volcengine: https://www.volcengine.com/\n    * Set the VECLI_API_KEY environment variable. In the following methods,\n      replace YOUR_VECLI_API_KEY with the API key you obtained from Volcengine\n      Ark Platform:\n      \n      * You can temporarily set the environment variable in your current shell\n        session using the following command:\n        \n        \n      \n      * For repeated use, you can add the environment variable to your .env\n        file.\n      \n      * Alternatively you can export the API key from your shell's configuration\n        file (like ~/.bashrc, ~/.zshrc, or ~/.profile). For example, the\n        following command adds the environment variable to a ~/.bashrc file:\n        \n        \n        \n        :warning: Be advised that when you export your API key inside your shell\n        configuration file, any other process executed from the shell can read\n        it.","routePath":"/en/cli/authentication","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":19,"title":"CLI Commands","content":"#\n\nVeCLI supports several built-in commands to help you manage your session,\ncustomize the interface, and control its behavior. These commands are prefixed\nwith a forward slash (/), an at symbol (@), or an exclamation mark (!).\n\n\nSlash commands (/)#\n\nSlash commands provide meta-level control over the CLI itself.\n\n\nBuilt-in Commands#\n\n * /bug\n   \n   * Description: File an issue about VeCLI. By default, the issue is filed\n     within the GitHub repository for VeCLI. The string you enter after /bug\n     will become the headline for the bug being filed. The default /bug behavior\n     can be modified using the advanced.bugCommand setting in your\n     .ve/settings.json files.\n\n * /chat\n   \n   * Description: Save and resume conversation history for branching\n     conversation state interactively, or resuming a previous state from a later\n     session.\n   * Sub-commands:\n     * save\n       * Description: Saves the current conversation history. You must add a\n         <tag> for identifying the conversation state.\n       * Usage: /chat save <tag>\n       * Details on Checkpoint Location: The default locations for saved chat\n         checkpoints are:\n         * Linux/macOS: ~/.ve/tmp/<project_hash>/\n         * Windows: C:\\Users\\<YourUsername>\\.ve\\tmp\\<project_hash>\\\n         * When you run /chat list, the CLI only scans these specific\n           directories to find available checkpoints.\n         * Note: These checkpoints are for manually saving and resuming\n           conversation states. For automatic checkpoints created before file\n           modifications, see the Checkpointing documentation.\n     * resume\n       * Description: Resumes a conversation from a previous save.\n       * Usage: /chat resume <tag>\n     * list\n       * Description: Lists available tags for chat state resumption.\n     * delete\n       * Description: Deletes a saved conversation checkpoint.\n       * Usage: /chat delete <tag>\n\n * /clear\n   \n   * Description: Clear the terminal screen, including the visible session\n     history and scrollback within the CLI. The underlying session data (for\n     history recall) might be preserved depending on the exact implementation,\n     but the visual display is cleared.\n   * Keyboard shortcut: Press Ctrl+L at any time to perform a clear action.\n\n * /compress\n   \n   * Description: Replace the entire chat context with a summary. This saves on\n     tokens used for future tasks while retaining a high level summary of what\n     has happened.\n\n * /copy\n   \n   * Description: Copies the last output produced by VeCLI to your clipboard,\n     for easy sharing or reuse.\n   * Note: This command requires platform-specific clipboard tools to be\n     installed.\n     * On Linux, it requires xclip or xsel. You can typically install them using\n       your system's package manager.\n     * On macOS, it requires pbcopy, and on Windows, it requires clip. These\n       tools are typically pre-installed on their respective systems.\n\n * /directory (or /dir)\n   \n   * Description: Manage workspace directories for multi-directory support.\n   * Sub-commands:\n     * add:\n       * Description: Add a directory to the workspace. The path can be absolute\n         or relative to the current working directory. Moreover, the reference\n         from home directory is supported as well.\n       * Usage: /directory add <path1>,<path2>\n       * Note: Disabled in restrictive sandbox profiles. If you're using that,\n         use --include-directories when starting the session instead.\n     * show:\n       * Description: Display all directories added by /directory add and\n         --include-directories.\n       * Usage: /directory show\n\n * /editor\n   \n   * Description: Open a dialog for selecting supported editors.\n\n * /extensions\n   \n   * Description: Lists all active extensions in the current VeCLI session. See\n     VeCLI Extensions.\n\n * /help (or /?)\n   \n   * Description: Display help information about VeCLI, including available\n     commands and their usage.\n\n * /mcp\n   \n   * Description: List configured Model Context Protocol (MCP) servers, their\n     connection status, server details, and available tools.\n   * Sub-commands:\n     * desc or descriptions:\n       * Description: Show detailed descriptions for MCP servers and tools.\n     * nodesc or nodescriptions:\n       * Description: Hide tool descriptions, showing only the tool names.\n     * schema:\n       * Description: Show the full JSON schema for the tool's configured\n         parameters.\n   * Keyboard Shortcut: Press Ctrl+T at any time to toggle between showing and\n     hiding tool descriptions.\n\n * /memory\n   \n   * Description: Manage the AI's instructional context (hierarchical memory\n     loaded from VE.md files).\n   * Sub-commands:\n     * add:\n       * Description: Adds the following text to the AI's memory. Usage: /memory\n         add <text to remember>\n     * show:\n       * Description: Display the full, concatenated content of the current\n         hierarchical memory that has been loaded from all VE.md files. This\n         lets you inspect the instructional context being provided to the\n         Volcano Engine model.\n     * refresh:\n       * Description: Reload the hierarchical instructional memory from all\n         VE.md files found in the configured locations (global,\n         project/ancestors, and sub-directories). This command updates the model\n         with the latest VE.md content.\n     * Note: For more details on how VE.md files contribute to hierarchical\n       memory, see the CLI Configuration documentation.\n\n * /restore\n   \n   * Description: Restores the project files to the state they were in just\n     before a tool was executed. This is particularly useful for undoing file\n     edits made by a tool. If run without a tool call ID, it will list available\n     checkpoints to restore from.\n   * Usage: /restore [tool_call_id]\n   * Note: Only available if the CLI is invoked with the --checkpointing option\n     or configured via settings. See Checkpointing documentation for more\n     details.\n\n * /settings\n   \n   * Description: Open the settings editor to view and modify VeCLI settings.\n   * Details: This command provides a user-friendly interface for changing\n     settings that control the behavior and appearance of VeCLI. It is\n     equivalent to manually editing the .ve/settings.json file, but with\n     validation and guidance to prevent errors.\n   * Usage: Simply run /settings and the editor will open. You can then browse\n     or search for specific settings, view their current values, and modify them\n     as desired. Changes to some settings are applied immediately, while others\n     require a restart.\n\n * /stats\n   \n   * Description: Display detailed statistics for the current VeCLI session,\n     including token usage, cached token savings (when available), and session\n     duration. Note: Cached token information is only displayed when cached\n     tokens are being used, which occurs with API key authentication but not\n     with OAuth authentication at this time.\n\n * /theme\n   \n   * Description: Open a dialog that lets you change the visual theme of VeCLI.\n\n * /auth\n   \n   * Description: Open a dialog that lets you change the authentication method.\n\n * /about\n   \n   * Description: Show version info. Please share this information when filing\n     issues.\n\n * /tools\n   \n   * Description: Display a list of tools that are currently available within\n     VeCLI.\n   * Usage: /tools [desc]\n   * Sub-commands:\n     * desc or descriptions:\n       * Description: Show detailed descriptions of each tool, including each\n         tool's name with its full description as provided to the model.\n     * nodesc or nodescriptions:\n       * Description: Hide tool descriptions, showing only the tool names.\n\n * /privacy\n   \n   * Description: Display the Privacy Notice and allow users to select whether\n     they consent to the collection of their data for service improvement\n     purposes.\n\n * /quit (or /exit)\n   \n   * Description: Exit VeCLI.\n\n * /vim\n   \n   * Description: Toggle vim mode on or off. When vim mode is enabled, the input\n     area supports vim-style navigation and editing commands in both NORMAL and\n     INSERT modes.\n   * Features:\n     * NORMAL mode: Navigate with h, j, k, l; jump by words with w, b, e; go to\n       line start/end with 0, $, ^; go to specific lines with G (or gg for first\n       line)\n     * INSERT mode: Standard text input with escape to return to NORMAL mode\n     * Editing commands: Delete with x, change with c, insert with i, a, o, O;\n       complex operations like dd, cc, dw, cw\n     * Count support: Prefix commands with numbers (e.g., 3h, 5w, 10G)\n     * Repeat last command: Use . to repeat the last editing operation\n     * Persistent setting: Vim mode preference is saved to ~/.ve/settings.json\n       and restored between sessions\n   * Status indicator: When enabled, shows [NORMAL] or [INSERT] in the footer\n\n * /init\n   \n   * Description: To help users easily create a VE.md file, this command\n     analyzes the current directory and generates a tailored context file,\n     making it simpler for them to provide project-specific instructions to the\n     Volcano Engine agent.\n\n\nCustom Commands#\n\nFor a quick start, see the example below.\n\nCustom commands allow you to save and reuse your favorite or most frequently\nused prompts as personal shortcuts within VeCLI. You can create commands that\nare specific to a single project or commands that are available globally across\nall your projects, streamlining your workflow and ensuring consistency.\n\nFile Locations & Precedence#\n\nVeCLI discovers commands from two locations, loaded in a specific order:\n\n 1. User Commands (Global): Located in ~/.ve/commands/. These commands are\n    available in any project you are working on.\n 2. Project Commands (Local): Located in ~/.ve/commands/. These commands are\n    specific to the current project and can be checked into version control to\n    be shared with your team.\n\nIf a command in the project directory has the same name as a command in the user\ndirectory, the project command will always be used. This allows projects to\noverride global commands with project-specific versions.\n\nNaming and Namespacing#\n\nThe name of a command is determined by its file path relative to its commands\ndirectory. Subdirectories are used to create namespaced commands, with the path\nseparator (/ or \\) being converted to a colon (:).\n\n * A file at ~/.ve/commands/test.toml becomes the command /test.\n * A file at <project>/.ve/commands/git/commit.toml becomes the namespaced\n   command /git:commit.\n\nTOML File Format (v1)#\n\nYour command definition files must be written in the TOML format and use the\n.toml file extension.\n\nRequired Fields#\n\n * prompt (String): The prompt that will be sent to the Volcano Engine model\n   when the command is executed. This can be a single-line or multi-line string.\n\nOptional Fields#\n\n * description (String): A brief, one-line description of what the command does.\n   This text will be displayed next to your command in the /help menu. If you\n   omit this field, a generic description will be generated from the filename.\n\nHandling Arguments#\n\nCustom commands support two powerful methods for handling arguments. The CLI\nautomatically chooses the correct method based on the content of your command's\nprompt.\n\n1. Context-Aware Injection with {{args}}#\n\nIf your prompt contains the special placeholder {{args}}, the CLI will replace\nthat placeholder with the text the user typed after the command name.\n\nThe behavior of this injection depends on where it is used:\n\nA. Raw Injection (Outside Shell Commands)\n\nWhen used in the main body of the prompt, the arguments are injected exactly as\nthe user typed them.\n\nExample (git/fix.toml):\n\n\n\nThe model receives: Please provide a code fix for the issue described here:\n\"Button is misaligned\".\n\nB. Using Arguments in Shell Commands (Inside !{...} Blocks)\n\nWhen you use {{args}} inside a shell injection block (!{...}), the arguments are\nautomatically shell-escaped before replacement. This allows you to safely pass\narguments to shell commands, ensuring the resulting command is syntactically\ncorrect and secure while preventing command injection vulnerabilities.\n\nExample (/grep-code.toml):\n\n\n\nWhen you run /grep-code It's complicated:\n\n 1. The CLI sees {{args}} used both outside and inside !{...}.\n 2. Outside: The first {{args}} is replaced raw with It's complicated.\n 3. Inside: The second {{args}} is replaced with the escaped version (e.g., on\n    Linux: \"It's complicated\").\n 4. The command executed is grep -r \"It's complicated\" ..\n 5. The CLI prompts you to confirm this exact, secure command before execution.\n 6. The final prompt is sent.\n\n2. Default Argument Handling#\n\nIf your prompt does not contain the special placeholder {{args}}, the CLI uses a\ndefault behavior for handling arguments.\n\nIf you provide arguments to the command (e.g., /mycommand arg1), the CLI will\nappend the full command you typed to the end of the prompt, separated by two\nnewlines. This allows the model to see both the original instructions and the\nspecific arguments you just provided.\n\nIf you do not provide any arguments (e.g., /mycommand), the prompt is sent to\nthe model exactly as it is, with nothing appended.\n\nExample (changelog.toml):\n\nThis example shows how to create a robust command by defining a role for the\nmodel, explaining where to find the user's input, and specifying the expected\nformat and behavior.\n\n\n\nWhen you run /changelog 1.2.0 added \"New feature\", the final text sent to the\nmodel will be the original prompt followed by two newlines and the command you\ntyped.\n\n3. Executing Shell Commands with !{...}#\n\nYou can make your commands dynamic by executing shell commands directly within\nyour prompt and injecting their output. This is ideal for gathering context from\nyour local environment, like reading file content or checking the status of Git.\n\nWhen a custom command attempts to execute a shell command, VeCLI will now prompt\nyou for confirmation before proceeding. This is a security measure to ensure\nthat only intended commands can be run.\n\nHow It Works:\n\n 1. Inject Commands: Use the !{...} syntax.\n 2. Argument Substitution: If {{args}} is present inside the block, it is\n    automatically shell-escaped (see Context-Aware Injection above).\n 3. Robust Parsing: The parser correctly handles complex shell commands that\n    include nested braces, such as JSON payloads. Note: The content inside\n    !{...} must have balanced braces ({ and }). If you need to execute a command\n    containing unbalanced braces, consider wrapping it in an external script\n    file and calling the script within the !{...} block.\n 4. Security Check and Confirmation: The CLI performs a security check on the\n    final, resolved command (after arguments are escaped and substituted). A\n    dialog will appear showing the exact command(s) to be executed.\n 5. Execution and Error Reporting: The command is executed. If the command\n    fails, the output injected into the prompt will include the error messages\n    (stderr) followed by a status line, e.g., [Shell command exited with code\n    1]. This helps the model understand the context of the failure.\n\nExample (git/commit.toml):\n\nThis command gets the staged git diff and uses it to ask the model to write a\ncommit message.\n\n\n\nWhen you run /git:commit, the CLI first executes git diff --staged, then\nreplaces !{git diff --staged} with the output of that command before sending the\nfinal, complete prompt to the model.\n\n4. Injecting File Content with @{...}#\n\nYou can directly embed the content of a file or a directory listing into your\nprompt using the @{...} syntax. This is useful for creating commands that\noperate on specific files.\n\nHow It Works:\n\n * File Injection: @{path/to/file.txt} is replaced by the content of file.txt.\n * Multimodal Support: If the path points to a supported image (e.g., PNG,\n   JPEG), PDF, audio, or video file, it will be correctly encoded and injected\n   as multimodal input. Other binary files are handled gracefully and skipped.\n * Directory Listing: @{path/to/dir} is traversed and each file present within\n   the directory and all subdirectories are inserted into the prompt. This\n   respects .gitignore and .veignore if enabled.\n * Workspace-Aware: The command searches for the path in the current directory\n   and any other workspace directories. Absolute paths are allowed if they are\n   within the workspace.\n * Processing Order: File content injection with @{...} is processed before\n   shell commands (!{...}) and argument substitution ({{args}}).\n * Parsing: The parser requires the content inside @{...} (the path) to have\n   balanced braces ({ and }).\n\nExample (review.toml):\n\nThis command injects the content of a fixed best practices file\n(docs/best-practices.md) and uses the user's arguments to provide context for\nthe review.\n\n\n\nWhen you run /review FileCommandLoader.ts, the @{docs/best-practices.md}\nplaceholder is replaced by the content of that file, and {{args}} is replaced by\nthe text you provided, before the final prompt is sent to the model.\n\n--------------------------------------------------------------------------------\n\nExample: A \"Pure Function\" Refactoring Command#\n\nLet's create a global command that asks the model to refactor a piece of code.\n\n1. Create the file and directories:\n\nFirst, ensure the user commands directory exists, then create a refactor\nsubdirectory for organization and the final TOML file.\n\n\n\n2. Add the content to the file:\n\nOpen ~/.ve/commands/refactor/pure.toml in your editor and add the following\ncontent. We are including the optional description for best practice.\n\n\n\n3. Run the Command:\n\nThat's it! You can now run your command in the CLI. First, you might add a file\nto the context, and then invoke your command:\n\n\n\nVeCLI will then execute the multi-line prompt defined in your TOML file.\n\n\nAt commands (@)#\n\nAt commands are used to include the content of files or directories as part of\nyour prompt to Volcano Engine. These commands include git-aware filtering.\n\n * @<path_to_file_or_directory>\n   \n   * Description: Inject the content of the specified file or files into your\n     current prompt. This is useful for asking questions about specific code,\n     text, or collections of files.\n   * Examples:\n     * @path/to/your/file.txt Explain this text.\n     * @src/my_project/ Summarize the code in this directory.\n     * What is this file about? @README.md\n   * Details:\n     * If a path to a single file is provided, the content of that file is read.\n     * If a path to a directory is provided, the command attempts to read the\n       content of files within that directory and any subdirectories.\n     * Spaces in paths should be escaped with a backslash (e.g., @My\\\n       Documents/file.txt).\n     * The command uses the read_many_files tool internally. The content is\n       fetched and then inserted into your query before being sent to the model.\n     * Git-aware filtering: By default, git-ignored files (like node_modules/,\n       dist/, .env, .git/) are excluded. This behavior can be changed via the\n       context.fileFiltering settings.\n     * File types: The command is intended for text-based files. While it might\n       attempt to read any file, binary files or very large files might be\n       skipped or truncated by the underlying read_many_files tool to ensure\n       performance and relevance. The tool indicates if files were skipped.\n   * Output: The CLI will show a tool call message indicating that\n     read_many_files was used, along with a message detailing the status and the\n     path(s) that were processed.\n\n * @ (Lone at symbol)\n   \n   * Description: If you type a lone @ symbol without a path, the query is\n     passed as-is to the Volcano Engine model. This might be useful if you are\n     specifically talking about the @ symbol in your prompt.\n\n\nError handling for @ commands#\n\n * If the path specified after @ is not found or is invalid, an error message\n   will be displayed, and the query might not be sent to the model, or it will\n   be sent without the file content.\n * If the read_many_files tool encounters an error (e.g., permission issues),\n   this will also be reported.\n\n\nShell mode & passthrough commands (!)#\n\nThe ! prefix lets you interact with your system's shell directly from within\nVeCLI.\n\n * !<shell_command>\n   \n   * Description: Execute the given <shell_command> using bash on Linux/macOS or\n     cmd.exe on Windows. Any output or errors from the command are displayed in\n     the terminal.\n   * Examples:\n     * !ls -la (executes ls -la and returns to VeCLI)\n     * !git status (executes git status and returns to VeCLI)\n\n * ! (Toggle shell mode)\n   \n   * Description: Typing ! on its own toggles shell mode.\n     * Entering shell mode:\n       * When active, shell mode uses a different coloring and a \"Shell Mode\n         Indicator\".\n       * While in shell mode, text you type is interpreted directly as a shell\n         command.\n     * Exiting shell mode:\n       * When exited, the UI reverts to its standard appearance and normal VeCLI\n         behavior resumes.\n\n * Caution for all ! usage: Commands you execute in shell mode have the same\n   permissions and impact as if you ran them directly in your terminal.\n\n * Environment Variable: When a command is executed via ! or in shell mode, the\n   GEMINI_CLI=1 environment variable is set in the subprocess's environment.\n   This allows scripts or tools to detect if they are being run from within the\n   VeCLI.","routePath":"/en/cli/commands","lang":"","toc":[{"text":"Slash commands (`/`)","id":"slash-commands-","depth":2,"charIndex":-1},{"text":"Built-in Commands","id":"built-in-commands","depth":3,"charIndex":315},{"text":"Custom Commands","id":"custom-commands","depth":3,"charIndex":9188},{"text":"File Locations & Precedence","id":"file-locations--precedence","depth":4,"charIndex":9557},{"text":"Naming and Namespacing","id":"naming-and-namespacing","depth":4,"charIndex":10187},{"text":"TOML File Format (v1)","id":"toml-file-format-v1","depth":4,"charIndex":10587},{"text":"Handling Arguments","id":"handling-arguments","depth":4,"charIndex":11145},{"text":"Example: A \"Pure Function\" Refactoring Command","id":"example-a-pure-function-refactoring-command","depth":4,"charIndex":17205},{"text":"At commands (`@`)","id":"at-commands-","depth":2,"charIndex":-1},{"text":"Error handling for `@` commands","id":"error-handling-for--commands","depth":3,"charIndex":-1},{"text":"Shell mode & passthrough commands (`!`)","id":"shell-mode--passthrough-commands-","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":20,"title":"VeCLI Configuration","content":"#\n\nNote on Deprecated Configuration Format\n\nThis document describes the legacy v1 format for the settings.json file. This\nformat is now deprecated.\n\n * The new format will be supported in the stable release starting [09/10/25].\n * Automatic migration from the old format to the new format will begin on\n   [09/17/25].\n\nFor details on the new, recommended format, please see the current Configuration\ndocumentation.\n\nVeCLI offers several ways to configure its behavior, including environment\nvariables, command-line arguments, and settings files. This document outlines\nthe different configuration methods and available settings.\n\n\nConfiguration layers#\n\nConfiguration is applied in the following order of precedence (lower numbers are\noverridden by higher numbers):\n\n 1. Default values: Hardcoded defaults within the application.\n 2. System defaults file: System-wide default settings that can be overridden by\n    other settings files.\n 3. User settings file: Global settings for the current user.\n 4. Project settings file: Project-specific settings.\n 5. System settings file: System-wide settings that override all other settings\n    files.\n 6. Environment variables: System-wide or session-specific variables,\n    potentially loaded from .env files.\n 7. Command-line arguments: Values passed when launching the CLI.\n\n\nSettings files#\n\nVeCLI uses JSON settings files for persistent configuration. There are four\nlocations for these files:\n\n * System defaults file:\n   * Location: /etc/gemini-cli/system-defaults.json (Linux),\n     C:\\ProgramData\\gemini-cli\\system-defaults.json (Windows) or\n     /Library/Application Support/GeminiCli/system-defaults.json (macOS). The\n     path can be overridden using the GEMINI_CLI_SYSTEM_DEFAULTS_PATH\n     environment variable.\n   * Scope: Provides a base layer of system-wide default settings. These\n     settings have the lowest precedence and are intended to be overridden by\n     user, project, or system override settings.\n * User settings file:\n   * Location: ~/.ve/settings.json (where ~ is your home directory).\n   * Scope: Applies to all VeCLI sessions for the current user. User settings\n     override system defaults.\n * Project settings file:\n   * Location: .ve/settings.json within your project's root directory.\n   * Scope: Applies only when running VeCLI from that specific project. Project\n     settings override user settings and system defaults.\n * System settings file:\n   * Location: /etc/gemini-cli/settings.json (Linux),\n     C:\\ProgramData\\gemini-cli\\settings.json (Windows) or /Library/Application\n     Support/GeminiCli/settings.json (macOS). The path can be overridden using\n     the GEMINI_CLI_SYSTEM_SETTINGS_PATH environment variable.\n   * Scope: Applies to all VeCLI sessions on the system, for all users. System\n     settings act as overrides, taking precedence over all other settings files.\n     May be useful for system administrators at enterprises to have controls\n     over users' VeCLI setups.\n\nNote on environment variables in settings: String values within your\nsettings.json files can reference environment variables using either $VAR_NAME\nor ${VAR_NAME} syntax. These variables will be automatically resolved when the\nsettings are loaded. For example, if you have an environment variable\nMY_API_TOKEN, you could use it in settings.json like this: \"apiKey\":\n\"$MY_API_TOKEN\".\n\n> Note for Enterprise Users: For guidance on deploying and managing VeCLI in a\n> corporate environment, please see the Enterprise Configuration documentation.\n\n\nThe .ve directory in your project#\n\nIn addition to a project settings file, a project's .ve directory can contain\nother project-specific files related to VeCLI's operation, such as:\n\n * Custom sandbox profiles (e.g., .ve/sandbox-macos-custom.sb,\n   .ve/sandbox.Dockerfile).\n\n\nAvailable settings in settings.json:#\n\n * contextFileName (string or array of strings):\n   \n   * Description: Specifies the filename for context files (e.g., VE.md,\n     AGENTS.md). Can be a single filename or a list of accepted filenames.\n   * Default: VE.md\n   * Example: \"contextFileName\": \"AGENTS.md\"\n\n * bugCommand (object):\n   \n   * Description: Overrides the default URL for the /bug command.\n   * Default: \"urlTemplate\":\n     \"https://github.com/google-gemini/gemini-cli/issues/new?template=bug_report\n     .yml&title={title}&info={info}\"\n   * Properties:\n     * urlTemplate (string): A URL that can contain {title} and {info}\n       placeholders.\n   * Example:\n     \n     \n\n * fileFiltering (object):\n   \n   * Description: Controls git-aware file filtering behavior for @ commands and\n     file discovery tools.\n   * Default: \"respectGitIgnore\": true, \"enableRecursiveFileSearch\": true\n   * Properties:\n     * respectGitIgnore (boolean): Whether to respect .gitignore patterns when\n       discovering files. When set to true, git-ignored files (like\n       node_modules/, dist/, .env) are automatically excluded from @ commands\n       and file listing operations.\n     * enableRecursiveFileSearch (boolean): Whether to enable searching\n       recursively for filenames under the current tree when completing @\n       prefixes in the prompt.\n     * disableFuzzySearch (boolean): When true, disables the fuzzy search\n       capabilities when searching for files, which can improve performance on\n       projects with a large number of files.\n   * Example:\n     \n     \n\n\nTroubleshooting File Search Performance#\n\nIf you are experiencing performance issues with file searching (e.g., with @\ncompletions), especially in projects with a very large number of files, here are\na few things you can try in order of recommendation:\n\n 1. Use .veignore: Create a .veignore file in your project root to exclude\n    directories that contain a large number of files that you don't need to\n    reference (e.g., build artifacts, logs, node_modules). Reducing the total\n    number of files crawled is the most effective way to improve performance.\n\n 2. Disable Fuzzy Search: If ignoring files is not enough, you can disable fuzzy\n    search by setting disableFuzzySearch to true in your settings.json file.\n    This will use a simpler, non-fuzzy matching algorithm, which can be faster.\n\n 3. Disable Recursive File Search: As a last resort, you can disable recursive\n    file search entirely by setting enableRecursiveFileSearch to false. This\n    will be the fastest option as it avoids a recursive crawl of your project.\n    However, it means you will need to type the full path to files when using @\n    completions.\n\n * coreTools (array of strings):\n   \n   * Description: Allows you to specify a list of core tool names that should be\n     made available to the model. This can be used to restrict the set of\n     built-in tools. See Built-in Tools for a list of core tools. You can also\n     specify command-specific restrictions for tools that support it, like the\n     ShellTool. For example, \"coreTools\": [\"ShellTool(ls -l)\"] will only allow\n     the ls -l command to be executed.\n   * Default: All tools available for use by the Gemini model.\n   * Example: \"coreTools\": [\"ReadFileTool\", \"GlobTool\", \"ShellTool(ls)\"].\n\n * allowedTools (array of strings):\n   \n   * Default: undefined\n   * Description: A list of tool names that will bypass the confirmation dialog.\n     This is useful for tools that you trust and use frequently. The match\n     semantics are the same as coreTools.\n   * Example: \"allowedTools\": [\"ShellTool(git status)\"].\n\n * excludeTools (array of strings):\n   \n   * Description: Allows you to specify a list of core tool names that should be\n     excluded from the model. A tool listed in both excludeTools and coreTools\n     is excluded. You can also specify command-specific restrictions for tools\n     that support it, like the ShellTool. For example, \"excludeTools\":\n     [\"ShellTool(rm -rf)\"] will block the rm -rf command.\n   * Default: No tools excluded.\n   * Example: \"excludeTools\": [\"run_shell_command\", \"findFiles\"].\n   * Security Note: Command-specific restrictions in excludeTools for\n     run_shell_command are based on simple string matching and can be easily\n     bypassed. This feature is not a security mechanism and should not be relied\n     upon to safely execute untrusted code. It is recommended to use coreTools\n     to explicitly select commands that can be executed.\n\n * allowMCPServers (array of strings):\n   \n   * Description: Allows you to specify a list of MCP server names that should\n     be made available to the model. This can be used to restrict the set of MCP\n     servers to connect to. Note that this will be ignored if\n     --allowed-mcp-server-names is set.\n   * Default: All MCP servers are available for use by the Gemini model.\n   * Example: \"allowMCPServers\": [\"myPythonServer\"].\n   * Security Note: This uses simple string matching on MCP server names, which\n     can be modified. If you're a system administrator looking to prevent users\n     from bypassing this, consider configuring the mcpServers at the system\n     settings level such that the user will not be able to configure any MCP\n     servers of their own. This should not be used as an airtight security\n     mechanism.\n\n * excludeMCPServers (array of strings):\n   \n   * Description: Allows you to specify a list of MCP server names that should\n     be excluded from the model. A server listed in both excludeMCPServers and\n     allowMCPServers is excluded. Note that this will be ignored if\n     --allowed-mcp-server-names is set.\n   * Default: No MCP servers excluded.\n   * Example: \"excludeMCPServers\": [\"myNodeServer\"].\n   * Security Note: This uses simple string matching on MCP server names, which\n     can be modified. If you're a system administrator looking to prevent users\n     from bypassing this, consider configuring the mcpServers at the system\n     settings level such that the user will not be able to configure any MCP\n     servers of their own. This should not be used as an airtight security\n     mechanism.\n\n * autoAccept (boolean):\n   \n   * Description: Controls whether the CLI automatically accepts and executes\n     tool calls that are considered safe (e.g., read-only operations) without\n     explicit user confirmation. If set to true, the CLI will bypass the\n     confirmation prompt for tools deemed safe.\n   * Default: false\n   * Example: \"autoAccept\": true\n\n * theme (string):\n   \n   * Description: Sets the visual theme for VeCLI.\n   * Default: \"Default\"\n   * Example: \"theme\": \"GitHub\"\n\n * vimMode (boolean):\n   \n   * Description: Enables or disables vim mode for input editing. When enabled,\n     the input area supports vim-style navigation and editing commands with\n     NORMAL and INSERT modes. The vim mode status is displayed in the footer and\n     persists between sessions.\n   * Default: false\n   * Example: \"vimMode\": true\n\n * sandbox (boolean or string):\n   \n   * Description: Controls whether and how to use sandboxing for tool execution.\n     If set to true, VeCLI uses a pre-built gemini-cli-sandbox Docker image. For\n     more information, see Sandboxing.\n   * Default: false\n   * Example: \"sandbox\": \"docker\"\n\n * toolDiscoveryCommand (string):\n   \n   * Description: Defines a custom shell command for discovering tools from your\n     project. The shell command must return on stdout a JSON array of function\n     declarations. Tool wrappers are optional.\n   * Default: Empty\n   * Example: \"toolDiscoveryCommand\": \"bin/get_tools\"\n\n * toolCallCommand (string):\n   \n   * Description: Defines a custom shell command for calling a specific tool\n     that was discovered using toolDiscoveryCommand. The shell command must meet\n     the following criteria:\n     * It must take function name (exactly as in function declaration) as first\n       command line argument.\n     * It must read function arguments as JSON on stdin, analogous to\n       functionCall.args.\n     * It must return function output as JSON on stdout, analogous to\n       functionResponse.response.content.\n   * Default: Empty\n   * Example: \"toolCallCommand\": \"bin/call_tool\"\n\n * mcpServers (object):\n   \n   * Description: Configures connections to one or more Model-Context Protocol\n     (MCP) servers for discovering and using custom tools. VeCLI attempts to\n     connect to each configured MCP server to discover available tools. If\n     multiple MCP servers expose a tool with the same name, the tool names will\n     be prefixed with the server alias you defined in the configuration (e.g.,\n     serverAlias__actualToolName) to avoid conflicts. Note that the system might\n     strip certain schema properties from MCP tool definitions for\n     compatibility. At least one of command, url, or httpUrl must be provided.\n     If multiple are specified, the order of precedence is httpUrl, then url,\n     then command.\n   * Default: Empty\n   * Properties:\n     * <SERVER_NAME> (object): The server parameters for the named server.\n       * command (string, optional): The command to execute to start the MCP\n         server via standard I/O.\n       * args (array of strings, optional): Arguments to pass to the command.\n       * env (object, optional): Environment variables to set for the server\n         process.\n       * cwd (string, optional): The working directory in which to start the\n         server.\n       * url (string, optional): The URL of an MCP server that uses Server-Sent\n         Events (SSE) for communication.\n       * httpUrl (string, optional): The URL of an MCP server that uses\n         streamable HTTP for communication.\n       * headers (object, optional): A map of HTTP headers to send with requests\n         to url or httpUrl.\n       * timeout (number, optional): Timeout in milliseconds for requests to\n         this MCP server.\n       * trust (boolean, optional): Trust this server and bypass all tool call\n         confirmations.\n       * description (string, optional): A brief description of the server,\n         which may be used for display purposes.\n       * includeTools (array of strings, optional): List of tool names to\n         include from this MCP server. When specified, only the tools listed\n         here will be available from this server (allowlist behavior). If not\n         specified, all tools from the server are enabled by default.\n       * excludeTools (array of strings, optional): List of tool names to\n         exclude from this MCP server. Tools listed here will not be available\n         to the model, even if they are exposed by the server. Note:\n         excludeTools takes precedence over includeTools - if a tool is in both\n         lists, it will be excluded.\n   * Example:\n     \n     \n\n * checkpointing (object):\n   \n   * Description: Configures the checkpointing feature, which allows you to save\n     and restore conversation and file states. See the Checkpointing\n     documentation for more details.\n   * Default: {\"enabled\": false}\n   * Properties:\n     * enabled (boolean): When true, the /restore command is available.\n\n * preferredEditor (string):\n   \n   * Description: Specifies the preferred editor to use for viewing diffs.\n   * Default: vscode\n   * Example: \"preferredEditor\": \"vscode\"\n\n * telemetry (object)\n   \n   * Description: Configures logging and metrics collection for VeCLI. For more\n     information, see Telemetry.\n   * Default: {\"enabled\": false, \"target\": \"local\", \"otlpEndpoint\":\n     \"http://localhost:4317\", \"logPrompts\": true}\n   * Properties:\n     * enabled (boolean): Whether or not telemetry is enabled.\n     * target (string): The destination for collected telemetry. Supported\n       values are local and gcp.\n     * otlpEndpoint (string): The endpoint for the OTLP Exporter.\n     * logPrompts (boolean): Whether or not to include the content of user\n       prompts in the logs.\n   * Example:\n     \n     \n\n * usageStatisticsEnabled (boolean):\n   \n   * Description: Enables or disables the collection of usage statistics. See\n     Usage Statistics for more information.\n   * Default: true\n   * Example:\n     \n     \n\n * hideTips (boolean):\n   \n   * Description: Enables or disables helpful tips in the CLI interface.\n   \n   * Default: false\n   \n   * Example:\n     \n     \n\n * hideBanner (boolean):\n   \n   * Description: Enables or disables the startup banner (ASCII art logo) in the\n     CLI interface.\n   \n   * Default: false\n   \n   * Example:\n     \n     \n\n * maxSessionTurns (number):\n   \n   * Description: Sets the maximum number of turns for a session. If the session\n     exceeds this limit, the CLI will stop processing and start a new chat.\n   * Default: -1 (unlimited)\n   * Example:\n     \n     \n\n * summarizeToolOutput (object):\n   \n   * Description: Enables or disables the summarization of tool output. You can\n     specify the token budget for the summarization using the tokenBudget\n     setting.\n   * Note: Currently only the run_shell_command tool is supported.\n   * Default: {} (Disabled by default)\n   * Example:\n     \n     \n\n * excludedProjectEnvVars (array of strings):\n   \n   * Description: Specifies environment variables that should be excluded from\n     being loaded from project .env files. This prevents project-specific\n     environment variables (like DEBUG=true) from interfering with gemini-cli\n     behavior. Variables from .ve/.env files are never excluded.\n   * Default: [\"DEBUG\", \"DEBUG_MODE\"]\n   * Example:\n     \n     \n\n * includeDirectories (array of strings):\n   \n   * Description: Specifies an array of additional absolute or relative paths to\n     include in the workspace context. Missing directories will be skipped with\n     a warning by default. Paths can use ~ to refer to the user's home\n     directory. This setting can be combined with the --include-directories\n     command-line flag.\n   * Default: []\n   * Example:\n     \n     \n\n * loadMemoryFromIncludeDirectories (boolean):\n   \n   * Description: Controls the behavior of the /memory refresh command. If set\n     to true, VE.md files should be loaded from all directories that are added.\n     If set to false, VE.md should only be loaded from the current directory.\n   * Default: false\n   * Example:\n     \n     \n\n * chatCompression (object):\n   \n   * Description: Controls the settings for chat history compression, both\n     automatic and when manually invoked through the /compress command.\n   * Properties:\n     * contextPercentageThreshold (number): A value between 0 and 1 that\n       specifies the token threshold for compression as a percentage of the\n       model's total token limit. For example, a value of 0.6 will trigger\n       compression when the chat history exceeds 60% of the token limit.\n   * Example:\n     \n     \n\n * showLineNumbers (boolean):\n   \n   * Description: Controls whether line numbers are displayed in code blocks in\n     the CLI output.\n   * Default: true\n   * Example:\n     \n     \n\n * accessibility (object):\n   \n   * Description: Configures accessibility features for the CLI.\n   * Properties:\n     * screenReader (boolean): Enables screen reader mode, which adjusts the TUI\n       for better compatibility with screen readers. This can also be enabled\n       with the --screen-reader command-line flag, which will take precedence\n       over the setting.\n     * disableLoadingPhrases (boolean): Disables the display of loading phrases\n       during operations.\n   * Default: {\"screenReader\": false, \"disableLoadingPhrases\": false}\n   * Example:\n     \n     \n\n\nExample settings.json:#\n\n\n\n\nShell History#\n\nThe CLI keeps a history of shell commands you run. To avoid conflicts between\ndifferent projects, this history is stored in a project-specific directory\nwithin your user's home folder.\n\n * Location: ~/.ve/tmp/<project_hash>/shell_history\n   * <project_hash> is a unique identifier generated from your project's root\n     path.\n   * The history is stored in a file named shell_history.\n\n\nEnvironment Variables & .env Files#\n\nEnvironment variables are a common way to configure applications, especially for\nsensitive information like API keys or for settings that might change between\nenvironments. For authentication setup, see the Authentication documentation\nwhich covers all available authentication methods.\n\nThe CLI automatically loads environment variables from an .env file. The loading\norder is:\n\n 1. .env file in the current working directory.\n 2. If not found, it searches upwards in parent directories until it finds an\n    .env file or reaches the project root (identified by a .git folder) or the\n    home directory.\n 3. If still not found, it looks for ~/.env (in the user's home directory).\n\nEnvironment Variable Exclusion: Some environment variables (like DEBUG and\nDEBUG_MODE) are automatically excluded from being loaded from project .env files\nto prevent interference with gemini-cli behavior. Variables from .ve/.env files\nare never excluded. You can customize this behavior using the\nexcludedProjectEnvVars setting in your settings.json file.\n\n * GEMINI_API_KEY:\n   * Your API key for the Gemini API.\n   * One of several available authentication methods.\n   * Set this in your shell profile (e.g., ~/.bashrc, ~/.zshrc) or an .env file.\n * GEMINI_MODEL:\n   * Specifies the default Gemini model to use.\n   * Overrides the hardcoded default\n   * Example: export GEMINI_MODEL=\"gemini-2.5-flash\"\n * GOOGLE_API_KEY:\n   * Your Google Cloud API key.\n   * Required for using Vertex AI in express mode.\n   * Ensure you have the necessary permissions.\n   * Example: export GOOGLE_API_KEY=\"YOUR_GOOGLE_API_KEY\".\n * GOOGLE_CLOUD_PROJECT:\n   * Your Google Cloud Project ID.\n   * Required for using Code Assist or Vertex AI.\n   * If using Vertex AI, ensure you have the necessary permissions in this\n     project.\n   * Cloud Shell Note: When running in a Cloud Shell environment, this variable\n     defaults to a special project allocated for Cloud Shell users. If you have\n     GOOGLE_CLOUD_PROJECT set in your global environment in Cloud Shell, it will\n     be overridden by this default. To use a different project in Cloud Shell,\n     you must define GOOGLE_CLOUD_PROJECT in a .env file.\n   * Example: export GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_ID\".\n * GOOGLE_APPLICATION_CREDENTIALS (string):\n   * Description: The path to your Google Application Credentials JSON file.\n   * Example: export\n     GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your/credentials.json\"\n * OTLP_GOOGLE_CLOUD_PROJECT:\n   * Your Google Cloud Project ID for Telemetry in Google Cloud\n   * Example: export OTLP_GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_ID\".\n * GOOGLE_CLOUD_LOCATION:\n   * Your Google Cloud Project Location (e.g., us-central1).\n   * Required for using Vertex AI in non express mode.\n   * Example: export GOOGLE_CLOUD_LOCATION=\"YOUR_PROJECT_LOCATION\".\n * GEMINI_SANDBOX:\n   * Alternative to the sandbox setting in settings.json.\n   * Accepts true, false, docker, podman, or a custom command string.\n * SEATBELT_PROFILE (macOS specific):\n   * Switches the Seatbelt (sandbox-exec) profile on macOS.\n   * permissive-open: (Default) Restricts writes to the project folder (and a\n     few other folders, see\n     packages/cli/src/utils/sandbox-macos-permissive-open.sb) but allows other\n     operations.\n   * strict: Uses a strict profile that declines operations by default.\n   * <profile_name>: Uses a custom profile. To define a custom profile, create a\n     file named sandbox-macos-<profile_name>.sb in your project's .ve/ directory\n     (e.g., my-project/.ve/sandbox-macos-custom.sb).\n * DEBUG or DEBUG_MODE (often used by underlying libraries or the CLI itself):\n   * Set to true or 1 to enable verbose debug logging, which can be helpful for\n     troubleshooting.\n   * Note: These variables are automatically excluded from project .env files by\n     default to prevent interference with gemini-cli behavior. Use .ve/.env\n     files if you need to set these for gemini-cli specifically.\n * NO_COLOR:\n   * Set to any value to disable all color output in the CLI.\n * CLI_TITLE:\n   * Set to a string to customize the title of the CLI.\n * CODE_ASSIST_ENDPOINT:\n   * Specifies the endpoint for the code assist server.\n   * This is useful for development and testing.\n\n\nCommand-Line Arguments#\n\nArguments passed directly when running the CLI can override other configurations\nfor that specific session.\n\n * --model <model_name> (-m <model_name>):\n   * Specifies the Gemini model to use for this session.\n   * Example: npm start -- --model gemini-1.5-pro-latest\n * --prompt <your_prompt> (-p <your_prompt>):\n   * Used to pass a prompt directly to the command. This invokes VeCLI in a\n     non-interactive mode.\n * --prompt-interactive <your_prompt> (-i <your_prompt>):\n   * Starts an interactive session with the provided prompt as the initial\n     input.\n   * The prompt is processed within the interactive session, not before it.\n   * Cannot be used when piping input from stdin.\n   * Example: gemini -i \"explain this code\"\n * --sandbox (-s):\n   * Enables sandbox mode for this session.\n * --sandbox-image:\n   * Sets the sandbox image URI.\n * --debug (-d):\n   * Enables debug mode for this session, providing more verbose output.\n * --all-files (-a):\n   * If set, recursively includes all files within the current directory as\n     context for the prompt.\n * --help (or -h):\n   * Displays help information about command-line arguments.\n * --show-memory-usage:\n   * Displays the current memory usage.\n * --yolo:\n   * Enables YOLO mode, which automatically approves all tool calls.\n * --approval-mode <mode>:\n   * Sets the approval mode for tool calls. Available modes:\n     * default: Prompt for approval on each tool call (default behavior)\n     * auto_edit: Automatically approve edit tools (replace, write_file) while\n       prompting for others\n     * yolo: Automatically approve all tool calls (equivalent to --yolo)\n   * Cannot be used together with --yolo. Use --approval-mode=yolo instead of\n     --yolo for the new unified approach.\n   * Example: gemini --approval-mode auto_edit\n * --allowed-tools <tool1,tool2,...>:\n   * A comma-separated list of tool names that will bypass the confirmation\n     dialog.\n   * Example: gemini --allowed-tools \"ShellTool(git status)\"\n * --telemetry:\n   * Enables telemetry.\n * --telemetry-target:\n   * Sets the telemetry target. See telemetry for more information.\n * --telemetry-otlp-endpoint:\n   * Sets the OTLP endpoint for telemetry. See telemetry for more information.\n * --telemetry-otlp-protocol:\n   * Sets the OTLP protocol for telemetry (grpc or http). Defaults to grpc. See\n     telemetry for more information.\n * --telemetry-log-prompts:\n   * Enables logging of prompts for telemetry. See telemetry for more\n     information.\n * --checkpointing:\n   * Enables checkpointing.\n * --extensions <extension_name ...> (-e <extension_name ...>):\n   * Specifies a list of extensions to use for the session. If not provided, all\n     available extensions are used.\n   * Use the special term gemini -e none to disable all extensions.\n   * Example: gemini -e my-extension -e my-other-extension\n * --list-extensions (-l):\n   * Lists all available extensions and exits.\n * --proxy:\n   * Sets the proxy for the CLI.\n   * Example: --proxy http://localhost:7890.\n * --include-directories <dir1,dir2,...>:\n   * Includes additional directories in the workspace for multi-directory\n     support.\n   * Can be specified multiple times or as comma-separated values.\n   * 5 directories can be added at maximum.\n   * Example: --include-directories /path/to/project1,/path/to/project2 or\n     --include-directories /path/to/project1 --include-directories\n     /path/to/project2\n * --screen-reader:\n   * Enables screen reader mode for accessibility.\n * --version:\n   * Displays the version of the CLI.\n\n\nContext Files (Hierarchical Instructional Context)#\n\nWhile not strictly configuration for the CLI's behavior, context files\n(defaulting to GEMINI.md but configurable via the contextFileName setting) are\ncrucial for configuring the instructional context (also referred to as \"memory\")\nprovided to the Gemini model. This powerful feature allows you to give\nproject-specific instructions, coding style guides, or any relevant background\ninformation to the AI, making its responses more tailored and accurate to your\nneeds. The CLI includes UI elements, such as an indicator in the footer showing\nthe number of loaded context files, to keep you informed about the active\ncontext.\n\n * Purpose: These Markdown files contain instructions, guidelines, or context\n   that you want the Gemini model to be aware of during your interactions. The\n   system is designed to manage this instructional context hierarchically.\n\n\nExample Context File Content (e.g., GEMINI.md)#\n\nHere's a conceptual example of what a context file at the root of a TypeScript\nproject might contain:\n\n\n\nThis example demonstrates how you can provide general project context, specific\ncoding conventions, and even notes about particular files or components. The\nmore relevant and precise your context files are, the better the AI can assist\nyou. Project-specific context files are highly encouraged to establish\nconventions and context.\n\n * Hierarchical Loading and Precedence: The CLI implements a sophisticated\n   hierarchical memory system by loading context files (e.g., GEMINI.md) from\n   several locations. Content from files lower in this list (more specific)\n   typically overrides or supplements content from files higher up (more\n   general). The exact concatenation order and final context can be inspected\n   using the /memory show command. The typical loading order is:\n   1. Global Context File:\n      * Location: ~/.ve/<contextFileName> (e.g., ~/.ve/GEMINI.md in your user\n        home directory).\n      * Scope: Provides default instructions for all your projects.\n   2. Project Root & Ancestors Context Files:\n      * Location: The CLI searches for the configured context file in the\n        current working directory and then in each parent directory up to either\n        the project root (identified by a .git folder) or your home directory.\n      * Scope: Provides context relevant to the entire project or a significant\n        portion of it.\n   3. Sub-directory Context Files (Contextual/Local):\n      * Location: The CLI also scans for the configured context file in\n        subdirectories below the current working directory (respecting common\n        ignore patterns like node_modules, .git, etc.). The breadth of this\n        search is limited to 200 directories by default, but can be configured\n        with a memoryDiscoveryMaxDirs field in your settings.json file.\n      * Scope: Allows for highly specific instructions relevant to a particular\n        component, module, or subsection of your project.\n * Concatenation & UI Indication: The contents of all found context files are\n   concatenated (with separators indicating their origin and path) and provided\n   as part of the system prompt to the Gemini model. The CLI footer displays the\n   count of loaded context files, giving you a quick visual cue about the active\n   instructional context.\n * Importing Content: You can modularize your context files by importing other\n   Markdown files using the @path/to/file.md syntax. For more details, see the\n   Memory Import Processor documentation.\n * Commands for Memory Management:\n   * Use /memory refresh to force a re-scan and reload of all context files from\n     all configured locations. This updates the AI's instructional context.\n   * Use /memory show to display the combined instructional context currently\n     loaded, allowing you to verify the hierarchy and content being used by the\n     AI.\n   * See the Commands documentation for full details on the /memory command and\n     its sub-commands (show and refresh).\n\nBy understanding and utilizing these configuration layers and the hierarchical\nnature of context files, you can effectively manage the AI's memory and tailor\nthe VeCLI's responses to your specific needs and projects.\n\n\nSandboxing#\n\nThe VeCLI can execute potentially unsafe operations (like shell commands and\nfile modifications) within a sandboxed environment to protect your system.\n\nSandboxing is disabled by default, but you can enable it in a few ways:\n\n * Using --sandbox or -s flag.\n * Setting GEMINI_SANDBOX environment variable.\n * Sandbox is enabled when using --yolo or --approval-mode=yolo by default.\n\nBy default, it uses a pre-built gemini-cli-sandbox Docker image.\n\nFor project-specific sandboxing needs, you can create a custom Dockerfile at\n.ve/sandbox.Dockerfile in your project's root directory. This Dockerfile can be\nbased on the base sandbox image:\n\n\n\nWhen .ve/sandbox.Dockerfile exists, you can use BUILD_SANDBOX environment\nvariable when running VeCLI to automatically build the custom sandbox image:\n\n\n\n\nUsage Statistics#\n\nTo help us improve the VeCLI, we collect anonymized usage statistics. This data\nhelps us understand how the CLI is used, identify common issues, and prioritize\nnew features.\n\nWhat we collect:\n\n * Tool Calls: We log the names of the tools that are called, whether they\n   succeed or fail, and how long they take to execute. We do not collect the\n   arguments passed to the tools or any data returned by them.\n * API Requests: We log the Gemini model used for each request, the duration of\n   the request, and whether it was successful. We do not collect the content of\n   the prompts or responses.\n * Session Information: We collect information about the configuration of the\n   CLI, such as the enabled tools and the approval mode.\n\nWhat we DON'T collect:\n\n * Personally Identifiable Information (PII): We do not collect any personal\n   information, such as your name, email address, or API keys.\n * Prompt and Response Content: We do not log the content of your prompts or the\n   responses from the Gemini model.\n * File Content: We do not log the content of any files that are read or written\n   by the CLI.\n\nHow to opt out:\n\nYou can opt out of usage statistics collection at any time by setting the\nusageStatisticsEnabled property to false in your settings.json file:\n\n","routePath":"/en/cli/configuration-v1","lang":"","toc":[{"text":"Configuration layers","id":"configuration-layers","depth":2,"charIndex":630},{"text":"Settings files","id":"settings-files","depth":2,"charIndex":1321},{"text":"The `.ve` directory in your project","id":"the-ve-directory-in-your-project","depth":3,"charIndex":-1},{"text":"Available settings in `settings.json`:","id":"available-settings-in-settingsjson","depth":3,"charIndex":-1},{"text":"Troubleshooting File Search Performance","id":"troubleshooting-file-search-performance","depth":3,"charIndex":5371},{"text":"Example `settings.json`:","id":"example-settingsjson","depth":3,"charIndex":-1},{"text":"Shell History","id":"shell-history","depth":2,"charIndex":19342},{"text":"Environment Variables & `.env` Files","id":"environment-variables--env-files","depth":2,"charIndex":-1},{"text":"Command-Line Arguments","id":"command-line-arguments","depth":2,"charIndex":24011},{"text":"Context Files (Hierarchical Instructional Context)","id":"context-files-hierarchical-instructional-context","depth":2,"charIndex":27572},{"text":"Example Context File Content (e.g., `GEMINI.md`)","id":"example-context-file-content-eg-geminimd","depth":3,"charIndex":-1},{"text":"Sandboxing","id":"sandboxing","depth":2,"charIndex":31812},{"text":"Usage Statistics","id":"usage-statistics","depth":2,"charIndex":32621}],"domain":"","frontmatter":{},"version":""},{"id":21,"title":"VeCLI Configuration","content":"#\n\nNote on New Configuration Format\n\nThe format of the settings.json file has been updated to a new, more organized\nstructure.\n\n * The new format will be supported in the stable release starting [09/10/25].\n * Automatic migration from the old format to the new format will begin on\n   [09/17/25].\n\nFor details on the previous format, please see the v1 Configuration\ndocumentation.\n\nVeCLI offers several ways to configure its behavior, including environment\nvariables, command-line arguments, and settings files. This document outlines\nthe different configuration methods and available settings.\n\n\nConfiguration layers#\n\nConfiguration is applied in the following order of precedence (lower numbers are\noverridden by higher numbers):\n\n 1. Default values: Hardcoded defaults within the application.\n 2. System defaults file: System-wide default settings that can be overridden by\n    other settings files.\n 3. User settings file: Global settings for the current user.\n 4. Project settings file: Project-specific settings.\n 5. System settings file: System-wide settings that override all other settings\n    files.\n 6. Environment variables: System-wide or session-specific variables,\n    potentially loaded from .env files.\n 7. Command-line arguments: Values passed when launching the CLI.\n\n\nSettings files#\n\nVeCLI uses JSON settings files for persistent configuration. There are four\nlocations for these files:\n\n * System defaults file:\n   * Location: /etc/vecli/system-defaults.json (Linux),\n     C:\\ProgramData\\vecli\\system-defaults.json (Windows) or /Library/Application\n     Support/GeminiCli/system-defaults.json (macOS). The path can be overridden\n     using the VE_CLI_SYSTEM_DEFAULTS_PATH environment variable.\n   * Scope: Provides a base layer of system-wide default settings. These\n     settings have the lowest precedence and are intended to be overridden by\n     user, project, or system override settings.\n * User settings file:\n   * Location: ~/.ve/settings.json (where ~ is your home directory).\n   * Scope: Applies to all VeCLI sessions for the current user. User settings\n     override system defaults.\n * Project settings file:\n   * Location: .ve/settings.json within your project's root directory.\n   * Scope: Applies only when running VeCLI from that specific project. Project\n     settings override user settings and system defaults.\n * System settings file:\n   * Location: /etc/vecli/settings.json (Linux),\n     C:\\ProgramData\\vecli\\settings.json (Windows) or /Library/Application\n     Support/VeCli/settings.json (macOS). The path can be overridden using the\n     VE_CLI_SYSTEM_SETTINGS_PATH environment variable.\n   * Scope: Applies to all VeCLI sessions on the system, for all users. System\n     settings act as overrides, taking precedence over all other settings files.\n     May be useful for system administrators at enterprises to have controls\n     over users' VeCLI setups.\n\nNote on environment variables in settings: String values within your\nsettings.json files can reference environment variables using either $VAR_NAME\nor ${VAR_NAME} syntax. These variables will be automatically resolved when the\nsettings are loaded. For example, if you have an environment variable\nMY_API_TOKEN, you could use it in settings.json like this: \"apiKey\":\n\"$MY_API_TOKEN\".\n\n> Note for Enterprise Users: For guidance on deploying and managing VeCLI in a\n> corporate environment, please see the Enterprise Configuration documentation.\n\n\nThe .ve directory in your project#\n\nIn addition to a project settings file, a project's .ve directory can contain\nother project-specific files related to VeCLI's operation, such as:\n\n * Custom sandbox profiles (e.g., .ve/sandbox-macos-custom.sb,\n   .ve/sandbox.Dockerfile).\n\n\nAvailable settings in settings.json#\n\nSettings are organized into categories. All settings should be placed within\ntheir corresponding top-level category object in your settings.json file.\n\ngeneral#\n\n * general.preferredEditor (string):\n   \n   * Description: The preferred editor to open files in.\n   * Default: undefined\n\n * general.vimMode (boolean):\n   \n   * Description: Enable Vim keybindings.\n   * Default: false\n\n * general.disableAutoUpdate (boolean):\n   \n   * Description: Disable automatic updates.\n   * Default: false\n\n * general.disableUpdateNag (boolean):\n   \n   * Description: Disable update notification prompts.\n   * Default: false\n\n * general.checkpointing.enabled (boolean):\n   \n   * Description: Enable session checkpointing for recovery.\n   * Default: false\n\nui#\n\n * ui.theme (string):\n   \n   * Description: The color theme for the UI. See Themes for available options.\n   * Default: undefined\n\n * ui.customThemes (object):\n   \n   * Description: Custom theme definitions.\n   * Default: {}\n\n * ui.hideWindowTitle (boolean):\n   \n   * Description: Hide the window title bar.\n   * Default: false\n\n * ui.hideTips (boolean):\n   \n   * Description: Hide helpful tips in the UI.\n   * Default: false\n\n * ui.hideBanner (boolean):\n   \n   * Description: Hide the application banner.\n   * Default: false\n\n * ui.hideFooter (boolean):\n   \n   * Description: Hide the footer from the UI.\n   * Default: false\n\n * ui.showMemoryUsage (boolean):\n   \n   * Description: Display memory usage information in the UI.\n   * Default: false\n\n * ui.showLineNumbers (boolean):\n   \n   * Description: Show line numbers in the chat.\n   * Default: false\n\n * ui.showCitations (boolean):\n   \n   * Description: Show citations for generated text in the chat.\n   * Default: false\n\n * ui.accessibility.disableLoadingPhrases (boolean):\n   \n   * Description: Disable loading phrases for accessibility.\n   * Default: false\n\nide#\n\n * ide.enabled (boolean):\n   \n   * Description: Enable IDE integration mode.\n   * Default: false\n\n * ide.hasSeenNudge (boolean):\n   \n   * Description: Whether the user has seen the IDE integration nudge.\n   * Default: false\n\nprivacy#\n\n * privacy.usageStatisticsEnabled (boolean):\n   * Description: Enable collection of usage statistics.\n   * Default: true\n\nmodel#\n\n * model.name (string):\n   \n   * Description: The model to use for conversations.\n   * Default: undefined\n\n * model.maxSessionTurns (number):\n   \n   * Description: Maximum number of user/model/tool turns to keep in a session.\n     -1 means unlimited.\n   * Default: -1\n\n * model.summarizeToolOutput (object):\n   \n   * Description: Enables or disables the summarization of tool output. You can\n     specify the token budget for the summarization using the tokenBudget\n     setting. Note: Currently only the run_shell_command tool is supported. For\n     example {\"run_shell_command\": {\"tokenBudget\": 2000}}\n   * Default: undefined\n\n * model.chatCompression.contextPercentageThreshold (number):\n   \n   * Description: Sets the threshold for chat history compression as a\n     percentage of the model's total token limit. This is a value between 0 and\n     1 that applies to both automatic compression and the manual /compress\n     command. For example, a value of 0.6 will trigger compression when the chat\n     history exceeds 60% of the token limit.\n   * Default: 0.7\n\n * model.skipNextSpeakerCheck (boolean):\n   \n   * Description: Skip the next speaker check.\n   * Default: false\n\ncontext#\n\n * context.fileName (string or array of strings):\n   \n   * Description: The name of the context file(s).\n   * Default: undefined\n\n * context.importFormat (string):\n   \n   * Description: The format to use when importing memory.\n   * Default: undefined\n\n * context.discoveryMaxDirs (number):\n   \n   * Description: Maximum number of directories to search for memory.\n   * Default: 200\n\n * context.includeDirectories (array):\n   \n   * Description: Additional directories to include in the workspace context.\n     Missing directories will be skipped with a warning.\n   * Default: []\n\n * context.loadFromIncludeDirectories (boolean):\n   \n   * Description: Controls the behavior of the /memory refresh command. If set\n     to true, VE.md files should be loaded from all directories that are added.\n     If set to false, VE.md should only be loaded from the current directory.\n   * Default: false\n\n * context.fileFiltering.respectGitIgnore (boolean):\n   \n   * Description: Respect .gitignore files when searching.\n   * Default: true\n\n * context.fileFiltering.respectGeminiIgnore (boolean):\n   \n   * Description: Respect .veignore files when searching.\n   * Default: true\n\n * context.fileFiltering.enableRecursiveFileSearch (boolean):\n   \n   * Description: Whether to enable searching recursively for filenames under\n     the current tree when completing @ prefixes in the prompt.\n   * Default: true\n\ntools#\n\n * tools.sandbox (boolean or string):\n   \n   * Description: Sandbox execution environment (can be a boolean or a path\n     string).\n   * Default: undefined\n\n * tools.usePty (boolean):\n   \n   * Description: Use node-pty for shell command execution. Fallback to\n     child_process still applies.\n   * Default: false\n\n * tools.core (array of strings):\n   \n   * Description: This can be used to restrict the set of built-in tools with an\n     allowlist. See Built-in Tools for a list of core tools. The match semantics\n     are the same as tools.allowed.\n   * Default: undefined\n\n * tools.exclude (array of strings):\n   \n   * Description: Tool names to exclude from discovery.\n   * Default: undefined\n\n * tools.allowed (array of strings):\n   \n   * Description: A list of tool names that will bypass the confirmation dialog.\n     This is useful for tools that you trust and use frequently. For example,\n     [\"run_shell_command(git)\", \"run_shell_command(npm test)\"] will skip the\n     confirmation dialog to run any git and npm test commands. See Shell Tool\n     command restrictions for details on prefix matching, command chaining, etc.\n   * Default: undefined\n\n * tools.discoveryCommand (string):\n   \n   * Description: Command to run for tool discovery.\n   * Default: undefined\n\n * tools.callCommand (string):\n   \n   * Description: Defines a custom shell command for calling a specific tool\n     that was discovered using tools.discoveryCommand. The shell command must\n     meet the following criteria:\n     * It must take function name (exactly as in function declaration) as first\n       command line argument.\n     * It must read function arguments as JSON on stdin, analogous to\n       functionCall.args.\n     * It must return function output as JSON on stdout, analogous to\n       functionResponse.response.content.\n   * Default: undefined\n\nmcp#\n\n * mcp.serverCommand (string):\n   \n   * Description: Command to start an MCP server.\n   * Default: undefined\n\n * mcp.allowed (array of strings):\n   \n   * Description: An allowlist of MCP servers to allow.\n   * Default: undefined\n\n * mcp.excluded (array of strings):\n   \n   * Description: A denylist of MCP servers to exclude.\n   * Default: undefined\n\nsecurity#\n\n * security.folderTrust.enabled (boolean):\n   \n   * Description: Setting to track whether Folder trust is enabled.\n   * Default: false\n\n * security.auth.selectedType (string):\n   \n   * Description: The currently selected authentication type.\n   * Default: undefined\n\n * security.auth.enforcedType (string):\n   \n   * Description: The required auth type (useful for enterprises).\n   * Default: undefined\n\n * security.auth.useExternal (boolean):\n   \n   * Description: Whether to use an external authentication flow.\n   * Default: undefined\n\nadvanced#\n\n * advanced.autoConfigureMemory (boolean):\n   \n   * Description: Automatically configure Node.js memory limits.\n   * Default: false\n\n * advanced.dnsResolutionOrder (string):\n   \n   * Description: The DNS resolution order.\n   * Default: undefined\n\n * advanced.excludedEnvVars (array of strings):\n   \n   * Description: Environment variables to exclude from project context.\n   * Default: [\"DEBUG\",\"DEBUG_MODE\"]\n\n * advanced.bugCommand (object):\n   \n   * Description: Configuration for the bug report command.\n   * Default: undefined\n\nmcpServers#\n\nConfigures connections to one or more Model-Context Protocol (MCP) servers for\ndiscovering and using custom tools. VeCLI attempts to connect to each configured\nMCP server to discover available tools. If multiple MCP servers expose a tool\nwith the same name, the tool names will be prefixed with the server alias you\ndefined in the configuration (e.g., serverAlias__actualToolName) to avoid\nconflicts. Note that the system might strip certain schema properties from MCP\ntool definitions for compatibility. At least one of command, url, or httpUrl\nmust be provided. If multiple are specified, the order of precedence is httpUrl,\nthen url, then command.\n\n * mcpServers.<SERVER_NAME> (object): The server parameters for the named\n   server.\n   * command (string, optional): The command to execute to start the MCP server\n     via standard I/O.\n   * args (array of strings, optional): Arguments to pass to the command.\n   * env (object, optional): Environment variables to set for the server\n     process.\n   * cwd (string, optional): The working directory in which to start the server.\n   * url (string, optional): The URL of an MCP server that uses Server-Sent\n     Events (SSE) for communication.\n   * httpUrl (string, optional): The URL of an MCP server that uses streamable\n     HTTP for communication.\n   * headers (object, optional): A map of HTTP headers to send with requests to\n     url or httpUrl.\n   * timeout (number, optional): Timeout in milliseconds for requests to this\n     MCP server.\n   * trust (boolean, optional): Trust this server and bypass all tool call\n     confirmations.\n   * description (string, optional): A brief description of the server, which\n     may be used for display purposes.\n   * includeTools (array of strings, optional): List of tool names to include\n     from this MCP server. When specified, only the tools listed here will be\n     available from this server (allowlist behavior). If not specified, all\n     tools from the server are enabled by default.\n   * excludeTools (array of strings, optional): List of tool names to exclude\n     from this MCP server. Tools listed here will not be available to the model,\n     even if they are exposed by the server. Note: excludeTools takes precedence\n     over includeTools - if a tool is in both lists, it will be excluded.\n\ntelemetry#\n\nConfigures logging and metrics collection for VeCLI. For more information, see\nTelemetry.\n\n * Properties:\n   * enabled (boolean): Whether or not telemetry is enabled.\n   * target (string): The destination for collected telemetry. Supported values\n     are local and gcp.\n   * otlpEndpoint (string): The endpoint for the OTLP Exporter.\n   * otlpProtocol (string): The protocol for the OTLP Exporter (grpc or http).\n   * logPrompts (boolean): Whether or not to include the content of user prompts\n     in the logs.\n   * outfile (string): The file to write telemetry to when target is local.\n\n\nExample settings.json#\n\nHere is an example of a settings.json file with the nested structure, new as of\nv0.3.0:\n\n\n\n\nShell History#\n\nThe CLI keeps a history of shell commands you run. To avoid conflicts between\ndifferent projects, this history is stored in a project-specific directory\nwithin your user's home folder.\n\n * Location: ~/.ve/tmp/<project_hash>/shell_history\n   * <project_hash> is a unique identifier generated from your project's root\n     path.\n   * The history is stored in a file named shell_history.\n\n\nEnvironment Variables & .env Files#\n\nEnvironment variables are a common way to configure applications, especially for\nsensitive information like API keys or for settings that might change between\nenvironments. For authentication setup, see the Authentication documentation\nwhich covers all available authentication methods.\n\nThe CLI automatically loads environment variables from an .env file. The loading\norder is:\n\n 1. .env file in the current working directory.\n 2. If not found, it searches upwards in parent directories until it finds an\n    .env file or reaches the project root (identified by a .git folder) or the\n    home directory.\n 3. If still not found, it looks for ~/.env (in the user's home directory).\n\nEnvironment Variable Exclusion: Some environment variables (like DEBUG and\nDEBUG_MODE) are automatically excluded from being loaded from project .env files\nto prevent interference with vecli behavior. Variables from .ve/.env files are\nnever excluded. You can customize this behavior using the\nadvanced.excludedEnvVars setting in your settings.json file.\n\n * GEMINI_API_KEY:\n   * Your API key for the Volcano Engine API.\n   * One of several available authentication methods.\n   * Set this in your shell profile (e.g., ~/.bashrc, ~/.zshrc) or an .env file.\n * GEMINI_MODEL:\n   * Specifies the default Volcano Engine model to use.\n   * Overrides the hardcoded default\n   * Example: export VECLI_MODEL=\"deepseek-v3-1\"\n * GOOGLE_API_KEY:\n   * Your Volcano Engine API key.\n   * Required for using Vertex AI in express mode.\n   * Ensure you have the necessary permissions.\n   * Example: export ARK_API_KEY=\"YOUR_Ark_API_KEY\".\n * GOOGLE_CLOUD_PROJECT:\n   * Your Volcano Engine Project ID.\n   * Required for using Code Assist or Vertex AI.\n   * If using Vertex AI, ensure you have the necessary permissions in this\n     project.\n   * Cloud Shell Note: When running in a Cloud Shell environment, this variable\n     defaults to a special project allocated for Cloud Shell users. If you have\n     VOLCANO_ENGINE_PROJECT set in your global environment in Cloud Shell, it\n     will be overridden by this default. To use a different project in Cloud\n     Shell, you must define VOLCANO_ENGINE_PROJECT in a .env file.\n   * Example: export VOLCANO_ENGINE_PROJECT=\"YOUR_PROJECT_ID\".\n * GOOGLE_APPLICATION_CREDENTIALS (string):\n   * Description: The path to your Volcano Engine Application Credentials JSON\n     file.\n   * Example: export\n     VOLCANO_ENGINE_APPLICATION_CREDENTIALS=\"/path/to/your/credentials.json\"\n * OTLP_VOLCANO_ENGINE_CLOUD_PROJECT:\n   * Your Volcano Engine Project ID for Telemetry in Volcano Engine\n   * Example: export OTLP_VOLCANO_ENGINE_PROJECT=\"YOUR_PROJECT_ID\".\n * VOLCANO_ENGINE_LOCATION:\n   * Your Volcano Engine Project Location (e.g., us-central1).\n   * Required for using Vertex AI in non express mode.\n   * Example: export VOLCANO_ENGINE_LOCATION=\"YOUR_PROJECT_LOCATION\".\n * GEMINI_SANDBOX:\n   * Alternative to the sandbox setting in settings.json.\n   * Accepts true, false, docker, podman, or a custom command string.\n * SEATBELT_PROFILE (macOS specific):\n   * Switches the Seatbelt (sandbox-exec) profile on macOS.\n   * permissive-open: (Default) Restricts writes to the project folder (and a\n     few other folders, see\n     packages/cli/src/utils/sandbox-macos-permissive-open.sb) but allows other\n     operations.\n   * strict: Uses a strict profile that declines operations by default.\n   * <profile_name>: Uses a custom profile. To define a custom profile, create a\n     file named sandbox-macos-<profile_name>.sb in your project's .ve/ directory\n     (e.g., my-project/.ve/sandbox-macos-custom.sb).\n * DEBUG or DEBUG_MODE (often used by underlying libraries or the CLI itself):\n   * Set to true or 1 to enable verbose debug logging, which can be helpful for\n     troubleshooting.\n   * Note: These variables are automatically excluded from project .env files by\n     default to prevent interference with vecli behavior. Use .ve/.env files if\n     you need to set these for vecli specifically.\n * NO_COLOR:\n   * Set to any value to disable all color output in the CLI.\n * CLI_TITLE:\n   * Set to a string to customize the title of the CLI.\n * CODE_ASSIST_ENDPOINT:\n   * Specifies the endpoint for the code assist server.\n   * This is useful for development and testing.\n\n\nCommand-Line Arguments#\n\nArguments passed directly when running the CLI can override other configurations\nfor that specific session.\n\n * --model <model_name> (-m <model_name>):\n   * Specifies the Volcano Engine model to use for this session.\n   * Example: npm start -- --model deepseek-v3-1\n * --prompt <your_prompt> (-p <your_prompt>):\n   * Used to pass a prompt directly to the command. This invokes VeCLI in a\n     non-interactive mode.\n * --prompt-interactive <your_prompt> (-i <your_prompt>):\n   * Starts an interactive session with the provided prompt as the initial\n     input.\n   * The prompt is processed within the interactive session, not before it.\n   * Cannot be used when piping input from stdin.\n   * Example: vecli -i \"explain this code\"\n * --sandbox (-s):\n   * Enables sandbox mode for this session.\n * --sandbox-image:\n   * Sets the sandbox image URI.\n * --debug (-d):\n   * Enables debug mode for this session, providing more verbose output.\n * --all-files (-a):\n   * If set, recursively includes all files within the current directory as\n     context for the prompt.\n * --help (or -h):\n   * Displays help information about command-line arguments.\n * --show-memory-usage:\n   * Displays the current memory usage.\n * --yolo:\n   * Enables YOLO mode, which automatically approves all tool calls.\n * --approval-mode <mode>:\n   * Sets the approval mode for tool calls. Available modes:\n     * default: Prompt for approval on each tool call (default behavior)\n     * auto_edit: Automatically approve edit tools (replace, write_file) while\n       prompting for others\n     * yolo: Automatically approve all tool calls (equivalent to --yolo)\n   * Cannot be used together with --yolo. Use --approval-mode=yolo instead of\n     --yolo for the new unified approach.\n   * Example: vecli --approval-mode auto_edit\n * --allowed-tools <tool1,tool2,...>:\n   * A comma-separated list of tool names that will bypass the confirmation\n     dialog.\n   * Example: vecli --allowed-tools \"ShellTool(git status)\"\n * --telemetry:\n   * Enables telemetry.\n * --telemetry-target:\n   * Sets the telemetry target. See telemetry for more information.\n * --telemetry-otlp-endpoint:\n   * Sets the OTLP endpoint for telemetry. See telemetry for more information.\n * --telemetry-otlp-protocol:\n   * Sets the OTLP protocol for telemetry (grpc or http). Defaults to grpc. See\n     telemetry for more information.\n * --telemetry-log-prompts:\n   * Enables logging of prompts for telemetry. See telemetry for more\n     information.\n * --checkpointing:\n   * Enables checkpointing.\n * --extensions <extension_name ...> (-e <extension_name ...>):\n   * Specifies a list of extensions to use for the session. If not provided, all\n     available extensions are used.\n   * Use the special term vecli -e none to disable all extensions.\n   * Example: vecli -e my-extension -e my-other-extension\n * --list-extensions (-l):\n   * Lists all available extensions and exits.\n * --proxy:\n   * Sets the proxy for the CLI.\n   * Example: --proxy http://localhost:7890.\n * --include-directories <dir1,dir2,...>:\n   * Includes additional directories in the workspace for multi-directory\n     support.\n   * Can be specified multiple times or as comma-separated values.\n   * 5 directories can be added at maximum.\n   * Example: --include-directories /path/to/project1,/path/to/project2 or\n     --include-directories /path/to/project1 --include-directories\n     /path/to/project2\n * --screen-reader:\n   * Enables screen reader mode, which adjusts the TUI for better compatibility\n     with screen readers.\n * --version:\n   * Displays the version of the CLI.\n\n\nContext Files (Hierarchical Instructional Context)#\n\nWhile not strictly configuration for the CLI's behavior, context files\n(defaulting to vecli.md but configurable via the context.fileName setting) are\ncrucial for configuring the instructional context (also referred to as \"memory\")\nprovided to the Volcano Engine model. This powerful feature allows you to give\nproject-specific instructions, coding style guides, or any relevant background\ninformation to the AI, making its responses more tailored and accurate to your\nneeds. The CLI includes UI elements, such as an indicator in the footer showing\nthe number of loaded context files, to keep you informed about the active\ncontext.\n\n * Purpose: These Markdown files contain instructions, guidelines, or context\n   that you want the Volcano Engine model to be aware of during your\n   interactions. The system is designed to manage this instructional context\n   hierarchically.\n\n\nExample Context File Content (e.g., VE.md)#\n\nHere's a conceptual example of what a context file at the root of a TypeScript\nproject might contain:\n\n\n\nThis example demonstrates how you can provide general project context, specific\ncoding conventions, and even notes about particular files or components. The\nmore relevant and precise your context files are, the better the AI can assist\nyou. Project-specific context files are highly encouraged to establish\nconventions and context.\n\n * Hierarchical Loading and Precedence: The CLI implements a sophisticated\n   hierarchical memory system by loading context files (e.g., VE.md) from\n   several locations. Content from files lower in this list (more specific)\n   typically overrides or supplements content from files higher up (more\n   general). The exact concatenation order and final context can be inspected\n   using the /memory show command. The typical loading order is:\n   1. Global Context File:\n      * Location: ~/.ve/<configured-context-filename> (e.g., ~/.ve/VE.md in your\n        user home directory).\n      * Scope: Provides default instructions for all your projects.\n   2. Project Root & Ancestors Context Files:\n      * Location: The CLI searches for the configured context file in the\n        current working directory and then in each parent directory up to either\n        the project root (identified by a .git folder) or your home directory.\n      * Scope: Provides context relevant to the entire project or a significant\n        portion of it.\n   3. Sub-directory Context Files (Contextual/Local):\n      * Location: The CLI also scans for the configured context file in\n        subdirectories below the current working directory (respecting common\n        ignore patterns like node_modules, .git, etc.). The breadth of this\n        search is limited to 200 directories by default, but can be configured\n        with the context.discoveryMaxDirs setting in your settings.json file.\n      * Scope: Allows for highly specific instructions relevant to a particular\n        component, module, or subsection of your project.\n * Concatenation & UI Indication: The contents of all found context files are\n   concatenated (with separators indicating their origin and path) and provided\n   as part of the system prompt to the Volcano Engine model. The CLI footer\n   displays the count of loaded context files, giving you a quick visual cue\n   about the active instructional context.\n * Importing Content: You can modularize your context files by importing other\n   Markdown files using the @path/to/file.md syntax. For more details, see the\n   Memory Import Processor documentation.\n * Commands for Memory Management:\n   * Use /memory refresh to force a re-scan and reload of all context files from\n     all configured locations. This updates the AI's instructional context.\n   * Use /memory show to display the combined instructional context currently\n     loaded, allowing you to verify the hierarchy and content being used by the\n     AI.\n   * See the Commands documentation for full details on the /memory command and\n     its sub-commands (show and refresh).\n\nBy understanding and utilizing these configuration layers and the hierarchical\nnature of context files, you can effectively manage the AI's memory and tailor\nthe VeCLI's responses to your specific needs and projects.\n\n\nSandboxing#\n\nThe VeCLI can execute potentially unsafe operations (like shell commands and\nfile modifications) within a sandboxed environment to protect your system.\n\nSandboxing is disabled by default, but you can enable it in a few ways:\n\n * Using --sandbox or -s flag.\n * Setting VECLI_SANDBOX environment variable.\n * Sandbox is enabled when using --yolo or --approval-mode=yolo by default.\n\nBy default, it uses a pre-built vecli-sandbox Docker image.\n\nFor project-specific sandboxing needs, you can create a custom Dockerfile at\n.ve/sandbox.Dockerfile in your project's root directory. This Dockerfile can be\nbased on the base sandbox image:\n\n\n\nWhen .ve/sandbox.Dockerfile exists, you can use BUILD_SANDBOX environment\nvariable when running VeCLI to automatically build the custom sandbox image:\n\n\n\n\nUsage Statistics#\n\nTo help us improve the VeCLI, we collect anonymized usage statistics. This data\nhelps us understand how the CLI is used, identify common issues, and prioritize\nnew features.\n\nWhat we collect:\n\n * Tool Calls: We log the names of the tools that are called, whether they\n   succeed or fail, and how long they take to execute. We do not collect the\n   arguments passed to the tools or any data returned by them.\n * API Requests: We log the Volcano Engine model used for each request, the\n   duration of the request, and whether it was successful. We do not collect the\n   content of the prompts or responses.\n * Session Information: We collect information about the configuration of the\n   CLI, such as the enabled tools and the approval mode.\n\nWhat we DON'T collect:\n\n * Personally Identifiable Information (PII): We do not collect any personal\n   information, such as your name, email address, or API keys.\n * Prompt and Response Content: We do not log the content of your prompts or the\n   responses from the Volcano Engine model.\n * File Content: We do not log the content of any files that are read or written\n   by the CLI.\n\nHow to opt out:\n\nYou can opt out of usage statistics collection at any time by setting the\nusageStatisticsEnabled property to false under the privacy category in your\nsettings.json file:\n\n","routePath":"/en/cli/configuration","lang":"","toc":[{"text":"Configuration layers","id":"configuration-layers","depth":2,"charIndex":596},{"text":"Settings files","id":"settings-files","depth":2,"charIndex":1287},{"text":"The `.ve` directory in your project","id":"the-ve-directory-in-your-project","depth":3,"charIndex":-1},{"text":"Available settings in `settings.json`","id":"available-settings-in-settingsjson","depth":3,"charIndex":-1},{"text":"`general`","id":"general","depth":4,"charIndex":-1},{"text":"`ui`","id":"ui","depth":4,"charIndex":-1},{"text":"`ide`","id":"ide","depth":4,"charIndex":-1},{"text":"`privacy`","id":"privacy","depth":4,"charIndex":-1},{"text":"`model`","id":"model","depth":4,"charIndex":-1},{"text":"`context`","id":"context","depth":4,"charIndex":-1},{"text":"`tools`","id":"tools","depth":4,"charIndex":-1},{"text":"`mcp`","id":"mcp","depth":4,"charIndex":-1},{"text":"`security`","id":"security","depth":4,"charIndex":-1},{"text":"`advanced`","id":"advanced","depth":4,"charIndex":-1},{"text":"`mcpServers`","id":"mcpservers","depth":4,"charIndex":-1},{"text":"`telemetry`","id":"telemetry","depth":4,"charIndex":-1},{"text":"Example `settings.json`","id":"example-settingsjson","depth":3,"charIndex":-1},{"text":"Shell History","id":"shell-history","depth":2,"charIndex":14915},{"text":"Environment Variables & `.env` Files","id":"environment-variables--env-files","depth":2,"charIndex":-1},{"text":"Command-Line Arguments","id":"command-line-arguments","depth":2,"charIndex":19628},{"text":"Context Files (Hierarchical Instructional Context)","id":"context-files-hierarchical-instructional-context","depth":2,"charIndex":23239},{"text":"Example Context File Content (e.g., `VE.md`)","id":"example-context-file-content-eg-vemd","depth":3,"charIndex":-1},{"text":"Sandboxing","id":"sandboxing","depth":2,"charIndex":27512},{"text":"Usage Statistics","id":"usage-statistics","depth":2,"charIndex":28315}],"domain":"","frontmatter":{},"version":""},{"id":22,"title":"VeCLI for the Enterprise","content":"#\n\nThis document outlines configuration patterns and best practices for deploying\nand managing VeCLI in an enterprise environment. By leveraging system-level\nsettings, administrators can enforce security policies, manage tool access, and\nensure a consistent experience for all users.\n\n> A Note on Security: The patterns described in this document are intended to\n> help administrators create a more controlled and secure environment for using\n> VeCLI. However, they should not be considered a foolproof security boundary. A\n> determined user with sufficient privileges on their local machine may still be\n> able to circumvent these configurations. These measures are designed to\n> prevent accidental misuse and enforce corporate policy in a managed\n> environment, not to defend against a malicious actor with local administrative\n> rights.\n\n\nCentralized Configuration: The System Settings File#\n\nThe most powerful tools for enterprise administration are the system-wide\nsettings files. These files allow you to define a baseline configuration\n(system-defaults.json) and a set of overrides (settings.json) that apply to all\nusers on a machine. For a complete overview of configuration options, see the\nConfiguration documentation.\n\nSettings are merged from four files. The precedence order for single-value\nsettings (like theme) is:\n\n 1. System Defaults (system-defaults.json)\n 2. User Settings (~/.ve/settings.json)\n 3. Workspace Settings (<project>/.ve/settings.json)\n 4. System Overrides (settings.json)\n\nThis means the System Overrides file has the final say. For settings that are\narrays (includeDirectories) or objects (mcpServers), the values are merged.\n\nExample of Merging and Precedence:\n\nHere is how settings from different levels are combined.\n\n * System Defaults system-defaults.json:\n   \n   \n\n * User settings.json (~/.ve/settings.json):\n   \n   \n\n * Workspace settings.json (<project>/.ve/settings.json):\n   \n   \n\n * System Overrides settings.json:\n   \n   \n\nThis results in the following merged configuration:\n\n * Final Merged Configuration:\n   \n   \n\nWhy:\n\n * theme: The value from the system overrides (system-enforced-theme) is used,\n   as it has the highest precedence.\n\n * mcpServers: The objects are merged. The corp-server definition from the\n   system overrides takes precedence over the user's definition. The unique\n   user-tool and project-tool are included.\n\n * includeDirectories: The arrays are concatenated in the order of System\n   Defaults, User, Workspace, and then System Overrides.\n\n * Location:\n   \n   * Linux: /etc/gemini-cli/settings.json\n   * Windows: C:\\ProgramData\\gemini-cli\\settings.json\n   * macOS: /Library/Application Support/GeminiCli/settings.json\n   * The path can be overridden using the GEMINI_CLI_SYSTEM_SETTINGS_PATH\n     environment variable.\n\n * Control: This file should be managed by system administrators and protected\n   with appropriate file permissions to prevent unauthorized modification by\n   users.\n\nBy using the system settings file, you can enforce the security and\nconfiguration patterns described below.\n\n\nRestricting Tool Access#\n\nYou can significantly enhance security by controlling which tools the Gemini\nmodel can use. This is achieved through the tools.core and tools.exclude\nsettings. For a list of available tools, see the Tools documentation.\n\n\nAllowlisting with coreTools#\n\nThe most secure approach is to explicitly add the tools and commands that users\nare permitted to execute to an allowlist. This prevents the use of any tool not\non the approved list.\n\nExample: Allow only safe, read-only file operations and listing files.\n\n\n\n\nBlocklisting with excludeTools#\n\nAlternatively, you can add specific tools that are considered dangerous in your\nenvironment to a blocklist.\n\nExample: Prevent the use of the shell tool for removing files.\n\n\n\nSecurity Note: Blocklisting with excludeTools is less secure than allowlisting\nwith coreTools, as it relies on blocking known-bad commands, and clever users\nmay find ways to bypass simple string-based blocks. Allowlisting is the\nrecommended approach.\n\n\nManaging Custom Tools (MCP Servers)#\n\nIf your organization uses custom tools via Model-Context Protocol (MCP) servers,\nit is crucial to understand how server configurations are managed to apply\nsecurity policies effectively.\n\n\nHow MCP Server Configurations are Merged#\n\nVeCLI loads settings.json files from three levels: System, Workspace, and User.\nWhen it comes to the mcpServers object, these configurations are merged:\n\n 1. Merging: The lists of servers from all three levels are combined into a\n    single list.\n 2. Precedence: If a server with the same name is defined at multiple levels\n    (e.g., a server named corp-api exists in both system and user settings), the\n    definition from the highest-precedence level is used. The order of\n    precedence is: System > Workspace > User.\n\nThis means a user cannot override the definition of a server that is already\ndefined in the system-level settings. However, they can add new servers with\nunique names.\n\n\nEnforcing a Catalog of Tools#\n\nThe security of your MCP tool ecosystem depends on a combination of defining the\ncanonical servers and adding their names to an allowlist.\n\n\nRestricting Tools Within an MCP Server#\n\nFor even greater security, especially when dealing with third-party MCP servers,\nyou can restrict which specific tools from a server are exposed to the model.\nThis is done using the includeTools and excludeTools properties within a\nserver's definition. This allows you to use a subset of tools from a server\nwithout allowing potentially dangerous ones.\n\nFollowing the principle of least privilege, it is highly recommended to use\nincludeTools to create an allowlist of only the necessary tools.\n\nExample: Only allow the code-search and get-ticket-details tools from a\nthird-party MCP server, even if the server offers other tools like\ndelete-ticket.\n\n\n\nMore Secure Pattern: Define and Add to Allowlist in System Settings#\n\nTo create a secure, centrally-managed catalog of tools, the system administrator\nmust do both of the following in the system-level settings.json file:\n\n 1. Define the full configuration for every approved server in the mcpServers\n    object. This ensures that even if a user defines a server with the same\n    name, the secure system-level definition will take precedence.\n 2. Add the names of those servers to an allowlist using the mcp.allowed\n    setting. This is a critical security step that prevents users from running\n    any servers that are not on this list. If this setting is omitted, the CLI\n    will merge and allow any server defined by the user.\n\nExample System settings.json:\n\n 1. Add the names of all approved servers to an allowlist. This will prevent\n    users from adding their own servers.\n\n 2. Provide the canonical definition for each server on the allowlist.\n\n\n\nThis pattern is more secure because it uses both definition and an allowlist.\nAny server a user defines will either be overridden by the system definition (if\nit has the same name) or blocked because its name is not in the mcp.allowed\nlist.\n\n\nLess Secure Pattern: Omitting the Allowlist#\n\nIf the administrator defines the mcpServers object but fails to also specify the\nmcp.allowed allowlist, users may add their own servers.\n\nExample System settings.json:\n\nThis configuration defines servers but does not enforce the allowlist. The\nadministrator has NOT included the \"mcp.allowed\" setting.\n\n\n\nIn this scenario, a user can add their own server in their local settings.json.\nBecause there is no mcp.allowed list to filter the merged results, the user's\nserver will be added to the list of available tools and allowed to run.\n\n\nEnforcing Sandboxing for Security#\n\nTo mitigate the risk of potentially harmful operations, you can enforce the use\nof sandboxing for all tool execution. The sandbox isolates tool execution in a\ncontainerized environment.\n\nExample: Force all tool execution to happen within a Docker sandbox.\n\n\n\nYou can also specify a custom, hardened Docker image for the sandbox using the\n--sandbox-image command-line argument or by building a custom sandbox.Dockerfile\nas described in the Sandboxing documentation.\n\n\nControlling Network Access via Proxy#\n\nIn corporate environments with strict network policies, you can configure VeCLI\nto route all outbound traffic through a corporate proxy. This can be set via an\nenvironment variable, but it can also be enforced for custom tools via the\nmcpServers configuration.\n\nExample (for an MCP Server):\n\n\n\n\nTelemetry and Auditing#\n\nFor auditing and monitoring purposes, you can configure VeCLI to send telemetry\ndata to a central location. This allows you to track tool usage and other\nevents. For more information, see the telemetry documentation.\n\nExample: Enable telemetry and send it to a local OTLP collector. If otlpEndpoint\nis not specified, it defaults to http://localhost:4317.\n\n\n\nNote: Ensure that logPrompts is set to false in an enterprise setting to avoid\ncollecting potentially sensitive information from user prompts.\n\n\nAuthentication#\n\nYou can enforce a specific authentication method for all users by setting the\nenforcedAuthType in the system-level settings.json file. This prevents users\nfrom choosing a different authentication method. See the Authentication docs for\nmore details.\n\nExample: Enforce the use of Google login for all users.\n\n\n\nIf a user has a different authentication method configured, they will be\nprompted to switch to the enforced method. In non-interactive mode, the CLI will\nexit with an error if the configured authentication method does not match the\nenforced one.\n\n\nPutting It All Together: Example System settings.json#\n\nHere is an example of a system settings.json file that combines several of the\npatterns discussed above to create a secure, controlled environment for VeCLI.\n\n\n\nThis configuration:\n\n * Forces all tool execution into a Docker sandbox.\n * Strictly uses an allowlist for a small set of safe shell commands and file\n   tools.\n * Defines and allows a single corporate MCP server for custom tools.\n * Enables telemetry for auditing, without logging prompt content.\n * Redirects the /bug command to an internal ticketing system.\n * Disables general usage statistics collection.","routePath":"/en/cli/enterprise","lang":"","toc":[{"text":"Centralized Configuration: The System Settings File","id":"centralized-configuration-the-system-settings-file","depth":2,"charIndex":841},{"text":"Restricting Tool Access","id":"restricting-tool-access","depth":2,"charIndex":3071},{"text":"Allowlisting with `coreTools`","id":"allowlisting-with-coretools","depth":3,"charIndex":-1},{"text":"Blocklisting with `excludeTools`","id":"blocklisting-with-excludetools","depth":3,"charIndex":-1},{"text":"Managing Custom Tools (MCP Servers)","id":"managing-custom-tools-mcp-servers","depth":2,"charIndex":4068},{"text":"How MCP Server Configurations are Merged","id":"how-mcp-server-configurations-are-merged","depth":3,"charIndex":4295},{"text":"Enforcing a Catalog of Tools","id":"enforcing-a-catalog-of-tools","depth":3,"charIndex":5031},{"text":"Restricting Tools Within an MCP Server","id":"restricting-tools-within-an-mcp-server","depth":3,"charIndex":5203},{"text":"More Secure Pattern: Define and Add to Allowlist in System Settings","id":"more-secure-pattern-define-and-add-to-allowlist-in-system-settings","depth":4,"charIndex":5897},{"text":"Less Secure Pattern: Omitting the Allowlist","id":"less-secure-pattern-omitting-the-allowlist","depth":3,"charIndex":7096},{"text":"Enforcing Sandboxing for Security","id":"enforcing-sandboxing-for-security","depth":2,"charIndex":7679},{"text":"Controlling Network Access via Proxy","id":"controlling-network-access-via-proxy","depth":2,"charIndex":8182},{"text":"Telemetry and Auditing","id":"telemetry-and-auditing","depth":2,"charIndex":8516},{"text":"Authentication","id":"authentication","depth":2,"charIndex":9044},{"text":"Putting It All Together: Example System `settings.json`","id":"putting-it-all-together-example-system-settingsjson","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":23,"title":"VeCLI","content":"#\n\nWithin VeCLI, packages/cli is the frontend for users to send and receive prompts\nwith the Gemini AI model and its associated tools. For a general overview of\nVeCLI, see the main documentation page.\n\n\nNavigating this section#\n\n * Authentication: A guide to setting up authentication with Google's AI\n   services.\n * Commands: A reference for VeCLI commands (e.g., /help, /tools, /theme).\n * Configuration: A guide to tailoring VeCLI behavior using configuration files.\n * Enterprise: A guide to enterprise configuration.\n * Token Caching: Optimize API costs through token caching.\n * Themes: A guide to customizing the CLI's appearance with different themes.\n * Tutorials: A tutorial showing how to use VeCLI to automate a development\n   task.\n\n\nNon-interactive mode#\n\nVeCLI can be run in a non-interactive mode, which is useful for scripting and\nautomation. In this mode, you pipe input to the CLI, it executes the command,\nand then it exits.\n\nThe following example pipes a command to VeCLI from your terminal:\n\n\n\nVeCLI executes the command and prints the output to your terminal. Note that you\ncan achieve the same behavior by using the --prompt or -p flag. For example:\n\n","routePath":"/en/cli/","lang":"","toc":[{"text":"Navigating this section","id":"navigating-this-section","depth":2,"charIndex":202},{"text":"Non-interactive mode","id":"non-interactive-mode","depth":2,"charIndex":747}],"domain":"","frontmatter":{},"version":""},{"id":24,"title":"Themes","content":"#\n\nVeCLI supports a variety of themes to customize its color scheme and appearance.\nYou can change the theme to suit your preferences via the /theme command or\n\"theme\": configuration setting.\n\n\nAvailable Themes#\n\nVeCLI comes with a selection of pre-defined themes, which you can list using the\n/theme command within VeCLI:\n\n * Dark Themes:\n   * ANSI\n   * Atom One\n   * Ayu\n   * Default\n   * Dracula\n   * GitHub\n * Light Themes:\n   * ANSI Light\n   * Ayu Light\n   * Default Light\n   * GitHub Light\n   * Google Code\n   * Xcode\n\n\nChanging Themes#\n\n 1. Enter /theme into VeCLI.\n 2. A dialog or selection prompt appears, listing the available themes.\n 3. Using the arrow keys, select a theme. Some interfaces might offer a live\n    preview or highlight as you select.\n 4. Confirm your selection to apply the theme.\n\nNote: If a theme is defined in your settings.json file (either by name or by a\nfile path), you must remove the \"theme\" setting from the file before you can\nchange the theme using the /theme command.\n\n\nTheme Persistence#\n\nSelected themes are saved in VeCLI's configuration so your preference is\nremembered across sessions.\n\n--------------------------------------------------------------------------------\n\n\nCustom Color Themes#\n\nVeCLI allows you to create your own custom color themes by specifying them in\nyour settings.json file. This gives you full control over the color palette used\nin the CLI.\n\n\nHow to Define a Custom Theme#\n\nAdd a customThemes block to your user, project, or system settings.json file.\nEach custom theme is defined as an object with a unique name and a set of color\nkeys. For example:\n\n\n\nColor keys:\n\n * Background\n * Foreground\n * LightBlue\n * AccentBlue\n * AccentPurple\n * AccentCyan\n * AccentGreen\n * AccentYellow\n * AccentRed\n * Comment\n * Gray\n * DiffAdded (optional, for added lines in diffs)\n * DiffRemoved (optional, for removed lines in diffs)\n * DiffModified (optional, for modified lines in diffs)\n\nRequired Properties:\n\n * name (must match the key in the customThemes object and be a string)\n * type (must be the string \"custom\")\n * Background\n * Foreground\n * LightBlue\n * AccentBlue\n * AccentPurple\n * AccentCyan\n * AccentGreen\n * AccentYellow\n * AccentRed\n * Comment\n * Gray\n\nYou can use either hex codes (e.g., #FF0000) or standard CSS color names (e.g.,\ncoral, teal, blue) for any color value. See CSS color names for a full list of\nsupported names.\n\nYou can define multiple custom themes by adding more entries to the customThemes\nobject.\n\n\nLoading Themes from a File#\n\nIn addition to defining custom themes in settings.json, you can also load a\ntheme directly from a JSON file by specifying the file path in your\nsettings.json. This is useful for sharing themes or keeping them separate from\nyour main configuration.\n\nTo load a theme from a file, set the theme property in your settings.json to the\npath of your theme file:\n\n\n\nThe theme file must be a valid JSON file that follows the same structure as a\ncustom theme defined in settings.json.\n\nExample my-theme.json:\n\n\n\nSecurity Note: For your safety, VeCLI will only load theme files that are\nlocated within your home directory. If you attempt to load a theme from outside\nyour home directory, a warning will be displayed and the theme will not be\nloaded. This is to prevent loading potentially malicious theme files from\nuntrusted sources.\n\n\nExample Custom Theme#\n\n\nUsing Your Custom Theme#\n\n * Select your custom theme using the /theme command in VeCLI. Your custom theme\n   will appear in the theme selection dialog.\n * Or, set it as the default by adding \"theme\": \"MyCustomTheme\" to the ui object\n   in your settings.json.\n * Custom themes can be set at the user, project, or system level, and follow\n   the same configuration precedence as other settings.\n\n--------------------------------------------------------------------------------\n\n\nDark Themes#\n\n\nANSI#\n\n\nAtom OneDark#\n\n\nAyu#\n\n\nDefault#\n\n\nDracula#\n\n\nGitHub#\n\n\nLight Themes#\n\n\nANSI Light#\n\n\nAyu Light#\n\n\nDefault Light#\n\n\nGitHub Light#\n\n\nGoogle Code#\n\n\nXcode#","routePath":"/en/cli/themes","lang":"","toc":[{"text":"Available Themes","id":"available-themes","depth":2,"charIndex":193},{"text":"Changing Themes","id":"changing-themes","depth":3,"charIndex":525},{"text":"Theme Persistence","id":"theme-persistence","depth":3,"charIndex":1010},{"text":"Custom Color Themes","id":"custom-color-themes","depth":2,"charIndex":1215},{"text":"How to Define a Custom Theme","id":"how-to-define-a-custom-theme","depth":3,"charIndex":1410},{"text":"Loading Themes from a File","id":"loading-themes-from-a-file","depth":3,"charIndex":2492},{"text":"Example Custom Theme","id":"example-custom-theme","depth":3,"charIndex":3347},{"text":"Using Your Custom Theme","id":"using-your-custom-theme","depth":3,"charIndex":3371},{"text":"Dark Themes","id":"dark-themes","depth":2,"charIndex":3849},{"text":"ANSI","id":"ansi","depth":3,"charIndex":3864},{"text":"Atom OneDark","id":"atom-onedark","depth":3,"charIndex":3872},{"text":"Ayu","id":"ayu","depth":3,"charIndex":3888},{"text":"Default","id":"default","depth":3,"charIndex":3895},{"text":"Dracula","id":"dracula","depth":3,"charIndex":3906},{"text":"GitHub","id":"github","depth":3,"charIndex":3917},{"text":"Light Themes","id":"light-themes","depth":2,"charIndex":3927},{"text":"ANSI Light","id":"ansi-light","depth":3,"charIndex":3943},{"text":"Ayu Light","id":"ayu-light","depth":3,"charIndex":3957},{"text":"Default Light","id":"default-light","depth":3,"charIndex":3970},{"text":"GitHub Light","id":"github-light","depth":3,"charIndex":3987},{"text":"Google Code","id":"google-code","depth":3,"charIndex":4003},{"text":"Xcode","id":"xcode","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":25,"title":"Token Caching and Cost Optimization","content":"#\n\nVeCLI automatically optimizes API costs through token caching when using API key\nauthentication (Gemini API key or Vertex AI). This feature reuses previous\nsystem instructions and context to reduce the number of tokens processed in\nsubsequent requests.\n\nToken caching is available for:\n\n * API key users (Gemini API key)\n * Vertex AI users (with project and location setup)\n\nToken caching is not available for:\n\n * OAuth users (Google Personal/Enterprise accounts) - the Code Assist API does\n   not support cached content creation at this time\n\nYou can view your token usage and cached token savings using the /stats command.\nWhen cached tokens are available, they will be displayed in the stats output.","routePath":"/en/cli/token-caching","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":26,"title":"Tutorials","content":"#\n\nThis page contains tutorials for interacting with VeCLI.\n\n\nSetting up a Model Context Protocol (MCP) server#\n\nCAUTION\n\nBefore using a third-party MCP server, ensure you trust its source and\nunderstand the tools it provides. Your use of third-party servers is at your own\nrisk.\n\nThis tutorial demonstrates how to set up a MCP server, using the GitHub MCP\nserver as an example. The GitHub MCP server provides tools for interacting with\nGitHub repositories, such as creating issues and commenting on pull requests.\n\n\nPrerequisites#\n\nBefore you begin, ensure you have the following installed and configured:\n\n * Docker: Install and run Docker.\n * GitHub Personal Access Token (PAT): Create a new classic or fine-grained PAT\n   with the necessary scopes.\n\n\nGuide#\n\nConfigure the MCP server in settings.json#\n\nIn your project's root directory, create or open the .ve/settings.json file.\nWithin the file, add the mcpServers configuration block, which provides\ninstructions for how to launch the GitHub MCP server.\n\n\n\nSet your GitHub token#\n\nCAUTION\n\nUsing a broadly scoped personal access token that has access to personal and\nprivate repositories can lead to information from the private repository being\nleaked into the public repository. We recommend using a fine-grained access\ntoken that doesn't share access to both public and private repositories.\n\nUse an environment variable to store your GitHub PAT:\n\n\n\nVeCLI uses this value in the mcpServers configuration that you defined in the\nsettings.json file.\n\nLaunch VeCLI and verify the connection#\n\nWhen you launch VeCLI, it automatically reads your configuration and launches\nthe GitHub MCP server in the background. You can then use natural language\nprompts to ask VeCLI to perform GitHub actions. For example:\n\n","routePath":"/en/cli/tutorials","lang":"","toc":[{"text":"Setting up a Model Context Protocol (MCP) server","id":"setting-up-a-model-context-protocol-mcp-server","depth":2,"charIndex":61},{"text":"Prerequisites","id":"prerequisites","depth":3,"charIndex":516},{"text":"Guide","id":"guide","depth":3,"charIndex":754},{"text":"Configure the MCP server in `settings.json`","id":"configure-the-mcp-server-in-settingsjson","depth":4,"charIndex":-1},{"text":"Set your GitHub token","id":"set-your-github-token","depth":4,"charIndex":1012},{"text":"Launch VeCLI and verify the connection","id":"launch-vecli-and-verify-the-connection","depth":4,"charIndex":1507}],"domain":"","frontmatter":{},"version":""},{"id":27,"title":"VeCLI Core","content":"#\n\nVeCLI's core package (packages/core) is the backend portion of VeCLI, handling\ncommunication with the Volcano Engine API, managing tools, and processing\nrequests sent from packages/cli. For a general overview of VeCLI, see the main\ndocumentation page.\n\n\nNavigating this section#\n\n * Core tools API: Information on how tools are defined, registered, and used by\n   the core.\n * Memory Import Processor: Documentation for the modular VE.md import feature\n   using @file.md syntax.\n\n\nRole of the core#\n\nWhile the packages/cli portion of VeCLI provides the user interface,\npackages/core is responsible for:\n\n * Volcano Engine API interaction: Securely communicating with the Volcano\n   Engine API, sending user prompts, and receiving model responses.\n * Prompt engineering: Constructing effective prompts for the Volcano Engine\n   model, potentially incorporating conversation history, tool definitions, and\n   instructional context from VE.md files.\n * Tool management & orchestration:\n   * Registering available tools (e.g., file system tools, shell command\n     execution).\n   * Interpreting tool use requests from the Volcano Engine model.\n   * Executing the requested tools with the provided arguments.\n   * Returning tool execution results to the Volcano Engine model for further\n     processing.\n * Session and state management: Keeping track of the conversation state,\n   including history and any relevant context required for coherent\n   interactions.\n * Configuration: Managing core-specific configurations, such as API key access,\n   model selection, and tool settings.\n\n\nSecurity considerations#\n\nThe core plays a vital role in security:\n\n * API key management: It handles the VOLCENGINE_API_KEY and ensures it's used\n   securely when communicating with the Volcano Engine API.\n * Tool execution: When tools interact with the local system (e.g.,\n   run_shell_command), the core (and its underlying tool implementations) must\n   do so with appropriate caution, often involving sandboxing mechanisms to\n   prevent unintended modifications.\n\n\nChat history compression#\n\nTo ensure that long conversations don't exceed the token limits of the Volcano\nEngine model, the core includes a chat history compression feature.\n\nWhen a conversation approaches the token limit for the configured model, the\ncore automatically compresses the conversation history before sending it to the\nmodel. This compression is designed to be lossless in terms of the information\nconveyed, but it reduces the overall number of tokens used.\n\nYou can find the token limits for each model in the Volcano Engine AI\ndocumentation.\n\n\nModel fallback#\n\nVeCLI includes a model fallback mechanism to ensure that you can continue to use\nthe CLI even if the default \"pro\" model is rate-limited.\n\n\nFile discovery service#\n\nThe file discovery service is responsible for finding files in the project that\nare relevant to the current context. It is used by the @ command and other tools\nthat need to access files.\n\n\nMemory discovery service#\n\nThe memory discovery service is responsible for finding and loading the VE.md\nfiles that provide context to the model. It searches for these files in a\nhierarchical manner, starting from the current working directory and moving up\nto the project root and the user's home directory. It also searches in\nsubdirectories.\n\nThis allows you to have global, project-level, and component-level context\nfiles, which are all combined to provide the model with the most relevant\ninformation.\n\nYou can use the /memory command to show, add, and refresh the content of loaded\nVE.md files.","routePath":"/en/core/","lang":"","toc":[{"text":"Navigating this section","id":"navigating-this-section","depth":2,"charIndex":256},{"text":"Role of the core","id":"role-of-the-core","depth":2,"charIndex":483},{"text":"Security considerations","id":"security-considerations","depth":2,"charIndex":1582},{"text":"Chat history compression","id":"chat-history-compression","depth":2,"charIndex":2051},{"text":"Model fallback","id":"model-fallback","depth":2,"charIndex":2610},{"text":"File discovery service","id":"file-discovery-service","depth":2,"charIndex":2767},{"text":"Memory discovery service","id":"memory-discovery-service","depth":2,"charIndex":2982}],"domain":"","frontmatter":{},"version":""},{"id":28,"title":"Memory Import Processor","content":"#\n\nThe Memory Import Processor is a feature that allows you to modularize your\nVE.md files by importing content from other files using the @file.md syntax.\n\n\nOverview#\n\nThis feature enables you to break down large VE.md files into smaller, more\nmanageable components that can be reused across different contexts. The import\nprocessor supports both relative and absolute paths, with built-in safety\nfeatures to prevent circular imports and ensure file access security.\n\n\nSyntax#\n\nUse the @ symbol followed by the path to the file you want to import:\n\n\n\n\nSupported Path Formats#\n\n\nRelative Paths#\n\n * @./file.md - Import from the same directory\n * @../file.md - Import from parent directory\n * @./components/file.md - Import from subdirectory\n\n\nAbsolute Paths#\n\n * @/absolute/path/to/file.md - Import using absolute path\n\n\nExamples#\n\n\nBasic Import#\n\n\n\n\nNested Imports#\n\nThe imported files can themselves contain imports, creating a nested structure:\n\n\n\n\n\n\nSafety Features#\n\n\nCircular Import Detection#\n\nThe processor automatically detects and prevents circular imports:\n\n\n\n\nFile Access Security#\n\nThe validateImportPath function ensures that imports are only allowed from\nspecified directories, preventing access to sensitive files outside the allowed\nscope.\n\n\nMaximum Import Depth#\n\nTo prevent infinite recursion, there's a configurable maximum import depth\n(default: 5 levels).\n\n\nError Handling#\n\n\nMissing Files#\n\nIf a referenced file doesn't exist, the import will fail gracefully with an\nerror comment in the output.\n\n\nFile Access Errors#\n\nPermission issues or other file system errors are handled gracefully with\nappropriate error messages.\n\n\nCode Region Detection#\n\nThe import processor uses the marked library to detect code blocks and inline\ncode spans, ensuring that @ imports inside these regions are properly ignored.\nThis provides robust handling of nested code blocks and complex Markdown\nstructures.\n\n\nImport Tree Structure#\n\nThe processor returns an import tree that shows the hierarchy of imported files,\nsimilar to Claude's /memory feature. This helps users debug problems with their\nVE.md files by showing which files were read and their import relationships.\n\nExample tree structure:\n\n\n\nThe tree preserves the order that files were imported and shows the complete\nimport chain for debugging purposes.\n\n\nComparison to Claude Code's /memory (claude.md) Approach#\n\nClaude Code's /memory feature (as seen in claude.md) produces a flat, linear\ndocument by concatenating all included files, always marking file boundaries\nwith clear comments and path names. It does not explicitly present the import\nhierarchy, but the LLM receives all file contents and paths, which is sufficient\nfor reconstructing the hierarchy if needed.\n\nNote: The import tree is mainly for clarity during development and has limited\nrelevance to LLM consumption.\n\n\nAPI Reference#\n\n\nprocessImports(content, basePath, debugMode?, importState?)#\n\nProcesses import statements in VE.md content.\n\nParameters:\n\n * content (string): The content to process for imports\n * basePath (string): The directory path where the current file is located\n * debugMode (boolean, optional): Whether to enable debug logging (default:\n   false)\n * importState (ImportState, optional): State tracking for circular import\n   prevention\n\nReturns: Promise - Object containing processed content and import tree\n\n\nProcessImportsResult#\n\n\n\n\nMemoryFile#\n\n\n\n\nvalidateImportPath(importPath, basePath, allowedDirectories)#\n\nValidates import paths to ensure they are safe and within allowed directories.\n\nParameters:\n\n * importPath (string): The import path to validate\n * basePath (string): The base directory for resolving relative paths\n * allowedDirectories (string[]): Array of allowed directory paths\n\nReturns: boolean - Whether the import path is valid\n\n\nfindProjectRoot(startDir)#\n\nFinds the project root by searching for a .git directory upwards from the given\nstart directory. Implemented as an async function using non-blocking file system\nAPIs to avoid blocking the Node.js event loop.\n\nParameters:\n\n * startDir (string): The directory to start searching from\n\nReturns: Promise - The project root directory (or the start directory if no .git\nis found)\n\n\nBest Practices#\n\n 1. Use descriptive file names for imported components\n 2. Keep imports shallow - avoid deeply nested import chains\n 3. Document your structure - maintain a clear hierarchy of imported files\n 4. Test your imports - ensure all referenced files exist and are accessible\n 5. Use relative paths when possible for better portability\n\n\nTroubleshooting#\n\n\nCommon Issues#\n\n 1. Import not working: Check that the file exists and the path is correct\n 2. Circular import warnings: Review your import structure for circular\n    references\n 3. Permission errors: Ensure the files are readable and within allowed\n    directories\n 4. Path resolution issues: Use absolute paths if relative paths aren't\n    resolving correctly\n\n\nDebug Mode#\n\nEnable debug mode to see detailed logging of the import process:\n\n","routePath":"/en/core/memport","lang":"","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":157},{"text":"Syntax","id":"syntax","depth":2,"charIndex":469},{"text":"Supported Path Formats","id":"supported-path-formats","depth":2,"charIndex":552},{"text":"Relative Paths","id":"relative-paths","depth":3,"charIndex":578},{"text":"Absolute Paths","id":"absolute-paths","depth":3,"charIndex":742},{"text":"Examples","id":"examples","depth":2,"charIndex":820},{"text":"Basic Import","id":"basic-import","depth":3,"charIndex":832},{"text":"Nested Imports","id":"nested-imports","depth":3,"charIndex":850},{"text":"Safety Features","id":"safety-features","depth":2,"charIndex":953},{"text":"Circular Import Detection","id":"circular-import-detection","depth":3,"charIndex":972},{"text":"File Access Security","id":"file-access-security","depth":3,"charIndex":1071},{"text":"Maximum Import Depth","id":"maximum-import-depth","depth":3,"charIndex":1258},{"text":"Error Handling","id":"error-handling","depth":2,"charIndex":1379},{"text":"Missing Files","id":"missing-files","depth":3,"charIndex":1397},{"text":"File Access Errors","id":"file-access-errors","depth":3,"charIndex":1520},{"text":"Code Region Detection","id":"code-region-detection","depth":2,"charIndex":1645},{"text":"Import Tree Structure","id":"import-tree-structure","depth":2,"charIndex":1913},{"text":"Comparison to Claude Code's `/memory` (`claude.md`) Approach","id":"comparison-to-claude-codes-memory-claudemd-approach","depth":2,"charIndex":-1},{"text":"API Reference","id":"api-reference","depth":2,"charIndex":2847},{"text":"`processImports(content, basePath, debugMode?, importState?)`","id":"processimportscontent-basepath-debugmode-importstate","depth":3,"charIndex":-1},{"text":"`ProcessImportsResult`","id":"processimportsresult","depth":3,"charIndex":-1},{"text":"`MemoryFile`","id":"memoryfile","depth":3,"charIndex":-1},{"text":"`validateImportPath(importPath, basePath, allowedDirectories)`","id":"validateimportpathimportpath-basepath-alloweddirectories","depth":3,"charIndex":-1},{"text":"`findProjectRoot(startDir)`","id":"findprojectrootstartdir","depth":3,"charIndex":-1},{"text":"Best Practices","id":"best-practices","depth":2,"charIndex":4212},{"text":"Troubleshooting","id":"troubleshooting","depth":2,"charIndex":4559},{"text":"Common Issues","id":"common-issues","depth":3,"charIndex":4578},{"text":"Debug Mode","id":"debug-mode","depth":3,"charIndex":4942}],"domain":"","frontmatter":{},"version":""},{"id":29,"title":"VeCLI Core: Tools API","content":"#\n\nThe VeCLI core (packages/core) features a robust system for defining,\nregistering, and executing tools. These tools extend the capabilities of the\nVolcano Engine model, allowing it to interact with the local environment, fetch\nweb content, and perform various actions beyond simple text generation.\n\n\nCore Concepts#\n\n * Tool (tools.ts): An interface and base class (BaseTool) that defines the\n   contract for all tools. Each tool must have:\n   \n   * name: A unique internal name (used in API calls to Volcano Engine).\n   * displayName: A user-friendly name.\n   * description: A clear explanation of what the tool does, which is provided\n     to the Volcano Engine model.\n   * parameterSchema: A JSON schema defining the parameters that the tool\n     accepts. This is crucial for the Volcano Engine model to understand how to\n     call the tool correctly.\n   * validateToolParams(): A method to validate incoming parameters.\n   * getDescription(): A method to provide a human-readable description of what\n     the tool will do with specific parameters before execution.\n   * shouldConfirmExecute(): A method to determine if user confirmation is\n     required before execution (e.g., for potentially destructive operations).\n   * execute(): The core method that performs the tool's action and returns a\n     ToolResult.\n\n * ToolResult (tools.ts): An interface defining the structure of a tool's\n   execution outcome:\n   \n   * llmContent: The factual content to be included in the history sent back to\n     the LLM for context. This can be a simple string or a PartListUnion (an\n     array of Part objects and strings) for rich content.\n   * returnDisplay: A user-friendly string (often Markdown) or a special object\n     (like FileDiff) for display in the CLI.\n\n * Returning Rich Content: Tools are not limited to returning simple text. The\n   llmContent can be a PartListUnion, which is an array that can contain a mix\n   of Part objects (for images, audio, etc.) and strings. This allows a single\n   tool execution to return multiple pieces of rich content.\n\n * Tool Registry (tool-registry.ts): A class (ToolRegistry) responsible for:\n   \n   * Registering Tools: Holding a collection of all available built-in tools\n     (e.g., ReadFileTool, ShellTool).\n   * Discovering Tools: It can also discover tools dynamically:\n     * Command-based Discovery: If tools.discoveryCommand is configured in\n       settings, this command is executed. It's expected to output JSON\n       describing custom tools, which are then registered as DiscoveredTool\n       instances.\n     * MCP-based Discovery: If mcp.serverCommand is configured, the registry can\n       connect to a Model Context Protocol (MCP) server to list and register\n       tools (DiscoveredMCPTool).\n   * Providing Schemas: Exposing the FunctionDeclaration schemas of all\n     registered tools to the Volcano Engine model, so it knows what tools are\n     available and how to use them.\n   * Retrieving Tools: Allowing the core to get a specific tool by name for\n     execution.\n\n\nBuilt-in Tools#\n\nThe core comes with a suite of pre-defined tools, typically found in\npackages/core/src/tools/. These include:\n\n * File System Tools:\n   * LSTool (ls.ts): Lists directory contents.\n   * ReadFileTool (read-file.ts): Reads the content of a single file. It takes\n     an absolute_path parameter, which must be an absolute path.\n   * WriteFileTool (write-file.ts): Writes content to a file.\n   * GrepTool (grep.ts): Searches for patterns in files.\n   * GlobTool (glob.ts): Finds files matching glob patterns.\n   * EditTool (edit.ts): Performs in-place modifications to files (often\n     requiring confirmation).\n   * ReadManyFilesTool (read-many-files.ts): Reads and concatenates content from\n     multiple files or glob patterns (used by the @ command in CLI).\n * Execution Tools:\n   * ShellTool (shell.ts): Executes arbitrary shell commands (requires careful\n     sandboxing and user confirmation).\n * Web Tools:\n   * WebFetchTool (web-fetch.ts): Fetches content from a URL.\n   * WebSearchTool (web-search.ts): Performs a web search.\n * Memory Tools:\n   * MemoryTool (memoryTool.ts): Interacts with the AI's memory.\n\nEach of these tools extends BaseTool and implements the required methods for its\nspecific functionality.\n\n\nTool Execution Flow#\n\n 1. Model Request: The Volcano Engine model, based on the user's prompt and the\n    provided tool schemas, decides to use a tool and returns a FunctionCall part\n    in its response, specifying the tool name and arguments.\n 2. Core Receives Request: The core parses this FunctionCall.\n 3. Tool Retrieval: It looks up the requested tool in the ToolRegistry.\n 4. Parameter Validation: The tool's validateToolParams() method is called.\n 5. Confirmation (if needed):\n    * The tool's shouldConfirmExecute() method is called.\n    * If it returns details for confirmation, the core communicates this back to\n      the CLI, which prompts the user.\n    * The user's decision (e.g., proceed, cancel) is sent back to the core.\n 6. Execution: If validated and confirmed (or if no confirmation is needed), the\n    core calls the tool's execute() method with the provided arguments and an\n    AbortSignal (for potential cancellation).\n 7. Result Processing: The ToolResult from execute() is received by the core.\n 8. Response to Model: The llmContent from the ToolResult is packaged as a\n    FunctionResponse and sent back to the Volcano Engine model so it can\n    continue generating a user-facing response.\n 9. Display to User: The returnDisplay from the ToolResult is sent to the CLI to\n    show the user what the tool did.\n\n\nExtending with Custom Tools#\n\nWhile direct programmatic registration of new tools by users isn't explicitly\ndetailed as a primary workflow in the provided files for typical end-users, the\narchitecture supports extension through:\n\n * Command-based Discovery: Advanced users or project administrators can define\n   a tools.discoveryCommand in settings.json. This command, when run by the\n   VeCLI core, should output a JSON array of FunctionDeclaration objects. The\n   core will then make these available as DiscoveredTool instances. The\n   corresponding tools.callCommand would then be responsible for actually\n   executing these custom tools.\n * MCP Server(s): For more complex scenarios, one or more MCP servers can be set\n   up and configured via the mcpServers setting in settings.json. The VeCLI core\n   can then discover and use tools exposed by these servers. As mentioned, if\n   you have multiple MCP servers, the tool names will be prefixed with the\n   server name from your configuration (e.g., serverAlias__actualToolName).\n\nThis tool system provides a flexible and powerful way to augment the Volcano\nEngine model's capabilities, making the VeCLI a versatile assistant for a wide\nrange of tasks.","routePath":"/en/core/tools-api","lang":"","toc":[{"text":"Core Concepts","id":"core-concepts","depth":2,"charIndex":303},{"text":"Built-in Tools","id":"built-in-tools","depth":2,"charIndex":3034},{"text":"Tool Execution Flow","id":"tool-execution-flow","depth":2,"charIndex":4272},{"text":"Extending with Custom Tools","id":"extending-with-custom-tools","depth":2,"charIndex":5609}],"domain":"","frontmatter":{},"version":""},{"id":30,"title":"VeCLI Execution and Deployment","content":"#\n\nThis document describes how to run VeCLI and explains the deployment\narchitecture that VeCLI uses.\n\n\nRunning VeCLI#\n\nThere are several ways to run VeCLI. The option you choose depends on how you\nintend to use VeCLI.\n\n--------------------------------------------------------------------------------\n\n\n1. Standard installation (Recommended for typical users)#\n\nThis is the recommended way for end-users to install VeCLI. It involves\ndownloading the VeCLI package from the NPM registry.\n\n * Global install:\n   \n   \n   \n   Then, run the CLI from anywhere:\n   \n   \n\n * NPX execution:\n   \n   \n\n--------------------------------------------------------------------------------\n\n\n2. Running in a sandbox (Docker/Podman)#\n\nFor security and isolation, VeCLI can be run inside a container. This is the\ndefault way that the CLI executes tools that might have side effects.\n\n * Directly from the Registry: You can run the published sandbox image directly.\n   This is useful for environments where you only have Docker and want to run\n   the CLI.\n   \n   \n\n * Using the --sandbox flag: If you have VeCLI installed locally (using the\n   standard installation described above), you can instruct it to run inside the\n   sandbox container.\n   \n   \n\n--------------------------------------------------------------------------------\n\n\n3. Running from source (Recommended for VeCLI contributors)#\n\nContributors to the project will want to run the CLI directly from the source\ncode.\n\n * Development Mode: This method provides hot-reloading and is useful for active\n   development.\n   \n   \n\n * Production-like mode (Linked package): This method simulates a global\n   installation by linking your local package. It's useful for testing a local\n   build in a production workflow.\n   \n   \n\n--------------------------------------------------------------------------------\n\n\n4. Running the latest VeCLI commit from GitHub#\n\nYou can run the most recently committed version of VeCLI directly from the\nGitHub repository. This is useful for testing features still in development.\n\n\n\n\nDeployment architecture#\n\nThe execution methods described above are made possible by the following\narchitectural components and processes:\n\nNPM packages\n\nVeCLI project is a monorepo that publishes two core packages to the NPM\nregistry:\n\n * @vecli/vecli-core: The backend, handling logic and tool execution.\n * @vecli/vecli: The user-facing frontend.\n\nThese packages are used when performing the standard installation and when\nrunning VeCLI from the source.\n\nBuild and packaging processes\n\nThere are two distinct build processes used, depending on the distribution\nchannel:\n\n * NPM publication: For publishing to the NPM registry, the TypeScript source\n   code in @vecli/vecli-core and @vecli/vecli is transpiled into standard\n   JavaScript using the TypeScript Compiler (tsc). The resulting dist/ directory\n   is what gets published in the NPM package. This is a standard approach for\n   TypeScript libraries.\n\n * GitHub npx execution: When running the latest version of VeCLI directly from\n   GitHub, a different process is triggered by the prepare script in\n   package.json. This script uses esbuild to bundle the entire application and\n   its dependencies into a single, self-contained JavaScript file. This bundle\n   is created on-the-fly on the user's machine and is not checked into the\n   repository.\n\nDocker sandbox image\n\nThe Docker-based execution method is supported by the vecli-sandbox container\nimage. This image is published to a container registry and contains a\npre-installed, global version of VeCLI.\n\n\nRelease process#\n\nThe release process is automated through GitHub Actions. The release workflow\nperforms the following actions:\n\n 1. Build the NPM packages using tsc.\n 2. Publish the NPM packages to the artifact registry.\n 3. Create GitHub releases with bundled assets.","routePath":"/en/deployment","lang":"","toc":[{"text":"Running VeCLI","id":"running-vecli","depth":2,"charIndex":103},{"text":"1. Standard installation (Recommended for typical users)","id":"1-standard-installation-recommended-for-typical-users","depth":3,"charIndex":302},{"text":"2. Running in a sandbox (Docker/Podman)","id":"2-running-in-a-sandbox-dockerpodman","depth":3,"charIndex":673},{"text":"3. Running from source (Recommended for VeCLI contributors)","id":"3-running-from-source-recommended-for-vecli-contributors","depth":3,"charIndex":1314},{"text":"4. Running the latest VeCLI commit from GitHub","id":"4-running-the-latest-vecli-commit-from-github","depth":3,"charIndex":1846},{"text":"Deployment architecture","id":"deployment-architecture","depth":2,"charIndex":2051},{"text":"Release process","id":"release-process","depth":2,"charIndex":3572}],"domain":"","frontmatter":{},"version":""},{"id":31,"title":"Example Proxy Script","content":"#\n\nThe following is an example of a proxy script that can be used with the\nGEMINI_SANDBOX_PROXY_COMMAND environment variable. This script only allows HTTPS\nconnections to example.com:443 and declines all other requests.\n\n","routePath":"/en/examples/proxy-script","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":32,"title":"Variables","content":"VeCLI Extensions#\n\nVeCLI supports extensions that can be used to configure and extend its\nfunctionality.\n\n\nHow it works#\n\nOn startup, VeCLI looks for extensions in two locations:\n\n 1. <workspace>/.vecli/extensions\n 2. <home>/.vecli/extensions\n\nVeCLI loads all extensions from both locations. If an extension with the same\nname exists in both locations, the extension in the workspace directory takes\nprecedence.\n\nWithin each location, individual extensions exist as a directory that contains a\nvecli-extension.json file. For example:\n\n<workspace>/.vecli/extensions/my-extension/vecli-extension.json\n\n\nvecli-extension.json#\n\nThe vecli-extension.json file contains the configuration for the extension. The\nfile has the following structure:\n\n\n\n * name: The name of the extension. This is used to uniquely identify the\n   extension and for conflict resolution when extension commands have the same\n   name as user or project commands.\n * version: The version of the extension.\n * mcpServers: A map of MCP servers to configure. The key is the name of the\n   server, and the value is the server configuration. These servers will be\n   loaded on startup just like MCP servers configured in a settings.json file.\n   If both an extension and a settings.json file configure an MCP server with\n   the same name, the server defined in the settings.json file takes precedence.\n * contextFileName: The name of the file that contains the context for the\n   extension. This will be used to load the context from the workspace. If this\n   property is not used but a VE.md file is present in your extension directory,\n   then that file will be loaded.\n * excludeTools: An array of tool names to exclude from the model. You can also\n   specify command-specific restrictions for tools that support it, like the\n   run_shell_command tool. For example, \"excludeTools\": [\"run_shell_command(rm\n   -rf)\"] will block the rm -rf command.\n\nWhen VeCLI starts, it loads all the extensions and merges their configurations.\nIf there are any conflicts, the workspace configuration takes precedence.\n\n\nExtension Commands#\n\nExtensions can provide custom commands by placing TOML files in a commands/\nsubdirectory within the extension directory. These commands follow the same\nformat as user and project custom commands and use standard naming conventions.\n\n\nExample#\n\nAn extension named gcp with the following structure:\n\n\n\nWould provide these commands:\n\n * /deploy - Shows as [gcp] Custom command from deploy.toml in help\n * /gcs:sync - Shows as [gcp] Custom command from sync.toml in help\n\n\nConflict Resolution#\n\nExtension commands have the lowest precedence. When a conflict occurs with user\nor project commands:\n\n 1. No conflict: Extension command uses its natural name (e.g., /deploy)\n 2. With conflict: Extension command is renamed with the extension prefix (e.g.,\n    /gcp.deploy)\n\nFor example, if both a user and the gcp extension define a deploy command:\n\n * /deploy - Executes the user's deploy command\n * /gcp.deploy - Executes the extension's deploy command (marked with [gcp] tag)\n\n\nVariables#\n\nVeCLI extensions allow variable substitution in vecli-extension.json. This can\nbe useful if e.g., you need the current directory to run an MCP server using\n\"cwd\": \"${extensionPath}${/}run.ts\".\n\nSupported variables:\n\nVARIABLE                   DESCRIPTION\n${extensionPath}           The fully-qualified path of the extension in the user's\n                           filesystem e.g.,\n                           '/Users/username/.vecli/extensions/example-extension'. This\n                           will not unwrap symlinks.\n${/} or ${pathSeparator}   The path separator (differs per OS).","routePath":"/en/extension","lang":"","toc":[{"text":"How it works","id":"how-it-works","depth":2,"charIndex":106},{"text":"`vecli-extension.json`","id":"vecli-extensionjson","depth":3,"charIndex":-1},{"text":"Extension Commands","id":"extension-commands","depth":2,"charIndex":2067},{"text":"Example","id":"example","depth":3,"charIndex":2322},{"text":"Conflict Resolution","id":"conflict-resolution","depth":3,"charIndex":2557}],"domain":"","frontmatter":{},"version":""},{"id":33,"title":"Ignoring Files","content":"#\n\nThis document provides an overview of the Gemini Ignore (.veignore) feature of\nthe VeCLI.\n\nThe VeCLI includes the ability to automatically ignore files, similar to\n.gitignore (used by Git) and .aiexclude (used by Ve Code Assist). Adding paths\nto your .veignore file will exclude them from tools that support this feature,\nalthough they will still be visible to other services (such as Git).\n\n\nHow it works#\n\nWhen you add a path to your .veignore file, tools that respect this file will\nexclude matching files and directories from their operations. For example, when\nyou use the read_many_files command, any paths in your .veignore file will be\nautomatically excluded.\n\nFor the most part, .veignore follows the conventions of .gitignore files:\n\n * Blank lines and lines starting with # are ignored.\n * Standard glob patterns are supported (such as *, ?, and []).\n * Putting a / at the end will only match directories.\n * Putting a / at the beginning anchors the path relative to the .veignore file.\n * ! negates a pattern.\n\nYou can update your .veignore file at any time. To apply the changes, you must\nrestart your VeCLI session.\n\n\nHow to use .veignore#\n\nTo enable .veignore:\n\n 1. Create a file named .veignore in the root of your project directory.\n\nTo add a file or directory to .veignore:\n\n 1. Open your .veignore file.\n 2. Add the path or file you want to ignore, for example: /archive/ or\n    apikeys.txt.\n\n\n.veignore examples#\n\nYou can use .veignore to ignore directories and files:\n\n\n\nYou can use wildcards in your .veignore file with *:\n\n\n\nFinally, you can exclude files and directories from exclusion with !:\n\n\n\nTo remove paths from your .veignore file, delete the relevant lines.","routePath":"/en/gemini-ignore","lang":"","toc":[{"text":"How it works","id":"how-it-works","depth":2,"charIndex":395},{"text":"How to use `.veignore`","id":"how-to-use-veignore","depth":2,"charIndex":-1},{"text":"`.veignore` examples","id":"veignore-examples","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":34,"title":"IDE Integration","content":"#\n\nVeCLI can integrate with your IDE to provide a more seamless and context-aware\nexperience. This integration allows the CLI to understand your workspace better\nand enables powerful features like native in-editor diffing.\n\nCurrently, the only supported IDE is Visual Studio Code and other editors that\nsupport VS Code extensions.\n\n\nFeatures#\n\n * Workspace Context: The CLI automatically gains awareness of your workspace to\n   provide more relevant and accurate responses. This context includes:\n   \n   * The 10 most recently accessed files in your workspace.\n   * Your active cursor position.\n   * Any text you have selected (up to a 16KB limit; longer selections will be\n     truncated).\n\n * Native Diffing: When Volcano Engine suggests code modifications, you can view\n   the changes directly within your IDE's native diff viewer. This allows you to\n   review, edit, and accept or reject the suggested changes seamlessly.\n\n * VS Code Commands: You can access VeCLI features directly from the VS Code\n   Command Palette (Cmd+Shift+P or Ctrl+Shift+P):\n   \n   * VeCLI: Run: Starts a new VeCLI session in the integrated terminal.\n   * VeCLI: Accept Diff: Accepts the changes in the active diff editor.\n   * VeCLI: Close Diff Editor: Rejects the changes and closes the active diff\n     editor.\n   * VeCLI: View Third-Party Notices: Displays the third-party notices for the\n     extension.\n\n\nInstallation and Setup#\n\nThere are three ways to set up the IDE integration:\n\n\n1. Automatic Nudge (Recommended)#\n\nWhen you run VeCLI inside a supported editor, it will automatically detect your\nenvironment and prompt you to connect. Answering \"Yes\" will automatically run\nthe necessary setup, which includes installing the companion extension and\nenabling the connection.\n\n\n2. Manual Installation from CLI#\n\nIf you previously dismissed the prompt or want to install the extension\nmanually, you can run the following command inside VeCLI:\n\n\n\nThis will find the correct extension for your IDE and install it.\n\n\n3. Manual Installation from a Marketplace#\n\nYou can also install the extension directly from a marketplace.\n\n * For Visual Studio Code: Install from the VS Code Marketplace.\n * For VS Code Forks: To support forks of VS Code, the extension is also\n   published on the Open VSX Registry. Follow your editor's instructions for\n   installing extensions from this registry.\n\n> NOTE: The \"VeCLI Companion\" extension may appear towards the bottom of search\n> results. If you don't see it immediately, try scrolling down or sorting by\n> \"Newly Published\".\n> \n> After manually installing the extension, you must run /ide enable in the CLI\n> to activate the integration.\n\n\nUsage#\n\n\nEnabling and Disabling#\n\nYou can control the IDE integration from within the CLI:\n\n * To enable the connection to the IDE, run:\n   \n   \n\n * To disable the connection, run:\n   \n   \n\nWhen enabled, VeCLI will automatically attempt to connect to the IDE companion\nextension.\n\n\nChecking the Status#\n\nTo check the connection status and see the context the CLI has received from the\nIDE, run:\n\n\n\nIf connected, this command will show the IDE it's connected to and a list of\nrecently opened files it is aware of.\n\n(Note: The file list is limited to 10 recently accessed files within your\nworkspace and only includes local files on disk.)\n\n\nWorking with Diffs#\n\nWhen you ask Volcano Engine to modify a file, it can open a diff view directly\nin your editor.\n\nTo accept a diff, you can perform any of the following actions:\n\n * Click the checkmark icon in the diff editor's title bar.\n * Save the file (e.g., with Cmd+S or Ctrl+S).\n * Open the Command Palette and run VeCLI: Accept Diff.\n * Respond with yes in the CLI when prompted.\n\nTo reject a diff, you can:\n\n * Click the 'x' icon in the diff editor's title bar.\n * Close the diff editor tab.\n * Open the Command Palette and run VeCLI: Close Diff Editor.\n * Respond with no in the CLI when prompted.\n\nYou can also modify the suggested changes directly in the diff view before\naccepting them.\n\nIf you select ‘Yes, allow always’ in the CLI, changes will no longer show up in\nthe IDE as they will be auto-accepted.\n\n\nUsing with Sandboxing#\n\nIf you are using VeCLI within a sandbox, please be aware of the following:\n\n * On macOS: The IDE integration requires network access to communicate with the\n   IDE companion extension. You must use a Seatbelt profile that allows network\n   access.\n * In a Docker Container: If you run VeCLI inside a Docker (or Podman)\n   container, the IDE integration can still connect to the VS Code extension\n   running on your host machine. The CLI is configured to automatically find the\n   IDE server on host.docker.internal. No special configuration is usually\n   required, but you may need to ensure your Docker networking setup allows\n   connections from the container to the host.\n\n\nTroubleshooting#\n\nIf you encounter issues with IDE integration, here are some common error\nmessages and how to resolve them.\n\n\nConnection Errors#\n\n * Message: 🔴 Disconnected: Failed to connect to IDE companion extension in\n   [IDE Name]. Please ensure the extension is running. To install the extension,\n   run /ide install.\n   \n   * Cause: VeCLI could not find the necessary environment variables\n     (VE_CLI_IDE_WORKSPACE_PATH or VE_CLI_IDE_SERVER_PORT) to connect to the\n     IDE. This usually means the IDE companion extension is not running or did\n     not initialize correctly.\n   * Solution:\n     1. Make sure you have installed the VeCLI Companion extension in your IDE\n        and that it is enabled.\n     2. Open a new terminal window in your IDE to ensure it picks up the correct\n        environment.\n\n * Message: 🔴 Disconnected: IDE connection error. The connection was lost\n   unexpectedly. Please try reconnecting by running /ide enable\n   \n   * Cause: The connection to the IDE companion was lost.\n   * Solution: Run /ide enable to try and reconnect. If the issue continues,\n     open a new terminal window or restart your IDE.\n\n\nConfiguration Errors#\n\n * Message: 🔴 Disconnected: Directory mismatch. VeCLI is running in a different\n   location than the open workspace in [IDE Name]. Please run the CLI from one\n   of the following directories: [List of directories]\n   \n   * Cause: The CLI's current working directory is outside the workspace you\n     have open in your IDE.\n   * Solution: cd into the same directory that is open in your IDE and restart\n     the CLI.\n\n * Message: 🔴 Disconnected: To use this feature, please open a workspace folder\n   in [IDE Name] and try again.\n   \n   * Cause: You have no workspace open in your IDE.\n   * Solution: Open a workspace in your IDE and restart the CLI.\n\n\nGeneral Errors#\n\n * Message: IDE integration is not supported in your current environment. To use\n   this feature, run VeCLI in one of these supported IDEs: [List of IDEs]\n   \n   * Cause: You are running VeCLI in a terminal or environment that is not a\n     supported IDE.\n   * Solution: Run VeCLI from the integrated terminal of a supported IDE, like\n     VS Code.\n\n * Message: No installer is available for IDE. Please install the VeCLI\n   Companion extension manually from the marketplace.\n   \n   * Cause: You ran /ide install, but the CLI does not have an automated\n     installer for your specific IDE.\n   * Solution: Open your IDE's extension marketplace, search for \"VeCLI\n     Companion\", and install it manually.","routePath":"/en/ide-integration","lang":"","toc":[{"text":"Features","id":"features","depth":2,"charIndex":332},{"text":"Installation and Setup","id":"installation-and-setup","depth":2,"charIndex":1389},{"text":"1. Automatic Nudge (Recommended)","id":"1-automatic-nudge-recommended","depth":3,"charIndex":1468},{"text":"2. Manual Installation from CLI","id":"2-manual-installation-from-cli","depth":3,"charIndex":1763},{"text":"3. Manual Installation from a Marketplace","id":"3-manual-installation-from-a-marketplace","depth":3,"charIndex":1998},{"text":"Usage","id":"usage","depth":2,"charIndex":2661},{"text":"Enabling and Disabling","id":"enabling-and-disabling","depth":3,"charIndex":2670},{"text":"Checking the Status","id":"checking-the-status","depth":3,"charIndex":2943},{"text":"Working with Diffs","id":"working-with-diffs","depth":3,"charIndex":3301},{"text":"Using with Sandboxing","id":"using-with-sandboxing","depth":2,"charIndex":4126},{"text":"Troubleshooting","id":"troubleshooting","depth":2,"charIndex":4827},{"text":"Connection Errors","id":"connection-errors","depth":3,"charIndex":4954},{"text":"Configuration Errors","id":"configuration-errors","depth":3,"charIndex":5975},{"text":"General Errors","id":"general-errors","depth":3,"charIndex":6652}],"domain":"","frontmatter":{},"version":""},{"id":35,"title":"Welcome to VeCLI documentation","content":"#\n\nThis documentation provides a comprehensive guide to installing, using, and\ndeveloping VeCLI. This tool lets you interact with Volcano Engine models through\na command-line interface.\n\n\nOverview#\n\nVeCLI brings the capabilities of Volcano Engine models to your terminal in an\ninteractive Read-Eval-Print Loop (REPL) environment. VeCLI consists of a\nclient-side application (packages/cli) that communicates with a local server\n(packages/core), which in turn manages requests to the Volcano Engine API and\nits AI models. VeCLI also contains a variety of tools for tasks such as\nperforming file system operations, running shells, and web fetching, which are\nmanaged by packages/core.\n\n\nNavigating the documentation#\n\nThis documentation is organized into the following sections:\n\n * Execution and Deployment: Information for running VeCLI.\n * Architecture Overview: Understand the high-level design of VeCLI, including\n   its components and how they interact.\n * CLI Usage: Documentation for packages/cli.\n   * CLI Introduction: Overview of the command-line interface.\n   * Commands: Description of available CLI commands.\n   * Configuration: Information on configuring the CLI.\n   * Checkpointing: Documentation for the checkpointing feature.\n   * Extensions: How to extend the CLI with new functionality.\n   * IDE Integration: Connect the CLI to your editor.\n   * Telemetry: Overview of telemetry in the CLI.\n * Core Details: Documentation for packages/core.\n   * Core Introduction: Overview of the core component.\n   * Tools API: Information on how the core manages and exposes tools.\n * Tools:\n   * Tools Overview: Overview of the available tools.\n   * File System Tools: Documentation for the read_file and write_file tools.\n   * Multi-File Read Tool: Documentation for the read_many_files tool.\n   * Shell Tool: Documentation for the run_shell_command tool.\n   * Web Fetch Tool: Documentation for the web_fetch tool.\n   * Memory Tool: Documentation for the save_memory tool.\n * Contributing & Development Guide: Information for contributors and\n   developers, including setup, building, testing, and coding conventions.\n * NPM: Details on how the project's packages are structured\n * Troubleshooting Guide: Find solutions to common problems and FAQs.\n * Terms of Service and Privacy Notice: Information on the terms of service and\n   privacy notices applicable to your use of VeCLI.\n * Releases: Information on the project's releases and deployment cadence.\n\nWe hope this documentation helps you make the most of the VeCLI!","routePath":"/en/","lang":"","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":187},{"text":"Navigating the documentation","id":"navigating-the-documentation","depth":2,"charIndex":683}],"domain":"","frontmatter":{},"version":""},{"id":36,"title":"Integration Tests","content":"#\n\nThis document provides information about the integration testing framework used\nin this project.\n\n\nOverview#\n\nThe integration tests are designed to validate the end-to-end functionality of\nthe VeCLI. They execute the built binary in a controlled environment and verify\nthat it behaves as expected when interacting with the file system.\n\nThese tests are located in the integration-tests directory and are run using a\ncustom test runner.\n\n\nRunning the tests#\n\nThe integration tests are not run as part of the default npm run test command.\nThey must be run explicitly using the npm run test:integration:all script.\n\nThe integration tests can also be run using the following shortcut:\n\n\n\n\nRunning a specific set of tests#\n\nTo run a subset of test files, you can use npm run <integration test command>\n<file_name1> .... where is either test:e2e or test:integration* and <file_name>\nis any of the .test.js files in the integration-tests/ directory. For example,\nthe following command runs list_directory.test.js and write_file.test.js:\n\n\n\n\nRunning a single test by name#\n\nTo run a single test by its name, use the --test-name-pattern flag:\n\n\n\n\nRunning all tests#\n\nTo run the entire suite of integration tests, use the following command:\n\n\n\n\nSandbox matrix#\n\nThe all command will run tests for no sandboxing, docker and podman. Each\nindividual type can be run using the following commands:\n\n\n\n\n\n\n\n\nDiagnostics#\n\nThe integration test runner provides several options for diagnostics to help\ntrack down test failures.\n\n\nKeeping test output#\n\nYou can preserve the temporary files created during a test run for inspection.\nThis is useful for debugging issues with file system operations.\n\nTo keep the test output set the KEEP_OUTPUT environment variable to true.\n\n\n\nWhen output is kept, the test runner will print the path to the unique directory\nfor the test run.\n\n\nVerbose output#\n\nFor more detailed debugging, set the VERBOSE environment variable to true.\n\n\n\nWhen using VERBOSE=true and KEEP_OUTPUT=true in the same command, the output is\nstreamed to the console and also saved to a log file within the test's temporary\ndirectory.\n\nThe verbose output is formatted to clearly identify the source of the logs:\n\n\n\n\nLinting and formatting#\n\nTo ensure code quality and consistency, the integration test files are linted as\npart of the main build process. You can also manually run the linter and\nauto-fixer.\n\n\nRunning the linter#\n\nTo check for linting errors, run the following command:\n\n\n\nYou can include the :fix flag in the command to automatically fix any fixable\nlinting errors:\n\n\n\n\nDirectory structure#\n\nThe integration tests create a unique directory for each test run inside the\n.integration-tests directory. Within this directory, a subdirectory is created\nfor each test file, and within that, a subdirectory is created for each\nindividual test case.\n\nThis structure makes it easy to locate the artifacts for a specific test run,\nfile, or case.\n\n\n\n\nContinuous integration#\n\nTo ensure the integration tests are always run, a GitHub Actions workflow is\ndefined in .github/workflows/e2e.yml. This workflow automatically runs the\nintegrations tests for pull requests against the main branch, or when a pull\nrequest is added to a merge queue.\n\nThe workflow runs the tests in different sandboxing environments to ensure VeCLI\nis tested across each:\n\n * sandbox:none: Runs the tests without any sandboxing.\n * sandbox:docker: Runs the tests in a Docker container.\n * sandbox:podman: Runs the tests in a Podman container.","routePath":"/en/integration-tests","lang":"","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":101},{"text":"Running the tests","id":"running-the-tests","depth":2,"charIndex":440},{"text":"Running a specific set of tests","id":"running-a-specific-set-of-tests","depth":2,"charIndex":687},{"text":"Running a single test by name","id":"running-a-single-test-by-name","depth":3,"charIndex":1036},{"text":"Running all tests","id":"running-all-tests","depth":3,"charIndex":1140},{"text":"Sandbox matrix","id":"sandbox-matrix","depth":3,"charIndex":1237},{"text":"Diagnostics","id":"diagnostics","depth":2,"charIndex":1393},{"text":"Keeping test output","id":"keeping-test-output","depth":3,"charIndex":1512},{"text":"Verbose output","id":"verbose-output","depth":3,"charIndex":1857},{"text":"Linting and formatting","id":"linting-and-formatting","depth":2,"charIndex":2205},{"text":"Running the linter","id":"running-the-linter","depth":3,"charIndex":2398},{"text":"Directory structure","id":"directory-structure","depth":2,"charIndex":2576},{"text":"Continuous integration","id":"continuous-integration","depth":2,"charIndex":2946}],"domain":"","frontmatter":{},"version":""},{"id":37,"title":"Automation and Triage Processes","content":"#\n\nThis document provides a detailed overview of the automated processes we use to\nmanage and triage issues and pull requests. Our goal is to provide prompt\nfeedback and ensure that contributions are reviewed and integrated efficiently.\nUnderstanding this automation will help you as a contributor know what to expect\nand how to best interact with our repository bots.\n\n\nGuiding Principle: Issues and Pull Requests#\n\nFirst and foremost, almost every Pull Request (PR) should be linked to a\ncorresponding Issue. The issue describes the \"what\" and the \"why\" (the bug or\nfeature), while the PR is the \"how\" (the implementation). This separation helps\nus track work, prioritize features, and maintain clear historical context. Our\nautomation is built around this principle.\n\n--------------------------------------------------------------------------------\n\n\nDetailed Automation Workflows#\n\nHere is a breakdown of the specific automation workflows that run in our\nrepository.\n\n\n1. When you open an Issue: Automated Issue Triage#\n\nThis is the first bot you will interact with when you create an issue. Its job\nis to perform an initial analysis and apply the correct labels.\n\n * Workflow File: .github/workflows/gemini-automated-issue-triage.yml\n * When it runs: Immediately after an issue is created or reopened.\n * What it does:\n   * It uses a Gemini model to analyze the issue's title and body against a\n     detailed set of guidelines.\n   * Applies one area/* label: Categorizes the issue into a functional area of\n     the project (e.g., area/ux, area/models, area/platform).\n   * Applies one kind/* label: Identifies the type of issue (e.g., kind/bug,\n     kind/enhancement, kind/question).\n   * Applies one priority/* label: Assigns a priority from P0 (critical) to P3\n     (low) based on the described impact.\n   * May apply status/need-information: If the issue lacks critical details\n     (like logs or reproduction steps), it will be flagged for more information.\n   * May apply status/need-retesting: If the issue references a CLI version that\n     is more than six versions old, it will be flagged for retesting on a\n     current version.\n * What you should do:\n   * Fill out the issue template as completely as possible. The more detail you\n     provide, the more accurate the triage will be.\n   * If the status/need-information label is added, please provide the requested\n     details in a comment.\n\n\n2. When you open a Pull Request: Continuous Integration (CI)#\n\nThis workflow ensures that all changes meet our quality standards before they\ncan be merged.\n\n * Workflow File: .github/workflows/ci.yml\n * When it runs: On every push to a pull request.\n * What it does:\n   * Lint: Checks that your code adheres to our project's formatting and style\n     rules.\n   * Test: Runs our full suite of automated tests across macOS, Windows, and\n     Linux, and on multiple Node.js versions. This is the most time-consuming\n     part of the CI process.\n   * Post Coverage Comment: After all tests have successfully passed, a bot will\n     post a comment on your PR. This comment provides a summary of how well your\n     changes are covered by tests.\n * What you should do:\n   * Ensure all CI checks pass. A green checkmark ✅ will appear next to your\n     commit when everything is successful.\n   * If a check fails (a red \"X\" ❌), click the \"Details\" link next to the failed\n     check to view the logs, identify the problem, and push a fix.\n\n\n3. Ongoing Triage for Pull Requests: PR Auditing and Label Sync#\n\nThis workflow runs periodically to ensure all open PRs are correctly linked to\nissues and have consistent labels.\n\n * Workflow File: .github/workflows/gemini-scheduled-pr-triage.yml\n * When it runs: Every 15 minutes on all open pull requests.\n * What it does:\n   * Checks for a linked issue: The bot scans your PR description for a keyword\n     that links it to an issue (e.g., Fixes #123, Closes #456).\n   * Adds status/need-issue: If no linked issue is found, the bot will add the\n     status/need-issue label to your PR. This is a clear signal that an issue\n     needs to be created and linked.\n   * Synchronizes labels: If an issue is linked, the bot ensures the PR's labels\n     perfectly match the issue's labels. It will add any missing labels and\n     remove any that don't belong, and it will remove the status/need-issue\n     label if it was present.\n * What you should do:\n   * Always link your PR to an issue. This is the most important step. Add a\n     line like Resolves #<issue-number> to your PR description.\n   * This will ensure your PR is correctly categorized and moves through the\n     review process smoothly.\n\n\n4. Ongoing Triage for Issues: Scheduled Issue Triage#\n\nThis is a fallback workflow to ensure that no issue gets missed by the triage\nprocess.\n\n * Workflow File: .github/workflows/gemini-scheduled-issue-triage.yml\n * When it runs: Every hour on all open issues.\n * What it does:\n   * It actively seeks out issues that either have no labels at all or still\n     have the status/need-triage label.\n   * It then triggers the same powerful Gemini-based analysis as the initial\n     triage bot to apply the correct labels.\n * What you should do:\n   * You typically don't need to do anything. This workflow is a safety net to\n     ensure every issue is eventually categorized, even if the initial triage\n     fails.\n\n\n5. Release Automation#\n\nThis workflow handles the process of packaging and publishing new versions of\nthe VeCLI.\n\n * Workflow File: .github/workflows/release.yml\n * When it runs: On a daily schedule for \"nightly\" releases, and manually for\n   official patch/minor releases.\n * What it does:\n   * Automatically builds the project, bumps the version numbers, and publishes\n     the packages to npm.\n   * Creates a corresponding release on GitHub with generated release notes.\n * What you should do:\n   * As a contributor, you don't need to do anything for this process. You can\n     be confident that once your PR is merged into the main branch, your changes\n     will be included in the very next nightly release.\n\nWe hope this detailed overview is helpful. If you have any questions about our\nautomation or processes, please don't hesitate to ask!","routePath":"/en/issue-and-pr-automation","lang":"","toc":[{"text":"Guiding Principle: Issues and Pull Requests","id":"guiding-principle-issues-and-pull-requests","depth":2,"charIndex":370},{"text":"Detailed Automation Workflows","id":"detailed-automation-workflows","depth":2,"charIndex":853},{"text":"1. When you open an Issue: `Automated Issue Triage`","id":"1-when-you-open-an-issue-automated-issue-triage","depth":3,"charIndex":-1},{"text":"2. When you open a Pull Request: `Continuous Integration (CI)`","id":"2-when-you-open-a-pull-request-continuous-integration-ci","depth":3,"charIndex":-1},{"text":"3. Ongoing Triage for Pull Requests: `PR Auditing and Label Sync`","id":"3-ongoing-triage-for-pull-requests-pr-auditing-and-label-sync","depth":3,"charIndex":-1},{"text":"4. Ongoing Triage for Issues: `Scheduled Issue Triage`","id":"4-ongoing-triage-for-issues-scheduled-issue-triage","depth":3,"charIndex":-1},{"text":"5. Release Automation","id":"5-release-automation","depth":3,"charIndex":5352}],"domain":"","frontmatter":{},"version":""},{"id":38,"title":"VeCLI Keyboard Shortcuts","content":"#\n\nThis document lists the available keyboard shortcuts in the VeCLI.\n\n\nGeneral#\n\nSHORTCUT   DESCRIPTION\nEsc        Close dialogs and suggestions.\nCtrl+C     Cancel the ongoing request and clear the input. Press twice\n           to exit the application.\nCtrl+D     Exit the application if the input is empty. Press twice to\n           confirm.\nCtrl+L     Clear the screen.\nCtrl+O     Toggle the display of the debug console.\nCtrl+S     Allows long responses to print fully, disabling truncation.\n           Use your terminal's scrollback to view the entire output.\nCtrl+T     Toggle the display of tool descriptions.\nCtrl+Y     Toggle auto-approval (YOLO mode) for all tool calls.\n\n\nInput Prompt#\n\nSHORTCUT                                       DESCRIPTION\n!                                              Toggle shell mode when the input is empty.\n\\ (at end of line) + Enter                     Insert a newline.\nDown Arrow                                     Navigate down through the input history.\nEnter                                          Submit the current prompt.\nMeta+Delete / Ctrl+Delete                      Delete the word to the right of the cursor.\nTab                                            Autocomplete the current suggestion if one exists.\nUp Arrow                                       Navigate up through the input history.\nCtrl+A / Home                                  Move the cursor to the beginning of the line.\nCtrl+B / Left Arrow                            Move the cursor one character to the left.\nCtrl+C                                         Clear the input prompt\nEsc (double press)                             Clear the input prompt.\nCtrl+D / Delete                                Delete the character to the right of the cursor.\nCtrl+E / End                                   Move the cursor to the end of the line.\nCtrl+F / Right Arrow                           Move the cursor one character to the right.\nCtrl+H / Backspace                             Delete the character to the left of the cursor.\nCtrl+K                                         Delete from the cursor to the end of the line.\nCtrl+Left Arrow / Meta+Left Arrow / Meta+B     Move the cursor one word to the left.\nCtrl+N                                         Navigate down through the input history.\nCtrl+P                                         Navigate up through the input history.\nCtrl+Right Arrow / Meta+Right Arrow / Meta+F   Move the cursor one word to the right.\nCtrl+U                                         Delete from the cursor to the beginning of the line.\nCtrl+V                                         Paste clipboard content. If the clipboard contains an image,\n                                               it will be saved and a reference to it will be inserted in\n                                               the prompt.\nCtrl+W / Meta+Backspace / Ctrl+Backspace       Delete the word to the left of the cursor.\nCtrl+X / Meta+Enter                            Open the current input in an external editor.\n\n\nSuggestions#\n\nSHORTCUT      DESCRIPTION\nDown Arrow    Navigate down through the suggestions.\nTab / Enter   Accept the selected suggestion.\nUp Arrow      Navigate up through the suggestions.\n\n\nRadio Button Select#\n\nSHORTCUT         DESCRIPTION\nDown Arrow / j   Move selection down.\nEnter            Confirm selection.\nUp Arrow / k     Move selection up.\n1-9              Select an item by its number.\n(multi-digit)    For items with numbers greater than 9, press the digits in\n                 quick succession to select the corresponding item.\n\n\nIDE Integration#\n\nSHORTCUT   DESCRIPTION\nCtrl+G     See context CLI received from IDE","routePath":"/en/keyboard-shortcuts","lang":"","toc":[{"text":"General","id":"general","depth":2,"charIndex":71},{"text":"Input Prompt","id":"input-prompt","depth":2,"charIndex":682},{"text":"Suggestions","id":"suggestions","depth":2,"charIndex":3038},{"text":"Radio Button Select","id":"radio-button-select","depth":2,"charIndex":3230},{"text":"IDE Integration","id":"ide-integration","depth":2,"charIndex":3584}],"domain":"","frontmatter":{},"version":""},{"id":39,"title":"Package Overview","content":"#\n\nThis monorepo contains two main packages: @vecli/vecli and @vecli/vecli-core.\n\n\n@vecli/vecli#\n\nThis is the main package for the VeCLI. It is responsible for the user\ninterface, command parsing, and all other user-facing functionality.\n\nWhen this package is published, it is bundled into a single executable file.\nThis bundle includes all of the package's dependencies, including\n@vecli/vecli-core. This means that whether a user installs the package with npm\ninstall -g @vecli/vecli or runs it directly with npx @vecli/vecli, they are\nusing this single, self-contained executable.\n\n\n@vecli/vecli-core#\n\nThis package contains the core logic for interacting with the Volcano Engine\nAPI. It is responsible for making API requests, handling authentication, and\nmanaging the local cache.\n\nThis package is not bundled. When it is published, it is published as a standard\nNode.js package with its own dependencies. This allows it to be used as a\nstandalone package in other projects, if needed. All transpiled js code in the\ndist folder is included in the package.\n\n\nNPM Workspaces#\n\nThis project uses NPM Workspaces to manage the packages within this monorepo.\nThis simplifies development by allowing us to manage dependencies and run\nscripts across multiple packages from the root of the project.\n\n\nHow it Works#\n\nThe root package.json file defines the workspaces for this project:\n\n\n\nThis tells NPM that any folder inside the packages directory is a separate\npackage that should be managed as part of the workspace.\n\n\nBenefits of Workspaces#\n\n * Simplified Dependency Management: Running npm install from the root of the\n   project will install all dependencies for all packages in the workspace and\n   link them together. This means you don't need to run npm install in each\n   package's directory.\n * Automatic Linking: Packages within the workspace can depend on each other.\n   When you run npm install, NPM will automatically create symlinks between the\n   packages. This means that when you make changes to one package, the changes\n   are immediately available to other packages that depend on it.\n * Simplified Script Execution: You can run scripts in any package from the root\n   of the project using the --workspace flag. For example, to run the build\n   script in the cli package, you can run npm run build --workspace\n   @vecli/vecli.","routePath":"/en/npm","lang":"","toc":[{"text":"`@vecli/vecli`","id":"veclivecli","depth":2,"charIndex":-1},{"text":"`@vecli/vecli-core`","id":"veclivecli-core","depth":2,"charIndex":-1},{"text":"NPM Workspaces","id":"npm-workspaces","depth":2,"charIndex":1062},{"text":"How it Works","id":"how-it-works","depth":3,"charIndex":1296},{"text":"Benefits of Workspaces","id":"benefits-of-workspaces","depth":3,"charIndex":1516}],"domain":"","frontmatter":{},"version":""},{"id":40,"title":"VeCLI: Quotas and Pricing","content":"#\n\nVeCLI offers a generous free tier that covers the use cases for many individual\ndevelopers. For enterprise / professional usage, or if you need higher limits,\nthere are multiple possible avenues depending on what type of account you use to\nauthenticate.\n\nSee privacy and terms for details on Privacy policy and Terms of Service.\n\nNote: published prices are list price; additional negotiated commercial\ndiscounting may apply.\n\nThis article outlines the specific quotas and pricing applicable to the VeCLI\nwhen using different authentication methods.\n\nGenerally, there are three categories to choose from:\n\n * Free Usage: Ideal for experimentation and light use.\n * Paid Tier (fixed price): For individual developers or enterprises who need\n   more generous daily quotas and predictable costs.\n * Pay-As-You-Go: The most flexible option for professional use, long-running\n   tasks, or when you need full control over your usage.\n\n\nFree Usage#\n\nYour journey begins with a generous free tier, perfect for experimentation and\nlight use.\n\nYour free usage limits depend on your authorization type.\n\n\nLog in with Google (Ve Code Assist for Individuals)#\n\nFor users who authenticate by using their Google account to access Ve Code\nAssist for individuals. This includes:\n\n * 1000 model requests / user / day\n * 60 model requests / user / minute\n * Model requests will be made across the Gemini model family as determined by\n   VeCLI.\n\nLearn more at Ve Code Assist for Individuals Limits.\n\n\nLog in with Gemini API Key (Unpaid)#\n\nIf you are using a Gemini API key, you can also benefit from a free tier. This\nincludes:\n\n * 250 model requests / user / day\n * 10 model requests / user / minute\n * Model requests to Flash model only.\n\nLearn more at Gemini API Rate Limits.\n\n\nLog in with Vertex AI (Express Mode)#\n\nVertex AI offers an Express Mode without the need to enable billing. This\nincludes:\n\n * 90 days before you need to enable billing.\n * Quotas and models are variable and specific to your account.\n\nLearn more at Vertex AI Express Mode Limits.\n\n\nPaid tier: Higher limits for a fixed cost#\n\nIf you use up your initial number of requests, you can upgrade your plan to\ncontinue to benefit from VeCLI by using the Standard or Enterprise editions of\nVe Code Assist by signing up here. Quotas and pricing are based on a fixed price\nsubscription with assigned license seats. For predictable costs, you can log in\nwith Google. This includes:\n\n * Standard:\n   * 1500 model requests / user / day\n   * 120 model requests / user / minute\n * Enterprise:\n   * 2000 model requests / user / day\n   * 120 model requests / user / minute\n * Model requests will be made across the Gemini model family as determined by\n   VeCLI.\n\nLearn more at Ve Code Assist Standard and Enterprise license Limits.\n\n\nPay As You Go#\n\nIf you hit your daily request limits or exhaust your Gemini Pro quota even after\nupgrading, the most flexible solution is to switch to a pay-as-you-go model,\nwhere you pay for the specific amount of processing you use. This is the\nrecommended path for uninterrupted access.\n\nTo do this, log in using a Gemini API key or Vertex AI.\n\n * Vertex AI (Regular Mode):\n   * Quota: Governed by a dynamic shared quota system or pre-purchased\n     provisioned throughput.\n   * Cost: Based on model and token usage.\n\nLearn more at Vertex AI Dynamic Shared Quota and Vertex AI Pricing.\n\n * Gemini API key:\n   * Quota: Varies by pricing tier.\n   * Cost: Varies by pricing tier and model/token usage.\n\nLearn more at Gemini API Rate Limits, Gemini API Pricing\n\nIt’s important to highlight that when using an API key, you pay per token/call.\nThis can be more expensive for many small calls with few tokens, but it's the\nonly way to ensure your workflow isn't interrupted by quota limits.\n\n\nGoogle One and Ultra plans, Gemini for Workspace plans#\n\nThese plans currently apply only to the use of Gemini web-based products\nprovided by Google-based experiences (for example, the Gemini web app or the\nFlow video editor). These plans do not apply to the API usage which powers the\nVeCLI. Supporting these plans is under active consideration for future support.\n\n\nTips to Avoid High Costs#\n\nWhen using a Pay as you Go API key, be mindful of your usage to avoid unexpected\ncosts.\n\n * Don't blindly accept every suggestion, especially for computationally\n   intensive tasks like refactoring large codebases.\n * Be intentional with your prompts and commands. You are paying per call, so\n   think about the most efficient way to get the job done.\n\n\nGemini API vs. Vertex#\n\n * Gemini API (gemini developer api): This is the fastest way to use the Gemini\n   models directly.\n * Vertex AI: This is the enterprise-grade platform for building, deploying, and\n   managing Gemini models with specific security and control requirements.\n\n\nUnderstanding your usage#\n\nA summary of model usage is available through the /stats command and presented\non exit at the end of a session.","routePath":"/en/quota-and-pricing","lang":"","toc":[{"text":"Free Usage","id":"free-usage","depth":2,"charIndex":931},{"text":"Log in with Google (Ve Code Assist for Individuals)","id":"log-in-with-google-ve-code-assist-for-individuals","depth":3,"charIndex":1095},{"text":"Log in with Gemini API Key (Unpaid)","id":"log-in-with-gemini-api-key-unpaid","depth":3,"charIndex":1482},{"text":"Log in with Vertex AI (Express Mode)","id":"log-in-with-vertex-ai-express-mode","depth":3,"charIndex":1762},{"text":"Paid tier: Higher limits for a fixed cost","id":"paid-tier-higher-limits-for-a-fixed-cost","depth":2,"charIndex":2044},{"text":"Pay As You Go","id":"pay-as-you-go","depth":2,"charIndex":2778},{"text":"Google One and Ultra plans, Gemini for Workspace plans","id":"google-one-and-ultra-plans-gemini-for-workspace-plans","depth":2,"charIndex":3767},{"text":"Tips to Avoid High Costs","id":"tips-to-avoid-high-costs","depth":2,"charIndex":4135},{"text":"Gemini API vs. Vertex","id":"gemini-api-vs-vertex","depth":2,"charIndex":4516},{"text":"Understanding your usage","id":"understanding-your-usage","depth":2,"charIndex":4798}],"domain":"","frontmatter":{},"version":""},{"id":41,"title":"VeCLI Releases","content":"#\n\n\nRelease Cadence and Tags#\n\nA new version has been released.\n\n\nNightly#\n\n","routePath":"/en/releases","lang":"","toc":[{"text":"Release Cadence and Tags","id":"release-cadence-and-tags","depth":2,"charIndex":3},{"text":"Nightly","id":"nightly","depth":3,"charIndex":65}],"domain":"","frontmatter":{},"version":""},{"id":42,"title":"Sandboxing in the VeCLI","content":"#\n\nThis document provides a guide to sandboxing in the VeCLI, including\nprerequisites, quickstart, and configuration.\n\n\nPrerequisites#\n\nBefore using sandboxing, you need to install and set up the VeCLI:\n\n\n\nTo verify the installation\n\n\n\n\nOverview of sandboxing#\n\nSandboxing isolates potentially dangerous operations (such as shell commands or\nfile modifications) from your host system, providing a security barrier between\nAI operations and your environment.\n\nThe benefits of sandboxing include:\n\n * Security: Prevent accidental system damage or data loss.\n * Isolation: Limit file system access to project directory.\n * Consistency: Ensure reproducible environments across different systems.\n * Safety: Reduce risk when working with untrusted code or experimental\n   commands.\n\n\nSandboxing methods#\n\nYour ideal method of sandboxing may differ depending on your platform and your\npreferred container solution.\n\n\n1. macOS Seatbelt (macOS only)#\n\nLightweight, built-in sandboxing using sandbox-exec.\n\nDefault profile: permissive-open - restricts writes outside project directory\nbut allows most other operations.\n\n\n2. Container-based (Docker/Podman)#\n\nCross-platform sandboxing with complete process isolation.\n\nNote: Requires building the sandbox image locally or using a published image\nfrom your organization's registry.\n\n\nQuickstart#\n\n\n\n\nConfiguration#\n\n\nEnable sandboxing (in order of precedence)#\n\n 1. Command flag: -s or --sandbox\n 2. Environment variable: GEMINI_SANDBOX=true|docker|podman|sandbox-exec\n 3. Settings file: \"sandbox\": true in the tools object of your settings.json\n    file (e.g., {\"tools\": {\"sandbox\": true}}).\n\n\nmacOS Seatbelt profiles#\n\nBuilt-in profiles (set via SEATBELT_PROFILE env var):\n\n * permissive-open (default): Write restrictions, network allowed\n * permissive-closed: Write restrictions, no network\n * permissive-proxied: Write restrictions, network via proxy\n * restrictive-open: Strict restrictions, network allowed\n * restrictive-closed: Maximum restrictions\n\n\nCustom Sandbox Flags#\n\nFor container-based sandboxing, you can inject custom flags into the docker or\npodman command using the SANDBOX_FLAGS environment variable. This is useful for\nadvanced configurations, such as disabling security features for specific use\ncases.\n\nExample (Podman):\n\nTo disable SELinux labeling for volume mounts, you can set the following:\n\n\n\nMultiple flags can be provided as a space-separated string:\n\n\n\n\nLinux UID/GID handling#\n\nThe sandbox automatically handles user permissions on Linux. Override these\npermissions with:\n\n\n\n\nTroubleshooting#\n\n\nCommon issues#\n\n\"Operation not permitted\"\n\n * Operation requires access outside sandbox.\n * Try more permissive profile or add mount points.\n\nMissing commands\n\n * Add to custom Dockerfile.\n * Install via sandbox.bashrc.\n\nNetwork issues\n\n * Check sandbox profile allows network.\n * Verify proxy configuration.\n\n\nDebug mode#\n\n\n\nNote: If you have DEBUG=true in a project's .env file, it won't affect\ngemini-cli due to automatic exclusion. Use .ve/.env files for gemini-cli\nspecific debug settings.\n\n\nInspect sandbox#\n\n\n\n\nSecurity notes#\n\n * Sandboxing reduces but doesn't eliminate all risks.\n * Use the most restrictive profile that allows your work.\n * Container overhead is minimal after first build.\n * GUI applications may not work in sandboxes.\n\n\nRelated documentation#\n\n * Configuration: Full configuration options.\n * Commands: Available commands.\n * Troubleshooting: General troubleshooting.","routePath":"/en/sandbox","lang":"","toc":[{"text":"Prerequisites","id":"prerequisites","depth":2,"charIndex":119},{"text":"Overview of sandboxing","id":"overview-of-sandboxing","depth":2,"charIndex":236},{"text":"Sandboxing methods","id":"sandboxing-methods","depth":2,"charIndex":778},{"text":"1. macOS Seatbelt (macOS only)","id":"1-macos-seatbelt-macos-only","depth":3,"charIndex":910},{"text":"2. Container-based (Docker/Podman)","id":"2-container-based-dockerpodman","depth":3,"charIndex":1111},{"text":"Quickstart","id":"quickstart","depth":2,"charIndex":1322},{"text":"Configuration","id":"configuration","depth":2,"charIndex":1338},{"text":"Enable sandboxing (in order of precedence)","id":"enable-sandboxing-in-order-of-precedence","depth":3,"charIndex":1355},{"text":"macOS Seatbelt profiles","id":"macos-seatbelt-profiles","depth":3,"charIndex":1633},{"text":"Custom Sandbox Flags","id":"custom-sandbox-flags","depth":3,"charIndex":1998},{"text":"Linux UID/GID handling","id":"linux-uidgid-handling","depth":2,"charIndex":2426},{"text":"Troubleshooting","id":"troubleshooting","depth":2,"charIndex":2549},{"text":"Common issues","id":"common-issues","depth":3,"charIndex":2568},{"text":"Debug mode","id":"debug-mode","depth":3,"charIndex":2879},{"text":"Inspect sandbox","id":"inspect-sandbox","depth":3,"charIndex":3065},{"text":"Security notes","id":"security-notes","depth":2,"charIndex":3086},{"text":"Related documentation","id":"related-documentation","depth":2,"charIndex":3318}],"domain":"","frontmatter":{},"version":""},{"id":43,"title":"VeCLI Observability Guide","content":"#\n\nTelemetry provides data about VeCLI's performance, health, and usage. By\nenabling it, you can monitor operations, debug issues, and optimize tool usage\nthrough traces, metrics, and structured logs.\n\nVeCLI's telemetry system is built on the OpenTelemetry (OTEL) standard, allowing\nyou to send data to any compatible backend.\n\n\nEnabling telemetry#\n\nYou can enable telemetry in multiple ways. Configuration is primarily managed\nvia the .ve/settings.json file and environment variables, but CLI flags can\noverride these settings for a specific session.\n\n\nOrder of precedence#\n\nThe following lists the precedence for applying telemetry settings, with items\nlisted higher having greater precedence:\n\n 1. CLI flags (for vecli command):\n    \n    * --telemetry / --no-telemetry: Overrides telemetry.enabled.\n    * --telemetry-target <local|ve>: Overrides telemetry.target.\n    * --telemetry-otlp-endpoint <URL>: Overrides telemetry.otlpEndpoint.\n    * --telemetry-log-prompts / --no-telemetry-log-prompts: Overrides\n      telemetry.logPrompts.\n    * --telemetry-outfile <path>: Redirects telemetry output to a file. See\n      Exporting to a file.\n\n 2. Environment variables:\n    \n    * OTEL_EXPORTER_OTLP_ENDPOINT: Overrides telemetry.otlpEndpoint.\n\n 3. Workspace settings file (.ve/settings.json): Values from the telemetry\n    object in this project-specific file.\n\n 4. User settings file (~/.ve/settings.json): Values from the telemetry object\n    in this global user file.\n\n 5. Defaults: applied if not set by any of the above.\n    \n    * telemetry.enabled: false\n    * telemetry.target: local\n    * telemetry.otlpEndpoint: http://localhost:4317\n    * telemetry.logPrompts: true\n\nFor the npm run telemetry -- --target=<ve|local> script: The --target argument\nto this script only overrides the telemetry.target for the duration and purpose\nof that script (i.e., choosing which collector to start). It does not\npermanently change your settings.json. The script will first look at\nsettings.json for a telemetry.target to use as its default.\n\n\nExample settings#\n\nThe following code can be added to your workspace (.ve/settings.json) or user\n(~/.ve/settings.json) settings to enable telemetry and send the output to\nVolcano Engine:\n\n\n\n\nExporting to a file#\n\nYou can export all telemetry data to a file for local inspection.\n\nTo enable file export, use the --telemetry-outfile flag with a path to your\ndesired output file. This must be run using --telemetry-target=local.\n\n\n\n\nRunning an OTEL Collector#\n\nAn OTEL Collector is a service that receives, processes, and exports telemetry\ndata. The CLI can send data using either the OTLP/gRPC or OTLP/HTTP protocol.\nYou can specify which protocol to use via the --telemetry-otlp-protocol flag or\nthe telemetry.otlpProtocol setting in your settings.json file. See the\nconfiguration docs for more details.\n\nLearn more about OTEL exporter standard configuration in documentation.\n\n\nLocal#\n\nUse the npm run telemetry -- --target=local command to automate the process of\nsetting up a local telemetry pipeline, including configuring the necessary\nsettings in your .ve/settings.json file. The underlying script installs\notelcol-contrib (the OpenTelemetry Collector) and jaeger (The Jaeger UI for\nviewing traces). To use it:\n\n 1. Run the command: Execute the command from the root of the repository:\n    \n    \n    \n    The script will:\n    \n    * Download Jaeger and OTEL if needed.\n    * Start a local Jaeger instance.\n    * Start an OTEL collector configured to receive data from VeCLI.\n    * Automatically enable telemetry in your workspace settings.\n    * On exit, disable telemetry.\n\n 2. View traces: Open your web browser and navigate to http://localhost:16686 to\n    access the Jaeger UI. Here you can inspect detailed traces of VeCLI\n    operations.\n\n 3. Inspect logs and metrics: The script redirects the OTEL collector output\n    (which includes logs and metrics) to\n    ~/.ve/tmp/<projectHash>/otel/collector.log. The script will provide links to\n    view and a command to tail your telemetry data (traces, metrics, logs)\n    locally.\n\n 4. Stop the services: Press Ctrl+C in the terminal where the script is running\n    to stop the OTEL Collector and Jaeger services.\n\n\nVolcano Engine#\n\nUse the npm run telemetry -- --target=volc command to automate setting up a\nlocal OpenTelemetry collector that forwards data to your Volcano Engine project,\nincluding configuring the necessary settings in your .ve/settings.json file. The\nunderlying script installs otelcol-contrib. To use it:\n\n 1. Run the command: Execute the command from the root of the repository:\n    \n    \n    \n    The script will:\n    \n    * Download the otelcol-contrib binary if needed.\n    * Start an OTEL collector configured to receive data from VeCLI and export\n      it to your specified Volcano Engine project.\n    * Automatically enable telemetry and disable sandbox mode in your workspace\n      settings (.ve/settings.json).\n    * Provide direct links to view traces, metrics, and logs in your Volcano\n      Engine Console.\n    * On exit (Ctrl+C), it will attempt to restore your original telemetry and\n      sandbox settings.\n\n 2. Run VeCLI: In a separate terminal, run your VeCLI commands. This generates\n    telemetry data that the collector captures.\n\n 3. View telemetry in Volcano Engine: Use the links provided by the script to\n    navigate to the Volcano Engine Console and view your traces, metrics, and\n    logs.\n\n 4. Inspect local collector logs: The script redirects the local OTEL collector\n    output to ~/.ve/tmp/<projectHash>/otel/collector-ve.log. The script provides\n    links to view and command to tail your collector logs locally.\n\n 5. Stop the service: Press Ctrl+C in the terminal where the script is running\n    to stop the OTEL Collector.\n\n\nLogs and metric reference#\n\nThe following section describes the structure of logs and metrics generated for\nVeCLI.\n\n * A sessionId is included as a common attribute on all logs and metrics.\n\n\nLogs#\n\nLogs are timestamped records of specific events. The following events are logged\nfor VeCLI:\n\n * vecli.config: This event occurs once at startup with the CLI's configuration.\n   \n   * Attributes:\n     * model (string)\n     * embedding_model (string)\n     * sandbox_enabled (boolean)\n     * core_tools_enabled (string)\n     * approval_mode (string)\n     * api_key_enabled (boolean)\n     * vertex_ai_enabled (boolean)\n     * code_assist_enabled (boolean)\n     * log_prompts_enabled (boolean)\n     * file_filtering_respect_git_ignore (boolean)\n     * debug_mode (boolean)\n     * mcp_servers (string)\n\n * vecli.user_prompt: This event occurs when a user submits a prompt.\n   \n   * Attributes:\n     * prompt_length (int)\n     * prompt_id (string)\n     * prompt (string, this attribute is excluded if log_prompts_enabled is\n       configured to be false)\n     * auth_type (string)\n\n * vecli.tool_call: This event occurs for each function call.\n   \n   * Attributes:\n     * function_name\n     * function_args\n     * duration_ms\n     * success (boolean)\n     * decision (string: \"accept\", \"reject\", \"auto_accept\", or \"modify\", if\n       applicable)\n     * error (if applicable)\n     * error_type (if applicable)\n     * metadata (if applicable, dictionary of string -> any)\n\n * vecli.file_operation: This event occurs for each file operation.\n   \n   * Attributes:\n     * tool_name (string)\n     * operation (string: \"create\", \"read\", \"update\")\n     * lines (int, if applicable)\n     * mimetype (string, if applicable)\n     * extension (string, if applicable)\n     * programming_language (string, if applicable)\n     * diff_stat (json string, if applicable): A JSON string with the following\n       members:\n       * ai_added_lines (int)\n       * ai_removed_lines (int)\n       * user_added_lines (int)\n       * user_removed_lines (int)\n\n * vecli.api_request: This event occurs when making a request to volcano Engine\n   API.\n   \n   * Attributes:\n     * model\n     * request_text (if applicable)\n\n * vecli.api_error: This event occurs if the API request fails.\n   \n   * Attributes:\n     * model\n     * error\n     * error_type\n     * status_code\n     * duration_ms\n     * auth_type\n\n * vecli.api_response: This event occurs upon receiving a response from vecli\n   API.\n   \n   * Attributes:\n     * model\n     * status_code\n     * duration_ms\n     * error (optional)\n     * input_token_count\n     * output_token_count\n     * cached_content_token_count\n     * thoughts_token_count\n     * tool_token_count\n     * response_text (if applicable)\n     * auth_type\n\n * vecli.malformed_json_response: This event occurs when a generateJson response\n   from vecli API cannot be parsed as a json.\n   \n   * Attributes:\n     * model\n\n * vecli.flash_fallback: This event occurs when VeCLI switches to flash as\n   fallback.\n   \n   * Attributes:\n     * auth_type\n\n * vecli.slash_command: This event occurs when a user executes a slash command.\n   \n   * Attributes:\n     * command (string)\n     * subcommand (string, if applicable)\n\n\nMetrics#\n\nMetrics are numerical measurements of behavior over time. The following metrics\nare collected for VeCLI:\n\n * vecli.session.count (Counter, Int): Incremented once per CLI startup.\n\n * vecli.tool.call.count (Counter, Int): Counts tool calls.\n   \n   * Attributes:\n     * function_name\n     * success (boolean)\n     * decision (string: \"accept\", \"reject\", or \"modify\", if applicable)\n     * tool_type (string: \"mcp\", or \"native\", if applicable)\n\n * vecli.tool.call.latency (Histogram, ms): Measures tool call latency.\n   \n   * Attributes:\n     * function_name\n     * decision (string: \"accept\", \"reject\", or \"modify\", if applicable)\n\n * vecli.api.request.count (Counter, Int): Counts all API requests.\n   \n   * Attributes:\n     * model\n     * status_code\n     * error_type (if applicable)\n\n * vecli.api.request.latency (Histogram, ms): Measures API request latency.\n   \n   * Attributes:\n     * model\n\n * gemini_cli.token.usage (Counter, Int): Counts the number of tokens used.\n   \n   * Attributes:\n     * model\n     * type (string: \"input\", \"output\", \"thought\", \"cache\", or \"tool\")\n\n * gemini_cli.file.operation.count (Counter, Int): Counts file operations.\n   \n   * Attributes:\n     * operation (string: \"create\", \"read\", \"update\"): The type of file\n       operation.\n     * lines (Int, if applicable): Number of lines in the file.\n     * mimetype (string, if applicable): Mimetype of the file.\n     * extension (string, if applicable): File extension of the file.\n     * model_added_lines (Int, if applicable): Number of lines added/changed by\n       the model.\n     * model_removed_lines (Int, if applicable): Number of lines removed/changed\n       by the model.\n     * user_added_lines (Int, if applicable): Number of lines added/changed by\n       user in AI proposed changes.\n     * user_removed_lines (Int, if applicable): Number of lines removed/changed\n       by user in AI proposed changes.\n     * programming_language (string, if applicable): The programming language of\n       the file.\n\n * gemini_cli.chat_compression (Counter, Int): Counts chat compression\n   operations\n   \n   * Attributes:\n     * tokens_before: (Int): Number of tokens in context prior to compression\n     * tokens_after: (Int): Number of tokens in context after compression","routePath":"/en/telemetry","lang":"","toc":[{"text":"Enabling telemetry","id":"enabling-telemetry","depth":2,"charIndex":328},{"text":"Order of precedence","id":"order-of-precedence","depth":3,"charIndex":553},{"text":"Example settings","id":"example-settings","depth":3,"charIndex":2037},{"text":"Exporting to a file","id":"exporting-to-a-file","depth":3,"charIndex":2228},{"text":"Running an OTEL Collector","id":"running-an-otel-collector","depth":2,"charIndex":2467},{"text":"Local","id":"local","depth":3,"charIndex":2915},{"text":"Volcano Engine","id":"volcano-engine","depth":3,"charIndex":4209},{"text":"Logs and metric reference","id":"logs-and-metric-reference","depth":2,"charIndex":5774},{"text":"Logs","id":"logs","depth":3,"charIndex":5966},{"text":"Metrics","id":"metrics","depth":3,"charIndex":8974}],"domain":"","frontmatter":{},"version":""},{"id":44,"title":"VeCLI file system tools","content":"#\n\nThe VeCLI provides a comprehensive suite of tools for interacting with the local\nfile system. These tools allow the Volcano Engine model to read from, write to,\nlist, search, and modify files and directories, all under your control and\ntypically with confirmation for sensitive operations.\n\nNote: All file system tools operate within a rootDirectory (usually the current\nworking directory where you launched the CLI) for security. Paths that you\nprovide to these tools are generally expected to be absolute or are resolved\nrelative to this root directory.\n\n\n1. list_directory (ReadFolder)#\n\nlist_directory lists the names of files and subdirectories directly within a\nspecified directory path. It can optionally ignore entries matching provided\nglob patterns.\n\n * Tool name: list_directory\n * Display name: ReadFolder\n * File: ls.ts\n * Parameters:\n   * path (string, required): The absolute path to the directory to list.\n   * ignore (array of strings, optional): A list of glob patterns to exclude\n     from the listing (e.g., [\"*.log\", \".git\"]).\n   * respect_git_ignore (boolean, optional): Whether to respect .gitignore\n     patterns when listing files. Defaults to true.\n * Behavior:\n   * Returns a list of file and directory names.\n   * Indicates whether each entry is a directory.\n   * Sorts entries with directories first, then alphabetically.\n * Output (llmContent): A string like: Directory listing for\n   /path/to/your/folder:\\n[DIR] subfolder1\\nfile1.txt\\nfile2.png\n * Confirmation: No.\n\n\n2. read_file (ReadFile)#\n\nread_file reads and returns the content of a specified file. This tool handles\ntext, images (PNG, JPG, GIF, WEBP, SVG, BMP), and PDF files. For text files, it\ncan read specific line ranges. Other binary file types are generally skipped.\n\n * Tool name: read_file\n * Display name: ReadFile\n * File: read-file.ts\n * Parameters:\n   * path (string, required): The absolute path to the file to read.\n   * offset (number, optional): For text files, the 0-based line number to start\n     reading from. Requires limit to be set.\n   * limit (number, optional): For text files, the maximum number of lines to\n     read. If omitted, reads a default maximum (e.g., 2000 lines) or the entire\n     file if feasible.\n * Behavior:\n   * For text files: Returns the content. If offset and limit are used, returns\n     only that slice of lines. Indicates if content was truncated due to line\n     limits or line length limits.\n   * For image and PDF files: Returns the file content as a base64-encoded data\n     structure suitable for model consumption.\n   * For other binary files: Attempts to identify and skip them, returning a\n     message indicating it's a generic binary file.\n * Output: (llmContent):\n   * For text files: The file content, potentially prefixed with a truncation\n     message (e.g., [File content truncated: showing lines 1-100 of 500 total\n     lines...]\\nActual file content...).\n   * For image/PDF files: An object containing inlineData with mimeType and\n     base64 data (e.g., { inlineData: { mimeType: 'image/png', data:\n     'base64encodedstring' } }).\n   * For other binary files: A message like Cannot display content of binary\n     file: /path/to/data.bin.\n * Confirmation: No.\n\n\n3. write_file (WriteFile)#\n\nwrite_file writes content to a specified file. If the file exists, it will be\noverwritten. If the file doesn't exist, it (and any necessary parent\ndirectories) will be created.\n\n * Tool name: write_file\n * Display name: WriteFile\n * File: write-file.ts\n * Parameters:\n   * file_path (string, required): The absolute path to the file to write to.\n   * content (string, required): The content to write into the file.\n * Behavior:\n   * Writes the provided content to the file_path.\n   * Creates parent directories if they don't exist.\n * Output (llmContent): A success message, e.g., Successfully overwrote file:\n   /path/to/your/file.txt or Successfully created and wrote to new file:\n   /path/to/new/file.txt.\n * Confirmation: Yes. Shows a diff of changes and asks for user approval before\n   writing.\n\n\n4. glob (FindFiles)#\n\nglob finds files matching specific glob patterns (e.g., src/**/*.ts, *.md),\nreturning absolute paths sorted by modification time (newest first).\n\n * Tool name: glob\n * Display name: FindFiles\n * File: glob.ts\n * Parameters:\n   * pattern (string, required): The glob pattern to match against (e.g.,\n     \"*.py\", \"src/**/*.js\").\n   * path (string, optional): The absolute path to the directory to search\n     within. If omitted, searches the tool's root directory.\n   * case_sensitive (boolean, optional): Whether the search should be\n     case-sensitive. Defaults to false.\n   * respect_git_ignore (boolean, optional): Whether to respect .gitignore\n     patterns when finding files. Defaults to true.\n * Behavior:\n   * Searches for files matching the glob pattern within the specified\n     directory.\n   * Returns a list of absolute paths, sorted with the most recently modified\n     files first.\n   * Ignores common nuisance directories like node_modules and .git by default.\n * Output (llmContent): A message like: Found 5 file(s) matching \"*.ts\" within\n   src, sorted by modification time (newest\n   first):\\nsrc/file1.ts\\nsrc/subdir/file2.ts...\n * Confirmation: No.\n\n\n5. search_file_content (SearchText)#\n\nsearch_file_content searches for a regular expression pattern within the content\nof files in a specified directory. Can filter files by a glob pattern. Returns\nthe lines containing matches, along with their file paths and line numbers.\n\n * Tool name: search_file_content\n * Display name: SearchText\n * File: grep.ts\n * Parameters:\n   * pattern (string, required): The regular expression (regex) to search for\n     (e.g., \"function\\s+myFunction\").\n   * path (string, optional): The absolute path to the directory to search\n     within. Defaults to the current working directory.\n   * include (string, optional): A glob pattern to filter which files are\n     searched (e.g., \"*.js\", \"src/**/*.{ts,tsx}\"). If omitted, searches most\n     files (respecting common ignores).\n * Behavior:\n   * Uses git grep if available in a Git repository for speed; otherwise, falls\n     back to system grep or a JavaScript-based search.\n   * Returns a list of matching lines, each prefixed with its file path\n     (relative to the search directory) and line number.\n * Output (llmContent): A formatted string of matches, e.g.:\n   \n   \n\n * Confirmation: No.\n\n\n6. replace (Edit)#\n\nreplace replaces text within a file. By default, replaces a single occurrence,\nbut can replace multiple occurrences when expected_replacements is specified.\nThis tool is designed for precise, targeted changes and requires significant\ncontext around the old_string to ensure it modifies the correct location.\n\n * Tool name: replace\n\n * Display name: Edit\n\n * File: edit.ts\n\n * Parameters:\n   \n   * file_path (string, required): The absolute path to the file to modify.\n   \n   * old_string (string, required): The exact literal text to replace.\n     \n     CRITICAL: This string must uniquely identify the single instance to change.\n     It should include at least 3 lines of context before and after the target\n     text, matching whitespace and indentation precisely. If old_string is\n     empty, the tool attempts to create a new file at file_path with new_string\n     as content.\n   \n   * new_string (string, required): The exact literal text to replace old_string\n     with.\n   \n   * expected_replacements (number, optional): The number of occurrences to\n     replace. Defaults to 1.\n\n * Behavior:\n   \n   * If old_string is empty and file_path does not exist, creates a new file\n     with new_string as content.\n   * If old_string is provided, it reads the file_path and attempts to find\n     exactly one occurrence of old_string.\n   * If one occurrence is found, it replaces it with new_string.\n   * Enhanced Reliability (Multi-Stage Edit Correction): To significantly\n     improve the success rate of edits, especially when the model-provided\n     old_string might not be perfectly precise, the tool incorporates a\n     multi-stage edit correction mechanism.\n     * If the initial old_string isn't found or matches multiple locations, the\n       tool can leverage the Volcano Engine model to iteratively refine\n       old_string (and potentially new_string).\n     * This self-correction process attempts to identify the unique segment the\n       model intended to modify, making the replace operation more robust even\n       with slightly imperfect initial context.\n\n * Failure conditions: Despite the correction mechanism, the tool will fail if:\n   \n   * file_path is not absolute or is outside the root directory.\n   * old_string is not empty, but the file_path does not exist.\n   * old_string is empty, but the file_path already exists.\n   * old_string is not found in the file after attempts to correct it.\n   * old_string is found multiple times, and the self-correction mechanism\n     cannot resolve it to a single, unambiguous match.\n\n * Output (llmContent):\n   \n   * On success: Successfully modified file: /path/to/file.txt (1 replacements).\n     or Created new file: /path/to/new_file.txt with provided content.\n   * On failure: An error message explaining the reason (e.g., Failed to edit, 0\n     occurrences found..., Failed to edit, expected 1 occurrences but found\n     2...).\n\n * Confirmation: Yes. Shows a diff of the proposed changes and asks for user\n   approval before writing to the file.\n\nThese file system tools provide a foundation for the VeCLI to understand and\ninteract with your local project context.","routePath":"/en/tools/file-system","lang":"","toc":[{"text":"1. `list_directory` (ReadFolder)","id":"1-list_directory-readfolder","depth":2,"charIndex":-1},{"text":"2. `read_file` (ReadFile)","id":"2-read_file-readfile","depth":2,"charIndex":-1},{"text":"3. `write_file` (WriteFile)","id":"3-write_file-writefile","depth":2,"charIndex":-1},{"text":"4. `glob` (FindFiles)","id":"4-glob-findfiles","depth":2,"charIndex":-1},{"text":"5. `search_file_content` (SearchText)","id":"5-search_file_content-searchtext","depth":2,"charIndex":-1},{"text":"6. `replace` (Edit)","id":"6-replace-edit","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":45,"title":"VeCLI tools","content":"#\n\nThe VeCLI includes built-in tools that the Volcano Engine model uses to interact\nwith your local environment, access information, and perform actions. These\ntools enhance the CLI's capabilities, enabling it to go beyond text generation\nand assist with a wide range of tasks.\n\n\nOverview of VeCLI tools#\n\nIn the context of the VeCLI, tools are specific functions or modules that the\nVolcano Engine model can request to be executed. For example, if you ask Volcano\nEngine to \"Summarize the contents of my_document.txt,\" the model will likely\nidentify the need to read that file and will request the execution of the\nread_file tool.\n\nThe core component (packages/core) manages these tools, presents their\ndefinitions (schemas) to the Volcano Engine model, executes them when requested,\nand returns the results to the model for further processing into a user-facing\nresponse.\n\nThese tools provide the following capabilities:\n\n * Access local information: Tools allow Volcano Engine to access your local\n   file system, read file contents, list directories, etc.\n * Execute commands: With tools like run_shell_command, Volcano Engine can run\n   shell commands (with appropriate safety measures and user confirmation).\n * Interact with the web: Tools can fetch content from URLs.\n * Take actions: Tools can modify files, write new files, or perform other\n   actions on your system (again, typically with safeguards).\n * Ground responses: By using tools to fetch real-time or specific local data,\n   Volcano Engine's responses can be more accurate, relevant, and grounded in\n   your actual context.\n\n\nHow to use VeCLI tools#\n\nTo use VeCLI tools, provide a prompt to the VeCLI. The process works as follows:\n\n 1. You provide a prompt to the VeCLI.\n 2. The CLI sends the prompt to the core.\n 3. The core, along with your prompt and conversation history, sends a list of\n    available tools and their descriptions/schemas to the Volcano Engine API.\n 4. The Volcano Engine model analyzes your request. If it determines that a tool\n    is needed, its response will include a request to execute a specific tool\n    with certain parameters.\n 5. The core receives this tool request, validates it, and (often after user\n    confirmation for sensitive operations) executes the tool.\n 6. The output from the tool is sent back to the Volcano Engine model.\n 7. The Volcano Engine model uses the tool's output to formulate its final\n    answer, which is then sent back through the core to the CLI and displayed to\n    you.\n\nYou will typically see messages in the CLI indicating when a tool is being\ncalled and whether it succeeded or failed.\n\n\nSecurity and confirmation#\n\nMany tools, especially those that can modify your file system or execute\ncommands (write_file, edit, run_shell_command), are designed with safety in\nmind. The VeCLI will typically:\n\n * Require confirmation: Prompt you before executing potentially sensitive\n   operations, showing you what action is about to be taken.\n * Utilize sandboxing: All tools are subject to restrictions enforced by\n   sandboxing (see Sandboxing in the VeCLI). This means that when operating in a\n   sandbox, any tools (including MCP servers) you wish to use must be available\n   inside the sandbox environment. For example, to run an MCP server through\n   npx, the npx executable must be installed within the sandbox's Docker image\n   or be available in the sandbox-exec environment.\n\nIt's important to always review confirmation prompts carefully before allowing a\ntool to proceed.\n\n\nLearn more about VeCLI's tools#\n\nVeCLI's built-in tools can be broadly categorized as follows:\n\n * File System Tools: For interacting with files and directories (reading,\n   writing, listing, searching, etc.).\n * Shell Tool (run_shell_command): For executing shell commands.\n * Web Fetch Tool (web_fetch): For retrieving content from URLs.\n * Web Search Tool (web_search): For searching the web.\n * Multi-File Read Tool (read_many_files): A specialized tool for reading\n   content from multiple files or directories, often used by the @ command.\n * Memory Tool (save_memory): For saving and recalling information across\n   sessions.\n\nAdditionally, these tools incorporate:\n\n * MCP servers: MCP servers act as a bridge between the Volcano Engine model and\n   your local environment or other services like APIs.\n * Sandboxing: Sandboxing isolates the model and its changes from your\n   environment to reduce potential risk.","routePath":"/en/tools/","lang":"","toc":[{"text":"Overview of VeCLI tools","id":"overview-of-vecli-tools","depth":2,"charIndex":279},{"text":"How to use VeCLI tools","id":"how-to-use-vecli-tools","depth":2,"charIndex":1595},{"text":"Security and confirmation","id":"security-and-confirmation","depth":2,"charIndex":2624},{"text":"Learn more about VeCLI's tools","id":"learn-more-about-veclis-tools","depth":2,"charIndex":3513}],"domain":"","frontmatter":{},"version":""},{"id":46,"title":"MCP servers with the VeCLI","content":"#\n\nThis document provides a guide to configuring and using Model Context Protocol\n(MCP) servers with the VeCLI.\n\n\nWhat is an MCP server?#\n\nAn MCP server is an application that exposes tools and resources to the VeCLI\nthrough the Model Context Protocol, allowing it to interact with external\nsystems and data sources. MCP servers act as a bridge between the Gemini model\nand your local environment or other services like APIs.\n\nAn MCP server enables the VeCLI to:\n\n * Discover tools: List available tools, their descriptions, and parameters\n   through standardized schema definitions.\n * Execute tools: Call specific tools with defined arguments and receive\n   structured responses.\n * Access resources: Read data from specific resources (though the VeCLI\n   primarily focuses on tool execution).\n\nWith an MCP server, you can extend the VeCLI's capabilities to perform actions\nbeyond its built-in features, such as interacting with databases, APIs, custom\nscripts, or specialized workflows.\n\n\nCore Integration Architecture#\n\nThe VeCLI integrates with MCP servers through a sophisticated discovery and\nexecution system built into the core package (packages/core/src/tools/):\n\n\nDiscovery Layer (mcp-client.ts)#\n\nThe discovery process is orchestrated by discoverMcpTools(), which:\n\n 1. Iterates through configured servers from your settings.json mcpServers\n    configuration\n 2. Establishes connections using appropriate transport mechanisms (Stdio, SSE,\n    or Streamable HTTP)\n 3. Fetches tool definitions from each server using the MCP protocol\n 4. Sanitizes and validates tool schemas for compatibility with the Gemini API\n 5. Registers tools in the global tool registry with conflict resolution\n\n\nExecution Layer (mcp-tool.ts)#\n\nEach discovered MCP tool is wrapped in a DiscoveredMCPTool instance that:\n\n * Handles confirmation logic based on server trust settings and user\n   preferences\n * Manages tool execution by calling the MCP server with proper parameters\n * Processes responses for both the LLM context and user display\n * Maintains connection state and handles timeouts\n\n\nTransport Mechanisms#\n\nThe VeCLI supports three MCP transport types:\n\n * Stdio Transport: Spawns a subprocess and communicates via stdin/stdout\n * SSE Transport: Connects to Server-Sent Events endpoints\n * Streamable HTTP Transport: Uses HTTP streaming for communication\n\n\nHow to set up your MCP server#\n\nThe VeCLI uses the mcpServers configuration in your settings.json file to locate\nand connect to MCP servers. This configuration supports multiple servers with\ndifferent transport mechanisms.\n\n\nConfigure the MCP server in settings.json#\n\nYou can configure MCP servers in your settings.json file in two main ways:\nthrough the top-level mcpServers object for specific server definitions, and\nthrough the mcp object for global settings that control server discovery and\nexecution.\n\nGlobal MCP Settings (mcp)#\n\nThe mcp object in your settings.json allows you to define global rules for all\nMCP servers.\n\n * mcp.serverCommand (string): A global command to start an MCP server.\n * mcp.allowed (array of strings): A list of MCP server names to allow. If this\n   is set, only servers from this list (matching the keys in the mcpServers\n   object) will be connected to.\n * mcp.excluded (array of strings): A list of MCP server names to exclude.\n   Servers in this list will not be connected to.\n\nExample:\n\n\n\nServer-Specific Configuration (mcpServers)#\n\nThe mcpServers object is where you define each individual MCP server you want\nthe CLI to connect to.\n\n\nConfiguration Structure#\n\nAdd an mcpServers object to your settings.json file:\n\n\n\n\nConfiguration Properties#\n\nEach server configuration supports the following properties:\n\nRequired (one of the following)#\n\n * command (string): Path to the executable for Stdio transport\n * url (string): SSE endpoint URL (e.g., \"http://localhost:8080/sse\")\n * httpUrl (string): HTTP streaming endpoint URL\n\nOptional#\n\n * args (string[]): Command-line arguments for Stdio transport\n * headers (object): Custom HTTP headers when using url or httpUrl\n * env (object): Environment variables for the server process. Values can\n   reference environment variables using $VAR_NAME or ${VAR_NAME} syntax\n * cwd (string): Working directory for Stdio transport\n * timeout (number): Request timeout in milliseconds (default: 600,000ms = 10\n   minutes)\n * trust (boolean): When true, bypasses all tool call confirmations for this\n   server (default: false)\n * includeTools (string[]): List of tool names to include from this MCP server.\n   When specified, only the tools listed here will be available from this server\n   (allowlist behavior). If not specified, all tools from the server are enabled\n   by default.\n * excludeTools (string[]): List of tool names to exclude from this MCP server.\n   Tools listed here will not be available to the model, even if they are\n   exposed by the server. Note: excludeTools takes precedence over includeTools\n   - if a tool is in both lists, it will be excluded.\n\n\nOAuth Support for Remote MCP Servers#\n\nThe VeCLI supports OAuth 2.0 authentication for remote MCP servers using SSE or\nHTTP transports. This enables secure access to MCP servers that require\nauthentication.\n\nAutomatic OAuth Discovery#\n\nFor servers that support OAuth discovery, you can omit the OAuth configuration\nand let the CLI discover it automatically:\n\n\n\nThe CLI will automatically:\n\n * Detect when a server requires OAuth authentication (401 responses)\n * Discover OAuth endpoints from server metadata\n * Perform dynamic client registration if supported\n * Handle the OAuth flow and token management\n\nAuthentication Flow#\n\nWhen connecting to an OAuth-enabled server:\n\n 1. Initial connection attempt fails with 401 Unauthorized\n 2. OAuth discovery finds authorization and token endpoints\n 3. Browser opens for user authentication (requires local browser access)\n 4. Authorization code is exchanged for access tokens\n 5. Tokens are stored securely for future use\n 6. Connection retry succeeds with valid tokens\n\nBrowser Redirect Requirements#\n\nImportant: OAuth authentication requires that your local machine can:\n\n * Open a web browser for authentication\n * Receive redirects on http://localhost:7777/oauth/callback\n\nThis feature will not work in:\n\n * Headless environments without browser access\n * Remote SSH sessions without X11 forwarding\n * Containerized environments without browser support\n\nManaging OAuth Authentication#\n\nUse the /mcp auth command to manage OAuth authentication:\n\n\n\nOAuth Configuration Properties#\n\n * enabled (boolean): Enable OAuth for this server\n * clientId (string): OAuth client identifier (optional with dynamic\n   registration)\n * clientSecret (string): OAuth client secret (optional for public clients)\n * authorizationUrl (string): OAuth authorization endpoint (auto-discovered if\n   omitted)\n * tokenUrl (string): OAuth token endpoint (auto-discovered if omitted)\n * scopes (string[]): Required OAuth scopes\n * redirectUri (string): Custom redirect URI (defaults to\n   http://localhost:7777/oauth/callback)\n * tokenParamName (string): Query parameter name for tokens in SSE URLs\n * audiences (string[]): Audiences the token is valid for\n\nToken Management#\n\nOAuth tokens are automatically:\n\n * Stored securely in ~/.ve/mcp-oauth-tokens.json\n * Refreshed when expired (if refresh tokens are available)\n * Validated before each connection attempt\n * Cleaned up when invalid or expired\n\nAuthentication Provider Type#\n\nYou can specify the authentication provider type using the authProviderType\nproperty:\n\n * authProviderType (string): Specifies the authentication provider. Can be one\n   of the following:\n   * dynamic_discovery (default): The CLI will automatically discover the OAuth\n     configuration from the server.\n   * google_credentials: The CLI will use the Google Application Default\n     Credentials (ADC) to authenticate with the server. When using this\n     provider, you must specify the required scopes.\n\n\n\n\nExample Configurations#\n\nPython MCP Server (Stdio)#\n\n\n\nNode.js MCP Server (Stdio)#\n\n\n\nDocker-based MCP Server#\n\n\n\nHTTP-based MCP Server#\n\n\n\nHTTP-based MCP Server with Custom Headers#\n\n\n\nMCP Server with Tool Filtering#\n\n\n\n\nDiscovery Process Deep Dive#\n\nWhen the VeCLI starts, it performs MCP server discovery through the following\ndetailed process:\n\n\n1. Server Iteration and Connection#\n\nFor each configured server in mcpServers:\n\n 1. Status tracking begins: Server status is set to CONNECTING\n 2. Transport selection: Based on configuration properties:\n    * httpUrl → StreamableHTTPClientTransport\n    * url → SSEClientTransport\n    * command → StdioClientTransport\n 3. Connection establishment: The MCP client attempts to connect with the\n    configured timeout\n 4. Error handling: Connection failures are logged and the server status is set\n    to DISCONNECTED\n\n\n2. Tool Discovery#\n\nUpon successful connection:\n\n 1. Tool listing: The client calls the MCP server's tool listing endpoint\n 2. Schema validation: Each tool's function declaration is validated\n 3. Tool filtering: Tools are filtered based on includeTools and excludeTools\n    configuration\n 4. Name sanitization: Tool names are cleaned to meet Gemini API requirements:\n    * Invalid characters (non-alphanumeric, underscore, dot, hyphen) are\n      replaced with underscores\n    * Names longer than 63 characters are truncated with middle replacement\n      (___)\n\n\n3. Conflict Resolution#\n\nWhen multiple servers expose tools with the same name:\n\n 1. First registration wins: The first server to register a tool name gets the\n    unprefixed name\n 2. Automatic prefixing: Subsequent servers get prefixed names:\n    serverName__toolName\n 3. Registry tracking: The tool registry maintains mappings between server names\n    and their tools\n\n\n4. Schema Processing#\n\nTool parameter schemas undergo sanitization for Gemini API compatibility:\n\n * $schema properties are removed\n * additionalProperties are stripped\n * anyOf with default have their default values removed (Vertex AI\n   compatibility)\n * Recursive processing applies to nested schemas\n\n\n5. Connection Management#\n\nAfter discovery:\n\n * Persistent connections: Servers that successfully register tools maintain\n   their connections\n * Cleanup: Servers that provide no usable tools have their connections closed\n * Status updates: Final server statuses are set to CONNECTED or DISCONNECTED\n\n\nTool Execution Flow#\n\nWhen the Gemini model decides to use an MCP tool, the following execution flow\noccurs:\n\n\n1. Tool Invocation#\n\nThe model generates a FunctionCall with:\n\n * Tool name: The registered name (potentially prefixed)\n * Arguments: JSON object matching the tool's parameter schema\n\n\n2. Confirmation Process#\n\nEach DiscoveredMCPTool implements sophisticated confirmation logic:\n\nTrust-based Bypass#\n\n\n\nDynamic Allow-listing#\n\nThe system maintains internal allow-lists for:\n\n * Server-level: serverName → All tools from this server are trusted\n * Tool-level: serverName.toolName → This specific tool is trusted\n\nUser Choice Handling#\n\nWhen confirmation is required, users can choose:\n\n * Proceed once: Execute this time only\n * Always allow this tool: Add to tool-level allow-list\n * Always allow this server: Add to server-level allow-list\n * Cancel: Abort execution\n\n\n3. Execution#\n\nUpon confirmation (or trust bypass):\n\n 1. Parameter preparation: Arguments are validated against the tool's schema\n\n 2. MCP call: The underlying CallableTool invokes the server with:\n    \n    \n\n 3. Response processing: Results are formatted for both LLM context and user\n    display\n\n\n4. Response Handling#\n\nThe execution result contains:\n\n * llmContent: Raw response parts for the language model's context\n * returnDisplay: Formatted output for user display (often JSON in markdown code\n   blocks)\n\n\nHow to interact with your MCP server#\n\n\nUsing the /mcp Command#\n\nThe /mcp command provides comprehensive information about your MCP server setup:\n\n\n\nThis displays:\n\n * Server list: All configured MCP servers\n * Connection status: CONNECTED, CONNECTING, or DISCONNECTED\n * Server details: Configuration summary (excluding sensitive data)\n * Available tools: List of tools from each server with descriptions\n * Discovery state: Overall discovery process status\n\n\nExample /mcp Output#\n\n\n\n\nTool Usage#\n\nOnce discovered, MCP tools are available to the Gemini model like built-in\ntools. The model will automatically:\n\n 1. Select appropriate tools based on your requests\n 2. Present confirmation dialogs (unless the server is trusted)\n 3. Execute tools with proper parameters\n 4. Display results in a user-friendly format\n\n\nStatus Monitoring and Troubleshooting#\n\n\nConnection States#\n\nThe MCP integration tracks several states:\n\nServer Status (MCPServerStatus)#\n\n * DISCONNECTED: Server is not connected or has errors\n * CONNECTING: Connection attempt in progress\n * CONNECTED: Server is connected and ready\n\nDiscovery State (MCPDiscoveryState)#\n\n * NOT_STARTED: Discovery hasn't begun\n * IN_PROGRESS: Currently discovering servers\n * COMPLETED: Discovery finished (with or without errors)\n\n\nCommon Issues and Solutions#\n\nServer Won't Connect#\n\nSymptoms: Server shows DISCONNECTED status\n\nTroubleshooting:\n\n 1. Check configuration: Verify command, args, and cwd are correct\n 2. Test manually: Run the server command directly to ensure it works\n 3. Check dependencies: Ensure all required packages are installed\n 4. Review logs: Look for error messages in the CLI output\n 5. Verify permissions: Ensure the CLI can execute the server command\n\nNo Tools Discovered#\n\nSymptoms: Server connects but no tools are available\n\nTroubleshooting:\n\n 1. Verify tool registration: Ensure your server actually registers tools\n 2. Check MCP protocol: Confirm your server implements the MCP tool listing\n    correctly\n 3. Review server logs: Check stderr output for server-side errors\n 4. Test tool listing: Manually test your server's tool discovery endpoint\n\nTools Not Executing#\n\nSymptoms: Tools are discovered but fail during execution\n\nTroubleshooting:\n\n 1. Parameter validation: Ensure your tool accepts the expected parameters\n 2. Schema compatibility: Verify your input schemas are valid JSON Schema\n 3. Error handling: Check if your tool is throwing unhandled exceptions\n 4. Timeout issues: Consider increasing the timeout setting\n\nSandbox Compatibility#\n\nSymptoms: MCP servers fail when sandboxing is enabled\n\nSolutions:\n\n 1. Docker-based servers: Use Docker containers that include all dependencies\n 2. Path accessibility: Ensure server executables are available in the sandbox\n 3. Network access: Configure sandbox to allow necessary network connections\n 4. Environment variables: Verify required environment variables are passed\n    through\n\n\nDebugging Tips#\n\n 1. Enable debug mode: Run the CLI with --debug for verbose output\n 2. Check stderr: MCP server stderr is captured and logged (INFO messages\n    filtered)\n 3. Test isolation: Test your MCP server independently before integrating\n 4. Incremental setup: Start with simple tools before adding complex\n    functionality\n 5. Use /mcp frequently: Monitor server status during development\n\n\nImportant Notes#\n\n\nSecurity Considerations#\n\n * Trust settings: The trust option bypasses all confirmation dialogs. Use\n   cautiously and only for servers you completely control\n * Access tokens: Be security-aware when configuring environment variables\n   containing API keys or tokens\n * Sandbox compatibility: When using sandboxing, ensure MCP servers are\n   available within the sandbox environment\n * Private data: Using broadly scoped personal access tokens can lead to\n   information leakage between repositories\n\n\nPerformance and Resource Management#\n\n * Connection persistence: The CLI maintains persistent connections to servers\n   that successfully register tools\n * Automatic cleanup: Connections to servers providing no tools are\n   automatically closed\n * Timeout management: Configure appropriate timeouts based on your server's\n   response characteristics\n * Resource monitoring: MCP servers run as separate processes and consume system\n   resources\n\n\nSchema Compatibility#\n\n * Property stripping: The system automatically removes certain schema\n   properties ($schema, additionalProperties) for Gemini API compatibility\n * Name sanitization: Tool names are automatically sanitized to meet API\n   requirements\n * Conflict resolution: Tool name conflicts between servers are resolved through\n   automatic prefixing\n\nThis comprehensive integration makes MCP servers a powerful way to extend the\nVeCLI's capabilities while maintaining security, reliability, and ease of use.\n\n\nReturning Rich Content from Tools#\n\nMCP tools are not limited to returning simple text. You can return rich,\nmulti-part content, including text, images, audio, and other binary data in a\nsingle tool response. This allows you to build powerful tools that can provide\ndiverse information to the model in a single turn.\n\nAll data returned from the tool is processed and sent to the model as context\nfor its next generation, enabling it to reason about or summarize the provided\ninformation.\n\n\nHow It Works#\n\nTo return rich content, your tool's response must adhere to the MCP\nspecification for a CallToolResult. The content field of the result should be an\narray of ContentBlock objects. The VeCLI will correctly process this array,\nseparating text from binary data and packaging it for the model.\n\nYou can mix and match different content block types in the content array. The\nsupported block types include:\n\n * text\n * image\n * audio\n * resource (embedded content)\n * resource_link\n\n\nExample: Returning Text and an Image#\n\nHere is an example of a valid JSON response from an MCP tool that returns both a\ntext description and an image:\n\n\n\nWhen the VeCLI receives this response, it will:\n\n 1. Extract all the text and combine it into a single functionResponse part for\n    the model.\n 2. Present the image data as a separate inlineData part.\n 3. Provide a clean, user-friendly summary in the CLI, indicating that both text\n    and an image were received.\n\nThis enables you to build sophisticated tools that can provide rich, multi-modal\ncontext to the Gemini model.\n\n\nMCP Prompts as Slash Commands#\n\nIn addition to tools, MCP servers can expose predefined prompts that can be\nexecuted as slash commands within the VeCLI. This allows you to create shortcuts\nfor common or complex queries that can be easily invoked by name.\n\n\nDefining Prompts on the Server#\n\nHere's a small example of a stdio MCP server that defines prompts:\n\n\n\nThis can be included in settings.json under mcpServers with:\n\n\n\n\nInvoking Prompts#\n\nOnce a prompt is discovered, you can invoke it using its name as a slash\ncommand. The CLI will automatically handle parsing arguments.\n\n\n\nor, using positional arguments:\n\n\n\nWhen you run this command, the VeCLI executes the prompts/get method on the MCP\nserver with the provided arguments. The server is responsible for substituting\nthe arguments into the prompt template and returning the final prompt text. The\nCLI then sends this prompt to the model for execution. This provides a\nconvenient way to automate and share common workflows.\n\n\nManaging MCP Servers with gemini mcp#\n\nWhile you can always configure MCP servers by manually editing your\nsettings.json file, the VeCLI provides a convenient set of commands to manage\nyour server configurations programmatically. These commands streamline the\nprocess of adding, listing, and removing MCP servers without needing to directly\nedit JSON files.\n\n\nAdding a Server (gemini mcp add)#\n\nThe add command configures a new MCP server in your settings.json. Based on the\nscope (-s, --scope), it will be added to either the user config\n~/.ve/settings.json or the project config .ve/settings.json file.\n\nCommand:\n\n\n\n * <name>: A unique name for the server.\n * <commandOrUrl>: The command to execute (for stdio) or the URL (for http/sse).\n * [args...]: Optional arguments for a stdio command.\n\nOptions (Flags):\n\n * -s, --scope: Configuration scope (user or project). [default: \"project\"]\n * -t, --transport: Transport type (stdio, sse, http). [default: \"stdio\"]\n * -e, --env: Set environment variables (e.g. -e KEY=value).\n * -H, --header: Set HTTP headers for SSE and HTTP transports (e.g. -H\n   \"X-Api-Key: abc123\" -H \"Authorization: Bearer abc123\").\n * --timeout: Set connection timeout in milliseconds.\n * --trust: Trust the server (bypass all tool call confirmation prompts).\n * --description: Set the description for the server.\n * --include-tools: A comma-separated list of tools to include.\n * --exclude-tools: A comma-separated list of tools to exclude.\n\nAdding an stdio server#\n\nThis is the default transport for running local servers.\n\n\n\nAdding an HTTP server#\n\nThis transport is for servers that use the streamable HTTP transport.\n\n\n\nAdding an SSE server#\n\nThis transport is for servers that use Server-Sent Events (SSE).\n\n\n\n\nListing Servers (gemini mcp list)#\n\nTo view all MCP servers currently configured, use the list command. It displays\neach server's name, configuration details, and connection status.\n\nCommand:\n\n\n\nExample Output:\n\n\n\n\nRemoving a Server (gemini mcp remove)#\n\nTo delete a server from your configuration, use the remove command with the\nserver's name.\n\nCommand:\n\n\n\nExample:\n\n\n\nThis will find and delete the \"my-server\" entry from the mcpServers object in\nthe appropriate settings.json file based on the scope (-s, --scope).","routePath":"/en/tools/mcp-server","lang":"","toc":[{"text":"What is an MCP server?","id":"what-is-an-mcp-server","depth":2,"charIndex":113},{"text":"Core Integration Architecture","id":"core-integration-architecture","depth":2,"charIndex":991},{"text":"Discovery Layer (`mcp-client.ts`)","id":"discovery-layer-mcp-clientts","depth":3,"charIndex":-1},{"text":"Execution Layer (`mcp-tool.ts`)","id":"execution-layer-mcp-toolts","depth":3,"charIndex":-1},{"text":"Transport Mechanisms","id":"transport-mechanisms","depth":3,"charIndex":2082},{"text":"How to set up your MCP server","id":"how-to-set-up-your-mcp-server","depth":2,"charIndex":2355},{"text":"Configure the MCP server in settings.json","id":"configure-the-mcp-server-in-settingsjson","depth":3,"charIndex":2580},{"text":"Global MCP Settings (`mcp`)","id":"global-mcp-settings-mcp","depth":4,"charIndex":-1},{"text":"Server-Specific Configuration (`mcpServers`)","id":"server-specific-configuration-mcpservers","depth":4,"charIndex":-1},{"text":"Configuration Structure","id":"configuration-structure","depth":3,"charIndex":3533},{"text":"Configuration Properties","id":"configuration-properties","depth":3,"charIndex":3616},{"text":"Required (one of the following)","id":"required-one-of-the-following","depth":4,"charIndex":3705},{"text":"Optional","id":"optional","depth":4,"charIndex":3923},{"text":"OAuth Support for Remote MCP Servers","id":"oauth-support-for-remote-mcp-servers","depth":3,"charIndex":5007},{"text":"Automatic OAuth Discovery","id":"automatic-oauth-discovery","depth":4,"charIndex":5215},{"text":"Authentication Flow","id":"authentication-flow","depth":4,"charIndex":5615},{"text":"Browser Redirect Requirements","id":"browser-redirect-requirements","depth":4,"charIndex":6024},{"text":"Managing OAuth Authentication","id":"managing-oauth-authentication","depth":4,"charIndex":6411},{"text":"OAuth Configuration Properties","id":"oauth-configuration-properties","depth":4,"charIndex":6504},{"text":"Token Management","id":"token-management","depth":4,"charIndex":7187},{"text":"Authentication Provider Type","id":"authentication-provider-type","depth":4,"charIndex":7432},{"text":"Example Configurations","id":"example-configurations","depth":3,"charIndex":7969},{"text":"Python MCP Server (Stdio)","id":"python-mcp-server-stdio","depth":4,"charIndex":7994},{"text":"Node.js MCP Server (Stdio)","id":"nodejs-mcp-server-stdio","depth":4,"charIndex":8024},{"text":"Docker-based MCP Server","id":"docker-based-mcp-server","depth":4,"charIndex":8055},{"text":"HTTP-based MCP Server","id":"http-based-mcp-server","depth":4,"charIndex":8083},{"text":"HTTP-based MCP Server with Custom Headers","id":"http-based-mcp-server-with-custom-headers","depth":4,"charIndex":8109},{"text":"MCP Server with Tool Filtering","id":"mcp-server-with-tool-filtering","depth":4,"charIndex":8155},{"text":"Discovery Process Deep Dive","id":"discovery-process-deep-dive","depth":2,"charIndex":8191},{"text":"1. Server Iteration and Connection","id":"1-server-iteration-and-connection","depth":3,"charIndex":8319},{"text":"2. Tool Discovery","id":"2-tool-discovery","depth":3,"charIndex":8835},{"text":"3. Conflict Resolution","id":"3-conflict-resolution","depth":3,"charIndex":9397},{"text":"4. Schema Processing","id":"4-schema-processing","depth":3,"charIndex":9769},{"text":"5. Connection Management","id":"5-connection-management","depth":3,"charIndex":10075},{"text":"Tool Execution Flow","id":"tool-execution-flow","depth":2,"charIndex":10377},{"text":"1. Tool Invocation","id":"1-tool-invocation","depth":3,"charIndex":10488},{"text":"2. Confirmation Process","id":"2-confirmation-process","depth":3,"charIndex":10673},{"text":"Trust-based Bypass","id":"trust-based-bypass","depth":4,"charIndex":10768},{"text":"Dynamic Allow-listing","id":"dynamic-allow-listing","depth":4,"charIndex":10791},{"text":"User Choice Handling","id":"user-choice-handling","depth":4,"charIndex":11000},{"text":"3. Execution","id":"3-execution","depth":3,"charIndex":11258},{"text":"4. Response Handling","id":"4-response-handling","depth":3,"charIndex":11558},{"text":"How to interact with your MCP server","id":"how-to-interact-with-your-mcp-server","depth":2,"charIndex":11774},{"text":"Using the `/mcp` Command","id":"using-the-mcp-command","depth":3,"charIndex":-1},{"text":"Example `/mcp` Output","id":"example-mcp-output","depth":3,"charIndex":-1},{"text":"Tool Usage","id":"tool-usage","depth":3,"charIndex":12260},{"text":"Status Monitoring and Troubleshooting","id":"status-monitoring-and-troubleshooting","depth":2,"charIndex":12591},{"text":"Connection States","id":"connection-states","depth":3,"charIndex":12632},{"text":"Server Status (`MCPServerStatus`)","id":"server-status-mcpserverstatus","depth":4,"charIndex":-1},{"text":"Discovery State (`MCPDiscoveryState`)","id":"discovery-state-mcpdiscoverystate","depth":4,"charIndex":-1},{"text":"Common Issues and Solutions","id":"common-issues-and-solutions","depth":3,"charIndex":13059},{"text":"Server Won't Connect","id":"server-wont-connect","depth":4,"charIndex":13089},{"text":"No Tools Discovered","id":"no-tools-discovered","depth":4,"charIndex":13508},{"text":"Tools Not Executing","id":"tools-not-executing","depth":4,"charIndex":13909},{"text":"Sandbox Compatibility","id":"sandbox-compatibility","depth":4,"charIndex":14289},{"text":"Debugging Tips","id":"debugging-tips","depth":3,"charIndex":14704},{"text":"Important Notes","id":"important-notes","depth":2,"charIndex":15105},{"text":"Security Considerations","id":"security-considerations","depth":3,"charIndex":15124},{"text":"Performance and Resource Management","id":"performance-and-resource-management","depth":3,"charIndex":15626},{"text":"Schema Compatibility","id":"schema-compatibility","depth":3,"charIndex":16072},{"text":"Returning Rich Content from Tools","id":"returning-rich-content-from-tools","depth":2,"charIndex":16594},{"text":"How It Works","id":"how-it-works","depth":3,"charIndex":17084},{"text":"Example: Returning Text and an Image","id":"example-returning-text-and-an-image","depth":3,"charIndex":17576},{"text":"MCP Prompts as Slash Commands","id":"mcp-prompts-as-slash-commands","depth":2,"charIndex":18158},{"text":"Defining Prompts on the Server","id":"defining-prompts-on-the-server","depth":3,"charIndex":18415},{"text":"Invoking Prompts","id":"invoking-prompts","depth":3,"charIndex":18583},{"text":"Managing MCP Servers with `gemini mcp`","id":"managing-mcp-servers-with-gemini-mcp","depth":2,"charIndex":-1},{"text":"Adding a Server (`gemini mcp add`)","id":"adding-a-server-gemini-mcp-add","depth":3,"charIndex":-1},{"text":"Adding an stdio server","id":"adding-an-stdio-server","depth":4,"charIndex":20607},{"text":"Adding an HTTP server","id":"adding-an-http-server","depth":4,"charIndex":20692},{"text":"Adding an SSE server","id":"adding-an-sse-server","depth":4,"charIndex":20789},{"text":"Listing Servers (`gemini mcp list`)","id":"listing-servers-gemini-mcp-list","depth":3,"charIndex":-1},{"text":"Removing a Server (`gemini mcp remove`)","id":"removing-a-server-gemini-mcp-remove","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":47,"title":"Memory Tool (`save_memory`)","content":"Memory Tool (save_memory)#\n\nThis document describes the save_memory tool for the VeCLI.\n\n\nDescription#\n\nUse save_memory to save and recall information across your VeCLI sessions. With\nsave_memory, you can direct the CLI to remember key details across sessions,\nproviding personalized and directed assistance.\n\n\nArguments#\n\nsave_memory takes one argument:\n\n * fact (string, required): The specific fact or piece of information to\n   remember. This should be a clear, self-contained statement written in natural\n   language.\n\n\nHow to use save_memory with the VeCLI#\n\nThe tool appends the provided fact to a special VE.md file located in the user's\nhome directory (~/.ve/VE.md). This file can be configured to have a different\nname.\n\nOnce added, the facts are stored under a ## vecli Added Memories section. This\nfile is loaded as context in subsequent sessions, allowing the CLI to recall the\nsaved information.\n\nUsage:\n\n\n\n\nsave_memory examples#\n\nRemember a user preference:\n\n\n\nStore a project-specific detail:\n\n\n\n\nImportant notes#\n\n * General usage: This tool should be used for concise, important facts. It is\n   not intended for storing large amounts of data or conversational history.\n * Memory file: The memory file is a plain text Markdown file, so you can view\n   and edit it manually if needed.","routePath":"/en/tools/memory","lang":"","toc":[{"text":"Description","id":"description","depth":2,"charIndex":89},{"text":"Arguments","id":"arguments","depth":3,"charIndex":310},{"text":"How to use `save_memory` with the VeCLI","id":"how-to-use-save_memory-with-the-vecli","depth":2,"charIndex":-1},{"text":"`save_memory` examples","id":"save_memory-examples","depth":3,"charIndex":-1},{"text":"Important notes","id":"important-notes","depth":2,"charIndex":1012}],"domain":"","frontmatter":{},"version":""},{"id":48,"title":"Multi File Read Tool (`read_many_files`)","content":"Multi File Read Tool (read_many_files)#\n\nThis document describes the read_many_files tool for the VeCLI.\n\n\nDescription#\n\nUse read_many_files to read content from multiple files specified by paths or\nglob patterns. The behavior of this tool depends on the provided files:\n\n * For text files, this tool concatenates their content into a single string.\n * For image (e.g., PNG, JPEG), PDF, audio (MP3, WAV), and video (MP4, MOV)\n   files, it reads and returns them as base64-encoded data, provided they are\n   explicitly requested by name or extension.\n\nread_many_files can be used to perform tasks such as getting an overview of a\ncodebase, finding where specific functionality is implemented, reviewing\ndocumentation, or gathering context from multiple configuration files.\n\nNote: read_many_files looks for files following the provided paths or glob\npatterns. A directory path such as \"/docs\" will return an empty result; the tool\nrequires a pattern such as \"/docs/*\" or \"/docs/*.md\" to identify the relevant\nfiles.\n\n\nArguments#\n\nread_many_files takes the following arguments:\n\n * paths (list[string], required): An array of glob patterns or paths relative\n   to the tool's target directory (e.g., [\"src/**/*.ts\"], [\"README.md\",\n   \"docs/*\", \"assets/logo.png\"]).\n * exclude (list[string], optional): Glob patterns for files/directories to\n   exclude (e.g., [\"**/*.log\", \"temp/\"]). These are added to default excludes if\n   useDefaultExcludes is true.\n * include (list[string], optional): Additional glob patterns to include. These\n   are merged with paths (e.g., [\"*.test.ts\"] to specifically add test files if\n   they were broadly excluded, or [\"images/*.jpg\"] to include specific image\n   types).\n * recursive (boolean, optional): Whether to search recursively. This is\n   primarily controlled by ** in glob patterns. Defaults to true.\n * useDefaultExcludes (boolean, optional): Whether to apply a list of default\n   exclusion patterns (e.g., node_modules, .git, non image/pdf binary files).\n   Defaults to true.\n * respect_git_ignore (boolean, optional): Whether to respect .gitignore\n   patterns when finding files. Defaults to true.\n\n\nHow to use read_many_files with the VeCLI#\n\nread_many_files searches for files matching the provided paths and include\npatterns, while respecting exclude patterns and default excludes (if enabled).\n\n * For text files: it reads the content of each matched file (attempting to skip\n   binary files not explicitly requested as image/PDF) and concatenates it into\n   a single string, with a separator --- {filePath} --- between the content of\n   each file. Uses UTF-8 encoding by default.\n * The tool inserts a --- End of content --- after the last file.\n * For image and PDF files: if explicitly requested by name or extension (e.g.,\n   paths: [\"logo.png\"] or include: [\"*.pdf\"]), the tool reads the file and\n   returns its content as a base64 encoded string.\n * The tool attempts to detect and skip other binary files (those not matching\n   common image/PDF types or not explicitly requested) by checking for null\n   bytes in their initial content.\n\nUsage:\n\n\n\n\nread_many_files examples#\n\nRead all TypeScript files in the src directory:\n\n\n\nRead the main README, all Markdown files in the docs directory, and a specific\nlogo image, excluding a specific file:\n\n\n\nRead all JavaScript files but explicitly include test files and all JPEGs in an\nimages folder:\n\n\n\n\nImportant notes#\n\n * Binary file handling:\n   * Image/PDF/Audio/Video files: The tool can read common image types (PNG,\n     JPEG, etc.), PDF, audio (mp3, wav), and video (mp4, mov) files, returning\n     them as base64 encoded data. These files must be explicitly targeted by the\n     paths or include patterns (e.g., by specifying the exact filename like\n     video.mp4 or a pattern like *.mov).\n   * Other binary files: The tool attempts to detect and skip other types of\n     binary files by examining their initial content for null bytes. The tool\n     excludes these files from its output.\n * Performance: Reading a very large number of files or very large individual\n   files can be resource-intensive.\n * Path specificity: Ensure paths and glob patterns are correctly specified\n   relative to the tool's target directory. For image/PDF files, ensure the\n   patterns are specific enough to include them.\n * Default excludes: Be aware of the default exclusion patterns (like\n   node_modules, .git) and use useDefaultExcludes=False if you need to override\n   them, but do so cautiously.","routePath":"/en/tools/multi-file","lang":"","toc":[{"text":"Description","id":"description","depth":2,"charIndex":106},{"text":"Arguments","id":"arguments","depth":3,"charIndex":1016},{"text":"How to use `read_many_files` with the VeCLI","id":"how-to-use-read_many_files-with-the-vecli","depth":2,"charIndex":-1},{"text":"`read_many_files` examples","id":"read_many_files-examples","depth":2,"charIndex":-1},{"text":"Important notes","id":"important-notes","depth":2,"charIndex":3395}],"domain":"","frontmatter":{},"version":""},{"id":49,"title":"Shell Tool (`run_shell_command`)","content":"Shell Tool (run_shell_command)#\n\nThis document describes the run_shell_command tool for the VeCLI.\n\n\nDescription#\n\nUse run_shell_command to interact with the underlying system, run scripts, or\nperform command-line operations. run_shell_command executes a given shell\ncommand. On Windows, the command will be executed with cmd.exe /c. On other\nplatforms, the command will be executed with bash -c.\n\n\nArguments#\n\nrun_shell_command takes the following arguments:\n\n * command (string, required): The exact shell command to execute.\n * description (string, optional): A brief description of the command's purpose,\n   which will be shown to the user.\n * directory (string, optional): The directory (relative to the project root) in\n   which to execute the command. If not provided, the command runs in the\n   project root.\n\n\nHow to use run_shell_command with the VeCLI#\n\nWhen using run_shell_command, the command is executed as a subprocess.\nrun_shell_command can start background processes using &. The tool returns\ndetailed information about the execution, including:\n\n * Command: The command that was executed.\n * Directory: The directory where the command was run.\n * Stdout: Output from the standard output stream.\n * Stderr: Output from the standard error stream.\n * Error: Any error message reported by the subprocess.\n * Exit Code: The exit code of the command.\n * Signal: The signal number if the command was terminated by a signal.\n * Background PIDs: A list of PIDs for any background processes started.\n\nUsage:\n\n\n\n\nrun_shell_command examples#\n\nList files in the current directory:\n\n\n\nRun a script in a specific directory:\n\n\n\nStart a background server:\n\n\n\n\nImportant notes#\n\n * Security: Be cautious when executing commands, especially those constructed\n   from user input, to prevent security vulnerabilities.\n * Interactive commands: Avoid commands that require interactive user input, as\n   this can cause the tool to hang. Use non-interactive flags if available\n   (e.g., npm init -y).\n * Error handling: Check the Stderr, Error, and Exit Code fields to determine if\n   a command executed successfully.\n * Background processes: When a command is run in the background with &, the\n   tool will return immediately and the process will continue to run in the\n   background. The Background PIDs field will contain the process ID of the\n   background process.\n\n\nEnvironment Variables#\n\nWhen run_shell_command executes a command, it sets the VE_CLI=1 environment\nvariable in the subprocess's environment. This allows scripts or tools to detect\nif they are being run from within the VeCLI.\n\n\nCommand Restrictions#\n\nYou can restrict the commands that can be executed by the run_shell_command tool\nby using the tools.core and tools.exclude settings in your configuration file.\n\n * tools.core: To restrict run_shell_command to a specific set of commands, add\n   entries to the core list under the tools category in the format\n   run_shell_command(<command>). For example, \"tools\": {\"core\":\n   [\"run_shell_command(git)\"]} will only allow git commands. Including the\n   generic run_shell_command acts as a wildcard, allowing any command not\n   explicitly blocked.\n * tools.exclude: To block specific commands, add entries to the exclude list\n   under the tools category in the format run_shell_command(<command>). For\n   example, \"tools\": {\"exclude\": [\"run_shell_command(rm)\"]} will block rm\n   commands.\n\nThe validation logic is designed to be secure and flexible:\n\n 1. Command Chaining Disabled: The tool automatically splits commands chained\n    with &&, ||, or ; and validates each part separately. If any part of the\n    chain is disallowed, the entire command is blocked.\n 2. Prefix Matching: The tool uses prefix matching. For example, if you allow\n    git, you can run git status or git log.\n 3. Blocklist Precedence: The tools.exclude list is always checked first. If a\n    command matches a blocked prefix, it will be denied, even if it also matches\n    an allowed prefix in tools.core.\n\n\nCommand Restriction Examples#\n\nAllow only specific command prefixes\n\nTo allow only git and npm commands, and block all others:\n\n\n\n * git status: Allowed\n * npm install: Allowed\n * ls -l: Blocked\n\nBlock specific command prefixes\n\nTo block rm and allow all other commands:\n\n\n\n * rm -rf /: Blocked\n * git status: Allowed\n * npm install: Allowed\n\nBlocklist takes precedence\n\nIf a command prefix is in both tools.core and tools.exclude, it will be blocked.\n\n\n\n * git push origin main: Blocked\n * git status: Allowed\n\nBlock all shell commands\n\nTo block all shell commands, add the run_shell_command wildcard to\ntools.exclude:\n\n\n\n * ls -l: Blocked\n * any other command: Blocked\n\n\nSecurity Note for excludeTools#\n\nCommand-specific restrictions in excludeTools for run_shell_command are based on\nsimple string matching and can be easily bypassed. This feature is not a\nsecurity mechanism and should not be relied upon to safely execute untrusted\ncode. It is recommended to use coreTools to explicitly select commands that can\nbe executed.","routePath":"/en/tools/shell","lang":"","toc":[{"text":"Description","id":"description","depth":2,"charIndex":100},{"text":"Arguments","id":"arguments","depth":3,"charIndex":398},{"text":"How to use `run_shell_command` with the VeCLI","id":"how-to-use-run_shell_command-with-the-vecli","depth":2,"charIndex":-1},{"text":"`run_shell_command` examples","id":"run_shell_command-examples","depth":2,"charIndex":-1},{"text":"Important notes","id":"important-notes","depth":2,"charIndex":1661},{"text":"Environment Variables","id":"environment-variables","depth":2,"charIndex":2365},{"text":"Command Restrictions","id":"command-restrictions","depth":2,"charIndex":2593},{"text":"Command Restriction Examples","id":"command-restriction-examples","depth":3,"charIndex":3995},{"text":"Security Note for `excludeTools`","id":"security-note-for-excludetools","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":50,"title":"Web Fetch Tool (`web_fetch`)","content":"Web Fetch Tool (web_fetch)#\n\nThis document describes the web_fetch tool for the VeCLI.\n\n\nDescription#\n\nUse web_fetch to summarize, compare, or extract information from web pages. The\nweb_fetch tool processes content from one or more URLs (up to 20) embedded in a\nprompt. web_fetch takes a natural language prompt and returns a generated\nresponse.\n\n\nArguments#\n\nweb_fetch takes one argument:\n\n * prompt (string, required): A comprehensive prompt that includes the URL(s)\n   (up to 20) to fetch and specific instructions on how to process their\n   content. For example: \"Summarize https://example.com/article and extract key\n   points from https://another.com/data\". The prompt must contain at least one\n   URL starting with http:// or https://.\n\n\nHow to use web_fetch with the VeCLI#\n\nTo use web_fetch with the VeCLI, provide a natural language prompt that contains\nURLs. The tool will ask for confirmation before fetching any URLs. Once\nconfirmed, the tool will process URLs through Gemini API's urlContext.\n\nIf the Gemini API cannot access the URL, the tool will fall back to fetching\ncontent directly from the local machine. The tool will format the response,\nincluding source attribution and citations where possible. The tool will then\nprovide the response to the user.\n\nUsage:\n\n\n\n\nweb_fetch examples#\n\nSummarize a single article:\n\n\n\nCompare two articles:\n\n\n\n\nImportant notes#\n\n * URL processing: web_fetch relies on the Gemini API's ability to access and\n   process the given URLs.\n * Output quality: The quality of the output will depend on the clarity of the\n   instructions in the prompt.","routePath":"/en/tools/web-fetch","lang":"","toc":[{"text":"Description","id":"description","depth":2,"charIndex":88},{"text":"Arguments","id":"arguments","depth":3,"charIndex":348},{"text":"How to use `web_fetch` with the VeCLI","id":"how-to-use-web_fetch-with-the-vecli","depth":2,"charIndex":-1},{"text":"`web_fetch` examples","id":"web_fetch-examples","depth":2,"charIndex":-1},{"text":"Important notes","id":"important-notes","depth":2,"charIndex":1363}],"domain":"","frontmatter":{},"version":""},{"id":51,"title":"VeCLI: Terms of Service and Privacy Notice","content":"#\n\nVeCLI is an open-source tool that lets you interact with Volcengine's powerful\nlanguage models directly from your command-line interface. The Terms of Service\nand Privacy Notices that apply to your usage of the VeCLI depend on the type of\naccount you use to authenticate with Volcengine.\n\nThis article outlines the specific terms and privacy policies applicable for\ndifferent account types and authentication methods. Note: See quotas and pricing\nfor the quota and pricing details that apply to your usage of the VeCLI.\n\n\nHow to determine your authentication method#\n\nYour authentication method refers to the method you use to log into and access\nthe VeCLI. There are four ways to authenticate:\n\n * Logging in with your Volcengine account to Ve Code Assist for Individuals\n * Logging in with your Volcengine account to Ve Code Assist for Standard, or\n   Enterprise Users\n * Using an API key with vecli Developer\n * Using an API key with Vertex AI GenAI API\n\nFor each of these four methods of authentication, different Terms of Service and\nPrivacy Notices may apply.\n\nAUTHENTICATION                  ACCOUNT               TERMS OF SERVICE                               PRIVACY NOTICE\nVe Code Assist via Volcengine   Individual            Volcengine Terms of Service                    Ve Code Assist Privacy Notice for Individuals\nVe Code Assist via Volcengine   Standard/Enterprise   Volcano Engine Platform Terms of Service       Ve Code Assist Privacy Notice for Standard and Enterprise\nvecli Developer API             Unpaid                vecli API Terms of Service - Unpaid Services   Volcengine Privacy Policy\nvecli Developer API             Paid                  vecli API Terms of Service - Paid Services     Volcengine Privacy Policy\nVertex AI Gen API                                     Volcano Engine Platform Service Terms          Volcano Engine Privacy Notice\n\n\n1. If you have logged in with your Volcengine account to Ve Code Assist for\nIndividuals#\n\nFor users who use their Volcengine account to access Ve Code Assist for\nIndividuals, these Terms of Service and Privacy Notice documents apply:\n\n * Terms of Service: Your use of the VeCLI is governed by the Volcengine Terms\n   of Service.\n * Privacy Notice: The collection and use of your data is described in the Ve\n   Code Assist Privacy Notice for Individuals.\n\n\n2. If you have logged in with your Volcengine account to Ve Code Assist for\nStandard, or Enterprise Users#\n\nFor users who use their Volcengine account to access the Standard or Enterprise\nedition of Ve Code Assist, these Terms of Service and Privacy Notice documents\napply:\n\n * Terms of Service: Your use of the VeCLI is governed by the Volcano Engine\n   Platform Terms of Service.\n * Privacy Notice: The collection and use of your data is described in the Ve\n   Code Assist Privacy Notices for Standard and Enterprise Users.\n\n\n3. If you have logged in with a vecli API key to the vecli Developer API#\n\nIf you are using a vecli API key for authentication with the vecli Developer\nAPI, these Terms of Service and Privacy Notice documents apply:\n\n * Terms of Service: Your use of the VeCLI is governed by the vecli API Terms of\n   Service. These terms may differ depending on whether you are using an unpaid\n   or paid service:\n   * For unpaid services, refer to the vecli API Terms of Service - Unpaid\n     Services.\n   * For paid services, refer to the vecli API Terms of Service - Paid Services.\n * Privacy Notice: The collection and use of your data is described in the\n   Volcengine Privacy Policy.\n\n\n4. If you have logged in with a vecli API key to the Vertex AI GenAI API#\n\nIf you are using a vecli API key for authentication with a Vertex AI GenAI API\nbackend, these Terms of Service and Privacy Notice documents apply:\n\n * Terms of Service: Your use of the VeCLI is governed by the Volcano Engine\n   Platform Service Terms.\n * Privacy Notice: The collection and use of your data is described in the\n   Volcano Engine Privacy Notice.\n\n\nUsage Statistics Opt-Out#\n\nYou may opt-out from sending Usage Statistics to Volcengine by following the\ninstructions available here: Usage Statistics Configuration.\n\n\nFrequently Asked Questions (FAQ) for the VeCLI#\n\n\n1. Is my code, including prompts and answers, used to train Volcengine's\nmodels?#\n\nWhether your code, including prompts and answers, is used to train Volcengine's\nmodels depends on the type of authentication method you use and your account\ntype.\n\nBy default (if you have not opted out):\n\n * Volcengine account with Ve Code Assist for Individuals: Yes. When you use\n   your personal Volcengine account, the Ve Code Assist Privacy Notice for\n   Individuals applies. Under this notice, your prompts, answers, and related\n   code are collected and may be used to improve Volcengine's products,\n   including for model training.\n * Volcengine account with Ve Code Assist for Standard, or Enterprise: No. For\n   these accounts, your data is governed by the Ve Code Assist Privacy Notices\n   terms, which treat your inputs as confidential. Your prompts, answers, and\n   related code are not collected and are not used to train models.\n * vecli API key via the vecli Developer API: Whether your code is collected or\n   used depends on whether you are using an unpaid or paid service.\n   * Unpaid services: Yes. When you use the vecli API key via the vecli\n     Developer API with an unpaid service, the vecli API Terms of Service -\n     Unpaid Services terms apply. Under this notice, your prompts, answers, and\n     related code are collected and may be used to improve Volcengine's\n     products, including for model training.\n   * Paid services: No. When you use the vecli API key via the vecli Developer\n     API with a paid service, the vecli API Terms of Service - Paid Services\n     terms apply, which treats your inputs as confidential. Your prompts,\n     answers, and related code are not collected and are not used to train\n     models.\n * vecli API key via the Vertex AI GenAI API: No. For these accounts, your data\n   is governed by the Volcano Engine Privacy Notice terms, which treat your\n   inputs as confidential. Your prompts, answers, and related code are not\n   collected and are not used to train models.\n\nFor more information about opting out, refer to the next question.\n\n\n2. What are Usage Statistics and what does the opt-out control?#\n\nThe Usage Statistics setting is the single control for all optional data\ncollection in the VeCLI.\n\nThe data it collects depends on your account and authentication type:\n\n * Volcengine account with Ve Code Assist for Individuals: When enabled, this\n   setting allows Volcengine to collect both anonymous telemetry (for example,\n   commands run and performance metrics) and your prompts and answers, including\n   code, for model improvement.\n * Volcengine account with Ve Code Assist for Standard, or Enterprise: This\n   setting only controls the collection of anonymous telemetry. Your prompts and\n   answers, including code, are never collected, regardless of this setting.\n * vecli API key via the vecli Developer API: Unpaid services: When enabled,\n   this setting allows Volcengine to collect both anonymous telemetry (like\n   commands run and performance metrics) and your prompts and answers, including\n   code, for model improvement. When disabled we will use your data as described\n   in How Volcengine Uses Your Data. Paid services: This setting only controls\n   the collection of anonymous telemetry. Volcengine logs prompts and responses\n   for a limited period of time, solely for the purpose of detecting violations\n   of the Prohibited Use Policy and any required legal or regulatory\n   disclosures.\n * vecli API key via the Vertex AI GenAI API: This setting only controls the\n   collection of anonymous telemetry. Your prompts and answers, including code,\n   are never collected, regardless of this setting.\n\nPlease refer to the Privacy Notice that applies to your authentication method\nfor more information about what data is collected and how this data is used.\n\nYou can disable Usage Statistics for any account type by following the\ninstructions in the Usage Statistics Configuration documentation.","routePath":"/en/tos-privacy","lang":"","toc":[{"text":"How to determine your authentication method","id":"how-to-determine-your-authentication-method","depth":2,"charIndex":524},{"text":"1. If you have logged in with your Volcengine account to Ve Code Assist for Individuals","id":"1-if-you-have-logged-in-with-your-volcengine-account-to-ve-code-assist-for-individuals","depth":2,"charIndex":-1},{"text":"2. If you have logged in with your Volcengine account to Ve Code Assist for Standard, or Enterprise Users","id":"2-if-you-have-logged-in-with-your-volcengine-account-to-ve-code-assist-for-standard-or-enterprise-users","depth":2,"charIndex":-1},{"text":"3. If you have logged in with a vecli API key to the vecli Developer API","id":"3-if-you-have-logged-in-with-a-vecli-api-key-to-the-vecli-developer-api","depth":2,"charIndex":2862},{"text":"4. If you have logged in with a vecli API key to the Vertex AI GenAI API","id":"4-if-you-have-logged-in-with-a-vecli-api-key-to-the-vertex-ai-genai-api","depth":2,"charIndex":3538},{"text":"Usage Statistics Opt-Out","id":"usage-statistics-opt-out","depth":3,"charIndex":3976},{"text":"Frequently Asked Questions (FAQ) for the VeCLI","id":"frequently-asked-questions-faq-for-the-vecli","depth":2,"charIndex":4143},{"text":"1. Is my code, including prompts and answers, used to train Volcengine's models?","id":"1-is-my-code-including-prompts-and-answers-used-to-train-volcengines-models","depth":3,"charIndex":-1},{"text":"2. What are Usage Statistics and what does the opt-out control?","id":"2-what-are-usage-statistics-and-what-does-the-opt-out-control","depth":3,"charIndex":6279}],"domain":"","frontmatter":{},"version":""},{"id":52,"title":"Troubleshooting guide","content":"#\n\nThis guide provides solutions to common issues and debugging tips, including\ntopics on:\n\n * Authentication or login errors\n * Frequently asked questions (FAQs)\n * Debugging tips\n * Existing GitHub Issues similar to yours or creating new Issues\n\n\nAuthentication or login errors#\n\n * Error: UNABLE_TO_GET_ISSUER_CERT_LOCALLY or unable to get local issuer\n   certificate\n   * Cause: You may be on a corporate network with a firewall that intercepts\n     and inspects SSL/TLS traffic. This often requires a custom root CA\n     certificate to be trusted by Node.js.\n   * Solution: Set the NODE_EXTRA_CA_CERTS environment variable to the absolute\n     path of your corporate root CA certificate file.\n     * Example: export NODE_EXTRA_CA_CERTS=/path/to/your/corporate-ca.crt\n\n\nFrequently asked questions (FAQs)#\n\n * Q: How do I update VeCLI to the latest version?\n   \n   * A: If you installed it globally via npm, update it using the command npm\n     install -g @vecli/vecli@latest. If you compiled it from source, pull the\n     latest changes from the repository, and then rebuild using the command npm\n     run build.\n\n * Q: Where are the VeCLI configuration or settings files stored?\n   \n   * A: The VeCLI configuration is stored in two settings.json files:\n     \n     1. In your home directory: ~/.ve/settings.json.\n     2. In your project's root directory: ./.ve/settings.json.\n     \n     Refer to VeCLI Configuration for more details.\n\n\nCommon error messages and solutions#\n\n * Error: EADDRINUSE (Address already in use) when starting an MCP server.\n   \n   * Cause: Another process is already using the port that the MCP server is\n     trying to bind to.\n   * Solution: Either stop the other process that is using the port or configure\n     the MCP server to use a different port.\n\n * Error: Command not found (when attempting to run VeCLI with vecli).\n   \n   * Cause: VeCLI is not correctly installed or it is not in your system's PATH.\n   * Solution: The update depends on how you installed VeCLI:\n     * If you installed vecli globally, check that your npm global binary\n       directory is in your PATH. You can update VeCLI using the command npm\n       install -g @vecli/vecli@latest.\n     * If you are running vecli from source, ensure you are using the correct\n       command to invoke it (e.g., node packages/cli/dist/index.js ...). To\n       update VeCLI, pull the latest changes from the repository, and then\n       rebuild using the command npm run build.\n\n * Error: MODULE_NOT_FOUND or import errors.\n   \n   * Cause: Dependencies are not installed correctly, or the project hasn't been\n     built.\n   * Solution:\n     1. Run npm install to ensure all dependencies are present.\n     2. Run npm run build to compile the project.\n     3. Verify that the build completed successfully with npm run start.\n\n * Error: \"Operation not permitted\", \"Permission denied\", or similar.\n   \n   * Cause: When sandboxing is enabled, VeCLI may attempt operations that are\n     restricted by your sandbox configuration, such as writing outside the\n     project directory or system temp directory.\n   * Solution: Refer to the Configuration: Sandboxing documentation for more\n     information, including how to customize your sandbox configuration.\n\n * VeCLI is not running in interactive mode in \"CI\" environments\n   \n   * Issue: The VeCLI does not enter interactive mode (no prompt appears) if an\n     environment variable starting with CI_ (e.g., CI_TOKEN) is set. This is\n     because the is-in-ci package, used by the underlying UI framework, detects\n     these variables and assumes a non-interactive CI environment.\n   * Cause: The is-in-ci package checks for the presence of CI,\n     CONTINUOUS_INTEGRATION, or any environment variable with a CI_ prefix. When\n     any of these are found, it signals that the environment is non-interactive,\n     which prevents the VeCLI from starting in its interactive mode.\n   * Solution: If the CI_ prefixed variable is not needed for the CLI to\n     function, you can temporarily unset it for the command. e.g., env -u\n     CI_TOKEN vecli\n\n * DEBUG mode not working from project .env file\n   \n   * Issue: Setting DEBUG=true in a project's .env file doesn't enable debug\n     mode for vecli.\n   * Cause: The DEBUG and DEBUG_MODE variables are automatically excluded from\n     project .env files to prevent interference with vecli behavior.\n   * Solution: Use a .ve/.env file instead, or configure the\n     advanced.excludedEnvVars setting in your settings.json to exclude fewer\n     variables.\n\n\nExit Codes#\n\nThe VeCLI uses specific exit codes to indicate the reason for termination. This\nis especially useful for scripting and automation.\n\nEXIT CODE   ERROR TYPE                 DESCRIPTION\n41          FatalAuthenticationError   An error occurred during the authentication process.\n42          FatalInputError            Invalid or missing input was provided to the CLI.\n                                       (non-interactive mode only)\n44          FatalSandboxError          An error occurred with the sandboxing environment (e.g.,\n                                       Docker, Podman, or Seatbelt).\n52          FatalConfigError           A configuration file (settings.json) is invalid or contains\n                                       errors.\n53          FatalTurnLimitedError      The maximum number of conversational turns for the session\n                                       was reached. (non-interactive mode only)\n\n\nDebugging Tips#\n\n * CLI debugging:\n   \n   * Use the --verbose flag (if available) with CLI commands for more detailed\n     output.\n   * Check the CLI logs, often found in a user-specific configuration or cache\n     directory.\n\n * Core debugging:\n   \n   * Check the server console output for error messages or stack traces.\n   * Increase log verbosity if configurable.\n   * Use Node.js debugging tools (e.g., node --inspect) if you need to step\n     through server-side code.\n\n * Tool issues:\n   \n   * If a specific tool is failing, try to isolate the issue by running the\n     simplest possible version of the command or operation the tool performs.\n   * For run_shell_command, check that the command works directly in your shell\n     first.\n   * For file system tools, verify that paths are correct and check the\n     permissions.\n\n * Pre-flight checks:\n   \n   * Always run npm run preflight before committing code. This can catch many\n     common issues related to formatting, linting, and type errors.\n\n\nExisting GitHub Issues similar to yours or creating new Issues#\n\nIf you encounter an issue that was not covered here in this Troubleshooting\nguide, consider searching the VeCLI Issue tracker on GitHub. If you can't find\nan issue similar to yours, consider creating a new GitHub Issue with a detailed\ndescription. Pull requests are also welcome!","routePath":"/en/troubleshooting","lang":"","toc":[{"text":"Authentication or login errors","id":"authentication-or-login-errors","depth":2,"charIndex":248},{"text":"Frequently asked questions (FAQs)","id":"frequently-asked-questions-faqs","depth":2,"charIndex":773},{"text":"Common error messages and solutions","id":"common-error-messages-and-solutions","depth":2,"charIndex":1439},{"text":"Exit Codes","id":"exit-codes","depth":2,"charIndex":4533},{"text":"Debugging Tips","id":"debugging-tips","depth":2,"charIndex":5468},{"text":"Existing GitHub Issues similar to yours or creating new Issues","id":"existing-github-issues-similar-to-yours-or-creating-new-issues","depth":2,"charIndex":6475}],"domain":"","frontmatter":{},"version":""},{"id":53,"title":"Ignoring Files","content":"#\n\nThis document provides an overview of the Gemini Ignore (.veignore) feature of\nthe VeCLI.\n\nThe VeCLI includes the ability to automatically ignore files, similar to\n.gitignore (used by Git) and .aiexclude (used by Ve Code Assist). Adding paths\nto your .veignore file will exclude them from tools that support this feature,\nalthough they will still be visible to other services (such as Git).\n\n\nHow it works#\n\nWhen you add a path to your .veignore file, tools that respect this file will\nexclude matching files and directories from their operations. For example, when\nyou use the read_many_files command, any paths in your .veignore file will be\nautomatically excluded.\n\nFor the most part, .veignore follows the conventions of .gitignore files:\n\n * Blank lines and lines starting with # are ignored.\n * Standard glob patterns are supported (such as *, ?, and []).\n * Putting a / at the end will only match directories.\n * Putting a / at the beginning anchors the path relative to the .veignore file.\n * ! negates a pattern.\n\nYou can update your .veignore file at any time. To apply the changes, you must\nrestart your VeCLI session.\n\n\nHow to use .veignore#\n\nTo enable .veignore:\n\n 1. Create a file named .veignore in the root of your project directory.\n\nTo add a file or directory to .veignore:\n\n 1. Open your .veignore file.\n 2. Add the path or file you want to ignore, for example: /archive/ or\n    apikeys.txt.\n\n\n.veignore examples#\n\nYou can use .veignore to ignore directories and files:\n\n\n\nYou can use wildcards in your .veignore file with *:\n\n\n\nFinally, you can exclude files and directories from exclusion with !:\n\n\n\nTo remove paths from your .veignore file, delete the relevant lines.","routePath":"/en/ve-ignore","lang":"","toc":[{"text":"How it works","id":"how-it-works","depth":2,"charIndex":395},{"text":"How to use `.veignore`","id":"how-to-use-veignore","depth":2,"charIndex":-1},{"text":"`.veignore` examples","id":"veignore-examples","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":54,"title":"Example Proxy Script","content":"#\n\nThe following is an example of a proxy script that can be used with the\nGEMINI_SANDBOX_PROXY_COMMAND environment variable. This script only allows HTTPS\nconnections to example.com:443 and declines all other requests.\n\n","routePath":"/examples/proxy-script","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":55,"title":"Variables","content":"VeCLI Extensions#\n\nVeCLI supports extensions that can be used to configure and extend its\nfunctionality.\n\n\nHow it works#\n\nOn startup, VeCLI looks for extensions in two locations:\n\n 1. <workspace>/.vecli/extensions\n 2. <home>/.vecli/extensions\n\nVeCLI loads all extensions from both locations. If an extension with the same\nname exists in both locations, the extension in the workspace directory takes\nprecedence.\n\nWithin each location, individual extensions exist as a directory that contains a\nvecli-extension.json file. For example:\n\n<workspace>/.vecli/extensions/my-extension/vecli-extension.json\n\n\nvecli-extension.json#\n\nThe vecli-extension.json file contains the configuration for the extension. The\nfile has the following structure:\n\n\n\n * name: The name of the extension. This is used to uniquely identify the\n   extension and for conflict resolution when extension commands have the same\n   name as user or project commands.\n * version: The version of the extension.\n * mcpServers: A map of MCP servers to configure. The key is the name of the\n   server, and the value is the server configuration. These servers will be\n   loaded on startup just like MCP servers configured in a settings.json file.\n   If both an extension and a settings.json file configure an MCP server with\n   the same name, the server defined in the settings.json file takes precedence.\n * contextFileName: The name of the file that contains the context for the\n   extension. This will be used to load the context from the workspace. If this\n   property is not used but a VE.md file is present in your extension directory,\n   then that file will be loaded.\n * excludeTools: An array of tool names to exclude from the model. You can also\n   specify command-specific restrictions for tools that support it, like the\n   run_shell_command tool. For example, \"excludeTools\": [\"run_shell_command(rm\n   -rf)\"] will block the rm -rf command.\n\nWhen VeCLI starts, it loads all the extensions and merges their configurations.\nIf there are any conflicts, the workspace configuration takes precedence.\n\n\nExtension Commands#\n\nExtensions can provide custom commands by placing TOML files in a commands/\nsubdirectory within the extension directory. These commands follow the same\nformat as user and project custom commands and use standard naming conventions.\n\n\nExample#\n\nAn extension named gcp with the following structure:\n\n\n\nWould provide these commands:\n\n * /deploy - Shows as [gcp] Custom command from deploy.toml in help\n * /gcs:sync - Shows as [gcp] Custom command from sync.toml in help\n\n\nConflict Resolution#\n\nExtension commands have the lowest precedence. When a conflict occurs with user\nor project commands:\n\n 1. No conflict: Extension command uses its natural name (e.g., /deploy)\n 2. With conflict: Extension command is renamed with the extension prefix (e.g.,\n    /gcp.deploy)\n\nFor example, if both a user and the gcp extension define a deploy command:\n\n * /deploy - Executes the user's deploy command\n * /gcp.deploy - Executes the extension's deploy command (marked with [gcp] tag)\n\n\nVariables#\n\nVeCLI extensions allow variable substitution in vecli-extension.json. This can\nbe useful if e.g., you need the current directory to run an MCP server using\n\"cwd\": \"${extensionPath}${/}run.ts\".\n\nSupported variables:\n\nVARIABLE                   DESCRIPTION\n${extensionPath}           The fully-qualified path of the extension in the user's\n                           filesystem e.g.,\n                           '/Users/username/.vecli/extensions/example-extension'. This\n                           will not unwrap symlinks.\n${/} or ${pathSeparator}   The path separator (differs per OS).","routePath":"/extension","lang":"","toc":[{"text":"How it works","id":"how-it-works","depth":2,"charIndex":106},{"text":"`vecli-extension.json`","id":"vecli-extensionjson","depth":3,"charIndex":-1},{"text":"Extension Commands","id":"extension-commands","depth":2,"charIndex":2067},{"text":"Example","id":"example","depth":3,"charIndex":2322},{"text":"Conflict Resolution","id":"conflict-resolution","depth":3,"charIndex":2557}],"domain":"","frontmatter":{},"version":""},{"id":56,"title":"Ignoring Files","content":"#\n\nThis document provides an overview of the Gemini Ignore (.veignore) feature of\nthe VeCLI.\n\nThe VeCLI includes the ability to automatically ignore files, similar to\n.gitignore (used by Git) and .aiexclude (used by Ve Code Assist). Adding paths\nto your .veignore file will exclude them from tools that support this feature,\nalthough they will still be visible to other services (such as Git).\n\n\nHow it works#\n\nWhen you add a path to your .veignore file, tools that respect this file will\nexclude matching files and directories from their operations. For example, when\nyou use the read_many_files command, any paths in your .veignore file will be\nautomatically excluded.\n\nFor the most part, .veignore follows the conventions of .gitignore files:\n\n * Blank lines and lines starting with # are ignored.\n * Standard glob patterns are supported (such as *, ?, and []).\n * Putting a / at the end will only match directories.\n * Putting a / at the beginning anchors the path relative to the .veignore file.\n * ! negates a pattern.\n\nYou can update your .veignore file at any time. To apply the changes, you must\nrestart your VeCLI session.\n\n\nHow to use .veignore#\n\nTo enable .veignore:\n\n 1. Create a file named .veignore in the root of your project directory.\n\nTo add a file or directory to .veignore:\n\n 1. Open your .veignore file.\n 2. Add the path or file you want to ignore, for example: /archive/ or\n    apikeys.txt.\n\n\n.veignore examples#\n\nYou can use .veignore to ignore directories and files:\n\n\n\nYou can use wildcards in your .veignore file with *:\n\n\n\nFinally, you can exclude files and directories from exclusion with !:\n\n\n\nTo remove paths from your .veignore file, delete the relevant lines.","routePath":"/gemini-ignore","lang":"","toc":[{"text":"How it works","id":"how-it-works","depth":2,"charIndex":395},{"text":"How to use `.veignore`","id":"how-to-use-veignore","depth":2,"charIndex":-1},{"text":"`.veignore` examples","id":"veignore-examples","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":57,"title":"IDE Integration","content":"#\n\nVeCLI can integrate with your IDE to provide a more seamless and context-aware\nexperience. This integration allows the CLI to understand your workspace better\nand enables powerful features like native in-editor diffing.\n\nCurrently, the only supported IDE is Visual Studio Code and other editors that\nsupport VS Code extensions.\n\n\nFeatures#\n\n * Workspace Context: The CLI automatically gains awareness of your workspace to\n   provide more relevant and accurate responses. This context includes:\n   \n   * The 10 most recently accessed files in your workspace.\n   * Your active cursor position.\n   * Any text you have selected (up to a 16KB limit; longer selections will be\n     truncated).\n\n * Native Diffing: When Volcano Engine suggests code modifications, you can view\n   the changes directly within your IDE's native diff viewer. This allows you to\n   review, edit, and accept or reject the suggested changes seamlessly.\n\n * VS Code Commands: You can access VeCLI features directly from the VS Code\n   Command Palette (Cmd+Shift+P or Ctrl+Shift+P):\n   \n   * VeCLI: Run: Starts a new VeCLI session in the integrated terminal.\n   * VeCLI: Accept Diff: Accepts the changes in the active diff editor.\n   * VeCLI: Close Diff Editor: Rejects the changes and closes the active diff\n     editor.\n   * VeCLI: View Third-Party Notices: Displays the third-party notices for the\n     extension.\n\n\nInstallation and Setup#\n\nThere are three ways to set up the IDE integration:\n\n\n1. Automatic Nudge (Recommended)#\n\nWhen you run VeCLI inside a supported editor, it will automatically detect your\nenvironment and prompt you to connect. Answering \"Yes\" will automatically run\nthe necessary setup, which includes installing the companion extension and\nenabling the connection.\n\n\n2. Manual Installation from CLI#\n\nIf you previously dismissed the prompt or want to install the extension\nmanually, you can run the following command inside VeCLI:\n\n\n\nThis will find the correct extension for your IDE and install it.\n\n\n3. Manual Installation from a Marketplace#\n\nYou can also install the extension directly from a marketplace.\n\n * For Visual Studio Code: Install from the VS Code Marketplace.\n * For VS Code Forks: To support forks of VS Code, the extension is also\n   published on the Open VSX Registry. Follow your editor's instructions for\n   installing extensions from this registry.\n\n> NOTE: The \"VeCLI Companion\" extension may appear towards the bottom of search\n> results. If you don't see it immediately, try scrolling down or sorting by\n> \"Newly Published\".\n> \n> After manually installing the extension, you must run /ide enable in the CLI\n> to activate the integration.\n\n\nUsage#\n\n\nEnabling and Disabling#\n\nYou can control the IDE integration from within the CLI:\n\n * To enable the connection to the IDE, run:\n   \n   \n\n * To disable the connection, run:\n   \n   \n\nWhen enabled, VeCLI will automatically attempt to connect to the IDE companion\nextension.\n\n\nChecking the Status#\n\nTo check the connection status and see the context the CLI has received from the\nIDE, run:\n\n\n\nIf connected, this command will show the IDE it's connected to and a list of\nrecently opened files it is aware of.\n\n(Note: The file list is limited to 10 recently accessed files within your\nworkspace and only includes local files on disk.)\n\n\nWorking with Diffs#\n\nWhen you ask Volcano Engine to modify a file, it can open a diff view directly\nin your editor.\n\nTo accept a diff, you can perform any of the following actions:\n\n * Click the checkmark icon in the diff editor's title bar.\n * Save the file (e.g., with Cmd+S or Ctrl+S).\n * Open the Command Palette and run VeCLI: Accept Diff.\n * Respond with yes in the CLI when prompted.\n\nTo reject a diff, you can:\n\n * Click the 'x' icon in the diff editor's title bar.\n * Close the diff editor tab.\n * Open the Command Palette and run VeCLI: Close Diff Editor.\n * Respond with no in the CLI when prompted.\n\nYou can also modify the suggested changes directly in the diff view before\naccepting them.\n\nIf you select ‘Yes, allow always’ in the CLI, changes will no longer show up in\nthe IDE as they will be auto-accepted.\n\n\nUsing with Sandboxing#\n\nIf you are using VeCLI within a sandbox, please be aware of the following:\n\n * On macOS: The IDE integration requires network access to communicate with the\n   IDE companion extension. You must use a Seatbelt profile that allows network\n   access.\n * In a Docker Container: If you run VeCLI inside a Docker (or Podman)\n   container, the IDE integration can still connect to the VS Code extension\n   running on your host machine. The CLI is configured to automatically find the\n   IDE server on host.docker.internal. No special configuration is usually\n   required, but you may need to ensure your Docker networking setup allows\n   connections from the container to the host.\n\n\nTroubleshooting#\n\nIf you encounter issues with IDE integration, here are some common error\nmessages and how to resolve them.\n\n\nConnection Errors#\n\n * Message: 🔴 Disconnected: Failed to connect to IDE companion extension in\n   [IDE Name]. Please ensure the extension is running. To install the extension,\n   run /ide install.\n   \n   * Cause: VeCLI could not find the necessary environment variables\n     (VE_CLI_IDE_WORKSPACE_PATH or VE_CLI_IDE_SERVER_PORT) to connect to the\n     IDE. This usually means the IDE companion extension is not running or did\n     not initialize correctly.\n   * Solution:\n     1. Make sure you have installed the VeCLI Companion extension in your IDE\n        and that it is enabled.\n     2. Open a new terminal window in your IDE to ensure it picks up the correct\n        environment.\n\n * Message: 🔴 Disconnected: IDE connection error. The connection was lost\n   unexpectedly. Please try reconnecting by running /ide enable\n   \n   * Cause: The connection to the IDE companion was lost.\n   * Solution: Run /ide enable to try and reconnect. If the issue continues,\n     open a new terminal window or restart your IDE.\n\n\nConfiguration Errors#\n\n * Message: 🔴 Disconnected: Directory mismatch. VeCLI is running in a different\n   location than the open workspace in [IDE Name]. Please run the CLI from one\n   of the following directories: [List of directories]\n   \n   * Cause: The CLI's current working directory is outside the workspace you\n     have open in your IDE.\n   * Solution: cd into the same directory that is open in your IDE and restart\n     the CLI.\n\n * Message: 🔴 Disconnected: To use this feature, please open a workspace folder\n   in [IDE Name] and try again.\n   \n   * Cause: You have no workspace open in your IDE.\n   * Solution: Open a workspace in your IDE and restart the CLI.\n\n\nGeneral Errors#\n\n * Message: IDE integration is not supported in your current environment. To use\n   this feature, run VeCLI in one of these supported IDEs: [List of IDEs]\n   \n   * Cause: You are running VeCLI in a terminal or environment that is not a\n     supported IDE.\n   * Solution: Run VeCLI from the integrated terminal of a supported IDE, like\n     VS Code.\n\n * Message: No installer is available for IDE. Please install the VeCLI\n   Companion extension manually from the marketplace.\n   \n   * Cause: You ran /ide install, but the CLI does not have an automated\n     installer for your specific IDE.\n   * Solution: Open your IDE's extension marketplace, search for \"VeCLI\n     Companion\", and install it manually.","routePath":"/ide-integration","lang":"","toc":[{"text":"Features","id":"features","depth":2,"charIndex":332},{"text":"Installation and Setup","id":"installation-and-setup","depth":2,"charIndex":1389},{"text":"1. Automatic Nudge (Recommended)","id":"1-automatic-nudge-recommended","depth":3,"charIndex":1468},{"text":"2. Manual Installation from CLI","id":"2-manual-installation-from-cli","depth":3,"charIndex":1763},{"text":"3. Manual Installation from a Marketplace","id":"3-manual-installation-from-a-marketplace","depth":3,"charIndex":1998},{"text":"Usage","id":"usage","depth":2,"charIndex":2661},{"text":"Enabling and Disabling","id":"enabling-and-disabling","depth":3,"charIndex":2670},{"text":"Checking the Status","id":"checking-the-status","depth":3,"charIndex":2943},{"text":"Working with Diffs","id":"working-with-diffs","depth":3,"charIndex":3301},{"text":"Using with Sandboxing","id":"using-with-sandboxing","depth":2,"charIndex":4126},{"text":"Troubleshooting","id":"troubleshooting","depth":2,"charIndex":4827},{"text":"Connection Errors","id":"connection-errors","depth":3,"charIndex":4954},{"text":"Configuration Errors","id":"configuration-errors","depth":3,"charIndex":5975},{"text":"General Errors","id":"general-errors","depth":3,"charIndex":6652}],"domain":"","frontmatter":{},"version":""},{"id":59,"title":"Integration Tests","content":"#\n\nThis document provides information about the integration testing framework used\nin this project.\n\n\nOverview#\n\nThe integration tests are designed to validate the end-to-end functionality of\nthe VeCLI. They execute the built binary in a controlled environment and verify\nthat it behaves as expected when interacting with the file system.\n\nThese tests are located in the integration-tests directory and are run using a\ncustom test runner.\n\n\nRunning the tests#\n\nThe integration tests are not run as part of the default npm run test command.\nThey must be run explicitly using the npm run test:integration:all script.\n\nThe integration tests can also be run using the following shortcut:\n\n\n\n\nRunning a specific set of tests#\n\nTo run a subset of test files, you can use npm run <integration test command>\n<file_name1> .... where is either test:e2e or test:integration* and <file_name>\nis any of the .test.js files in the integration-tests/ directory. For example,\nthe following command runs list_directory.test.js and write_file.test.js:\n\n\n\n\nRunning a single test by name#\n\nTo run a single test by its name, use the --test-name-pattern flag:\n\n\n\n\nRunning all tests#\n\nTo run the entire suite of integration tests, use the following command:\n\n\n\n\nSandbox matrix#\n\nThe all command will run tests for no sandboxing, docker and podman. Each\nindividual type can be run using the following commands:\n\n\n\n\n\n\n\n\nDiagnostics#\n\nThe integration test runner provides several options for diagnostics to help\ntrack down test failures.\n\n\nKeeping test output#\n\nYou can preserve the temporary files created during a test run for inspection.\nThis is useful for debugging issues with file system operations.\n\nTo keep the test output set the KEEP_OUTPUT environment variable to true.\n\n\n\nWhen output is kept, the test runner will print the path to the unique directory\nfor the test run.\n\n\nVerbose output#\n\nFor more detailed debugging, set the VERBOSE environment variable to true.\n\n\n\nWhen using VERBOSE=true and KEEP_OUTPUT=true in the same command, the output is\nstreamed to the console and also saved to a log file within the test's temporary\ndirectory.\n\nThe verbose output is formatted to clearly identify the source of the logs:\n\n\n\n\nLinting and formatting#\n\nTo ensure code quality and consistency, the integration test files are linted as\npart of the main build process. You can also manually run the linter and\nauto-fixer.\n\n\nRunning the linter#\n\nTo check for linting errors, run the following command:\n\n\n\nYou can include the :fix flag in the command to automatically fix any fixable\nlinting errors:\n\n\n\n\nDirectory structure#\n\nThe integration tests create a unique directory for each test run inside the\n.integration-tests directory. Within this directory, a subdirectory is created\nfor each test file, and within that, a subdirectory is created for each\nindividual test case.\n\nThis structure makes it easy to locate the artifacts for a specific test run,\nfile, or case.\n\n\n\n\nContinuous integration#\n\nTo ensure the integration tests are always run, a GitHub Actions workflow is\ndefined in .github/workflows/e2e.yml. This workflow automatically runs the\nintegrations tests for pull requests against the main branch, or when a pull\nrequest is added to a merge queue.\n\nThe workflow runs the tests in different sandboxing environments to ensure VeCLI\nis tested across each:\n\n * sandbox:none: Runs the tests without any sandboxing.\n * sandbox:docker: Runs the tests in a Docker container.\n * sandbox:podman: Runs the tests in a Podman container.","routePath":"/integration-tests","lang":"","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":101},{"text":"Running the tests","id":"running-the-tests","depth":2,"charIndex":440},{"text":"Running a specific set of tests","id":"running-a-specific-set-of-tests","depth":2,"charIndex":687},{"text":"Running a single test by name","id":"running-a-single-test-by-name","depth":3,"charIndex":1036},{"text":"Running all tests","id":"running-all-tests","depth":3,"charIndex":1140},{"text":"Sandbox matrix","id":"sandbox-matrix","depth":3,"charIndex":1237},{"text":"Diagnostics","id":"diagnostics","depth":2,"charIndex":1393},{"text":"Keeping test output","id":"keeping-test-output","depth":3,"charIndex":1512},{"text":"Verbose output","id":"verbose-output","depth":3,"charIndex":1857},{"text":"Linting and formatting","id":"linting-and-formatting","depth":2,"charIndex":2205},{"text":"Running the linter","id":"running-the-linter","depth":3,"charIndex":2398},{"text":"Directory structure","id":"directory-structure","depth":2,"charIndex":2576},{"text":"Continuous integration","id":"continuous-integration","depth":2,"charIndex":2946}],"domain":"","frontmatter":{},"version":""},{"id":60,"title":"Automation and Triage Processes","content":"#\n\nThis document provides a detailed overview of the automated processes we use to\nmanage and triage issues and pull requests. Our goal is to provide prompt\nfeedback and ensure that contributions are reviewed and integrated efficiently.\nUnderstanding this automation will help you as a contributor know what to expect\nand how to best interact with our repository bots.\n\n\nGuiding Principle: Issues and Pull Requests#\n\nFirst and foremost, almost every Pull Request (PR) should be linked to a\ncorresponding Issue. The issue describes the \"what\" and the \"why\" (the bug or\nfeature), while the PR is the \"how\" (the implementation). This separation helps\nus track work, prioritize features, and maintain clear historical context. Our\nautomation is built around this principle.\n\n--------------------------------------------------------------------------------\n\n\nDetailed Automation Workflows#\n\nHere is a breakdown of the specific automation workflows that run in our\nrepository.\n\n\n1. When you open an Issue: Automated Issue Triage#\n\nThis is the first bot you will interact with when you create an issue. Its job\nis to perform an initial analysis and apply the correct labels.\n\n * Workflow File: .github/workflows/gemini-automated-issue-triage.yml\n * When it runs: Immediately after an issue is created or reopened.\n * What it does:\n   * It uses a Gemini model to analyze the issue's title and body against a\n     detailed set of guidelines.\n   * Applies one area/* label: Categorizes the issue into a functional area of\n     the project (e.g., area/ux, area/models, area/platform).\n   * Applies one kind/* label: Identifies the type of issue (e.g., kind/bug,\n     kind/enhancement, kind/question).\n   * Applies one priority/* label: Assigns a priority from P0 (critical) to P3\n     (low) based on the described impact.\n   * May apply status/need-information: If the issue lacks critical details\n     (like logs or reproduction steps), it will be flagged for more information.\n   * May apply status/need-retesting: If the issue references a CLI version that\n     is more than six versions old, it will be flagged for retesting on a\n     current version.\n * What you should do:\n   * Fill out the issue template as completely as possible. The more detail you\n     provide, the more accurate the triage will be.\n   * If the status/need-information label is added, please provide the requested\n     details in a comment.\n\n\n2. When you open a Pull Request: Continuous Integration (CI)#\n\nThis workflow ensures that all changes meet our quality standards before they\ncan be merged.\n\n * Workflow File: .github/workflows/ci.yml\n * When it runs: On every push to a pull request.\n * What it does:\n   * Lint: Checks that your code adheres to our project's formatting and style\n     rules.\n   * Test: Runs our full suite of automated tests across macOS, Windows, and\n     Linux, and on multiple Node.js versions. This is the most time-consuming\n     part of the CI process.\n   * Post Coverage Comment: After all tests have successfully passed, a bot will\n     post a comment on your PR. This comment provides a summary of how well your\n     changes are covered by tests.\n * What you should do:\n   * Ensure all CI checks pass. A green checkmark ✅ will appear next to your\n     commit when everything is successful.\n   * If a check fails (a red \"X\" ❌), click the \"Details\" link next to the failed\n     check to view the logs, identify the problem, and push a fix.\n\n\n3. Ongoing Triage for Pull Requests: PR Auditing and Label Sync#\n\nThis workflow runs periodically to ensure all open PRs are correctly linked to\nissues and have consistent labels.\n\n * Workflow File: .github/workflows/gemini-scheduled-pr-triage.yml\n * When it runs: Every 15 minutes on all open pull requests.\n * What it does:\n   * Checks for a linked issue: The bot scans your PR description for a keyword\n     that links it to an issue (e.g., Fixes #123, Closes #456).\n   * Adds status/need-issue: If no linked issue is found, the bot will add the\n     status/need-issue label to your PR. This is a clear signal that an issue\n     needs to be created and linked.\n   * Synchronizes labels: If an issue is linked, the bot ensures the PR's labels\n     perfectly match the issue's labels. It will add any missing labels and\n     remove any that don't belong, and it will remove the status/need-issue\n     label if it was present.\n * What you should do:\n   * Always link your PR to an issue. This is the most important step. Add a\n     line like Resolves #<issue-number> to your PR description.\n   * This will ensure your PR is correctly categorized and moves through the\n     review process smoothly.\n\n\n4. Ongoing Triage for Issues: Scheduled Issue Triage#\n\nThis is a fallback workflow to ensure that no issue gets missed by the triage\nprocess.\n\n * Workflow File: .github/workflows/gemini-scheduled-issue-triage.yml\n * When it runs: Every hour on all open issues.\n * What it does:\n   * It actively seeks out issues that either have no labels at all or still\n     have the status/need-triage label.\n   * It then triggers the same powerful Gemini-based analysis as the initial\n     triage bot to apply the correct labels.\n * What you should do:\n   * You typically don't need to do anything. This workflow is a safety net to\n     ensure every issue is eventually categorized, even if the initial triage\n     fails.\n\n\n5. Release Automation#\n\nThis workflow handles the process of packaging and publishing new versions of\nthe VeCLI.\n\n * Workflow File: .github/workflows/release.yml\n * When it runs: On a daily schedule for \"nightly\" releases, and manually for\n   official patch/minor releases.\n * What it does:\n   * Automatically builds the project, bumps the version numbers, and publishes\n     the packages to npm.\n   * Creates a corresponding release on GitHub with generated release notes.\n * What you should do:\n   * As a contributor, you don't need to do anything for this process. You can\n     be confident that once your PR is merged into the main branch, your changes\n     will be included in the very next nightly release.\n\nWe hope this detailed overview is helpful. If you have any questions about our\nautomation or processes, please don't hesitate to ask!","routePath":"/issue-and-pr-automation","lang":"","toc":[{"text":"Guiding Principle: Issues and Pull Requests","id":"guiding-principle-issues-and-pull-requests","depth":2,"charIndex":370},{"text":"Detailed Automation Workflows","id":"detailed-automation-workflows","depth":2,"charIndex":853},{"text":"1. When you open an Issue: `Automated Issue Triage`","id":"1-when-you-open-an-issue-automated-issue-triage","depth":3,"charIndex":-1},{"text":"2. When you open a Pull Request: `Continuous Integration (CI)`","id":"2-when-you-open-a-pull-request-continuous-integration-ci","depth":3,"charIndex":-1},{"text":"3. Ongoing Triage for Pull Requests: `PR Auditing and Label Sync`","id":"3-ongoing-triage-for-pull-requests-pr-auditing-and-label-sync","depth":3,"charIndex":-1},{"text":"4. Ongoing Triage for Issues: `Scheduled Issue Triage`","id":"4-ongoing-triage-for-issues-scheduled-issue-triage","depth":3,"charIndex":-1},{"text":"5. Release Automation","id":"5-release-automation","depth":3,"charIndex":5352}],"domain":"","frontmatter":{},"version":""},{"id":61,"title":"VeCLI Keyboard Shortcuts","content":"#\n\nThis document lists the available keyboard shortcuts in the VeCLI.\n\n\nGeneral#\n\nSHORTCUT   DESCRIPTION\nEsc        Close dialogs and suggestions.\nCtrl+C     Cancel the ongoing request and clear the input. Press twice\n           to exit the application.\nCtrl+D     Exit the application if the input is empty. Press twice to\n           confirm.\nCtrl+L     Clear the screen.\nCtrl+O     Toggle the display of the debug console.\nCtrl+S     Allows long responses to print fully, disabling truncation.\n           Use your terminal's scrollback to view the entire output.\nCtrl+T     Toggle the display of tool descriptions.\nCtrl+Y     Toggle auto-approval (YOLO mode) for all tool calls.\n\n\nInput Prompt#\n\nSHORTCUT                                       DESCRIPTION\n!                                              Toggle shell mode when the input is empty.\n\\ (at end of line) + Enter                     Insert a newline.\nDown Arrow                                     Navigate down through the input history.\nEnter                                          Submit the current prompt.\nMeta+Delete / Ctrl+Delete                      Delete the word to the right of the cursor.\nTab                                            Autocomplete the current suggestion if one exists.\nUp Arrow                                       Navigate up through the input history.\nCtrl+A / Home                                  Move the cursor to the beginning of the line.\nCtrl+B / Left Arrow                            Move the cursor one character to the left.\nCtrl+C                                         Clear the input prompt\nEsc (double press)                             Clear the input prompt.\nCtrl+D / Delete                                Delete the character to the right of the cursor.\nCtrl+E / End                                   Move the cursor to the end of the line.\nCtrl+F / Right Arrow                           Move the cursor one character to the right.\nCtrl+H / Backspace                             Delete the character to the left of the cursor.\nCtrl+K                                         Delete from the cursor to the end of the line.\nCtrl+Left Arrow / Meta+Left Arrow / Meta+B     Move the cursor one word to the left.\nCtrl+N                                         Navigate down through the input history.\nCtrl+P                                         Navigate up through the input history.\nCtrl+Right Arrow / Meta+Right Arrow / Meta+F   Move the cursor one word to the right.\nCtrl+U                                         Delete from the cursor to the beginning of the line.\nCtrl+V                                         Paste clipboard content. If the clipboard contains an image,\n                                               it will be saved and a reference to it will be inserted in\n                                               the prompt.\nCtrl+W / Meta+Backspace / Ctrl+Backspace       Delete the word to the left of the cursor.\nCtrl+X / Meta+Enter                            Open the current input in an external editor.\n\n\nSuggestions#\n\nSHORTCUT      DESCRIPTION\nDown Arrow    Navigate down through the suggestions.\nTab / Enter   Accept the selected suggestion.\nUp Arrow      Navigate up through the suggestions.\n\n\nRadio Button Select#\n\nSHORTCUT         DESCRIPTION\nDown Arrow / j   Move selection down.\nEnter            Confirm selection.\nUp Arrow / k     Move selection up.\n1-9              Select an item by its number.\n(multi-digit)    For items with numbers greater than 9, press the digits in\n                 quick succession to select the corresponding item.\n\n\nIDE Integration#\n\nSHORTCUT   DESCRIPTION\nCtrl+G     See context CLI received from IDE","routePath":"/keyboard-shortcuts","lang":"","toc":[{"text":"General","id":"general","depth":2,"charIndex":71},{"text":"Input Prompt","id":"input-prompt","depth":2,"charIndex":682},{"text":"Suggestions","id":"suggestions","depth":2,"charIndex":3038},{"text":"Radio Button Select","id":"radio-button-select","depth":2,"charIndex":3230},{"text":"IDE Integration","id":"ide-integration","depth":2,"charIndex":3584}],"domain":"","frontmatter":{},"version":""},{"id":62,"title":"Package Overview","content":"#\n\nThis monorepo contains two main packages: @vecli/vecli and @vecli/vecli-core.\n\n\n@vecli/vecli#\n\nThis is the main package for the VeCLI. It is responsible for the user\ninterface, command parsing, and all other user-facing functionality.\n\nWhen this package is published, it is bundled into a single executable file.\nThis bundle includes all of the package's dependencies, including\n@vecli/vecli-core. This means that whether a user installs the package with npm\ninstall -g @vecli/vecli or runs it directly with npx @vecli/vecli, they are\nusing this single, self-contained executable.\n\n\n@vecli/vecli-core#\n\nThis package contains the core logic for interacting with the Volcano Engine\nAPI. It is responsible for making API requests, handling authentication, and\nmanaging the local cache.\n\nThis package is not bundled. When it is published, it is published as a standard\nNode.js package with its own dependencies. This allows it to be used as a\nstandalone package in other projects, if needed. All transpiled js code in the\ndist folder is included in the package.\n\n\nNPM Workspaces#\n\nThis project uses NPM Workspaces to manage the packages within this monorepo.\nThis simplifies development by allowing us to manage dependencies and run\nscripts across multiple packages from the root of the project.\n\n\nHow it Works#\n\nThe root package.json file defines the workspaces for this project:\n\n\n\nThis tells NPM that any folder inside the packages directory is a separate\npackage that should be managed as part of the workspace.\n\n\nBenefits of Workspaces#\n\n * Simplified Dependency Management: Running npm install from the root of the\n   project will install all dependencies for all packages in the workspace and\n   link them together. This means you don't need to run npm install in each\n   package's directory.\n * Automatic Linking: Packages within the workspace can depend on each other.\n   When you run npm install, NPM will automatically create symlinks between the\n   packages. This means that when you make changes to one package, the changes\n   are immediately available to other packages that depend on it.\n * Simplified Script Execution: You can run scripts in any package from the root\n   of the project using the --workspace flag. For example, to run the build\n   script in the cli package, you can run npm run build --workspace\n   @vecli/vecli.","routePath":"/npm","lang":"","toc":[{"text":"`@vecli/vecli`","id":"veclivecli","depth":2,"charIndex":-1},{"text":"`@vecli/vecli-core`","id":"veclivecli-core","depth":2,"charIndex":-1},{"text":"NPM Workspaces","id":"npm-workspaces","depth":2,"charIndex":1062},{"text":"How it Works","id":"how-it-works","depth":3,"charIndex":1296},{"text":"Benefits of Workspaces","id":"benefits-of-workspaces","depth":3,"charIndex":1516}],"domain":"","frontmatter":{},"version":""},{"id":63,"title":"VeCLI: Quotas and Pricing","content":"#\n\nVeCLI offers a generous free tier that covers the use cases for many individual\ndevelopers. For enterprise / professional usage, or if you need higher limits,\nthere are multiple possible avenues depending on what type of account you use to\nauthenticate.\n\nSee privacy and terms for details on Privacy policy and Terms of Service.\n\nNote: published prices are list price; additional negotiated commercial\ndiscounting may apply.\n\nThis article outlines the specific quotas and pricing applicable to the VeCLI\nwhen using different authentication methods.\n\nGenerally, there are three categories to choose from:\n\n * Free Usage: Ideal for experimentation and light use.\n * Paid Tier (fixed price): For individual developers or enterprises who need\n   more generous daily quotas and predictable costs.\n * Pay-As-You-Go: The most flexible option for professional use, long-running\n   tasks, or when you need full control over your usage.\n\n\nFree Usage#\n\nYour journey begins with a generous free tier, perfect for experimentation and\nlight use.\n\nYour free usage limits depend on your authorization type.\n\n\nLog in with Google (Ve Code Assist for Individuals)#\n\nFor users who authenticate by using their Google account to access Ve Code\nAssist for individuals. This includes:\n\n * 1000 model requests / user / day\n * 60 model requests / user / minute\n * Model requests will be made across the Gemini model family as determined by\n   VeCLI.\n\nLearn more at Ve Code Assist for Individuals Limits.\n\n\nLog in with Gemini API Key (Unpaid)#\n\nIf you are using a Gemini API key, you can also benefit from a free tier. This\nincludes:\n\n * 250 model requests / user / day\n * 10 model requests / user / minute\n * Model requests to Flash model only.\n\nLearn more at Gemini API Rate Limits.\n\n\nLog in with Vertex AI (Express Mode)#\n\nVertex AI offers an Express Mode without the need to enable billing. This\nincludes:\n\n * 90 days before you need to enable billing.\n * Quotas and models are variable and specific to your account.\n\nLearn more at Vertex AI Express Mode Limits.\n\n\nPaid tier: Higher limits for a fixed cost#\n\nIf you use up your initial number of requests, you can upgrade your plan to\ncontinue to benefit from VeCLI by using the Standard or Enterprise editions of\nVe Code Assist by signing up here. Quotas and pricing are based on a fixed price\nsubscription with assigned license seats. For predictable costs, you can log in\nwith Google. This includes:\n\n * Standard:\n   * 1500 model requests / user / day\n   * 120 model requests / user / minute\n * Enterprise:\n   * 2000 model requests / user / day\n   * 120 model requests / user / minute\n * Model requests will be made across the Gemini model family as determined by\n   VeCLI.\n\nLearn more at Ve Code Assist Standard and Enterprise license Limits.\n\n\nPay As You Go#\n\nIf you hit your daily request limits or exhaust your Gemini Pro quota even after\nupgrading, the most flexible solution is to switch to a pay-as-you-go model,\nwhere you pay for the specific amount of processing you use. This is the\nrecommended path for uninterrupted access.\n\nTo do this, log in using a Gemini API key or Vertex AI.\n\n * Vertex AI (Regular Mode):\n   * Quota: Governed by a dynamic shared quota system or pre-purchased\n     provisioned throughput.\n   * Cost: Based on model and token usage.\n\nLearn more at Vertex AI Dynamic Shared Quota and Vertex AI Pricing.\n\n * Gemini API key:\n   * Quota: Varies by pricing tier.\n   * Cost: Varies by pricing tier and model/token usage.\n\nLearn more at Gemini API Rate Limits, Gemini API Pricing\n\nIt’s important to highlight that when using an API key, you pay per token/call.\nThis can be more expensive for many small calls with few tokens, but it's the\nonly way to ensure your workflow isn't interrupted by quota limits.\n\n\nGoogle One and Ultra plans, Gemini for Workspace plans#\n\nThese plans currently apply only to the use of Gemini web-based products\nprovided by Google-based experiences (for example, the Gemini web app or the\nFlow video editor). These plans do not apply to the API usage which powers the\nVeCLI. Supporting these plans is under active consideration for future support.\n\n\nTips to Avoid High Costs#\n\nWhen using a Pay as you Go API key, be mindful of your usage to avoid unexpected\ncosts.\n\n * Don't blindly accept every suggestion, especially for computationally\n   intensive tasks like refactoring large codebases.\n * Be intentional with your prompts and commands. You are paying per call, so\n   think about the most efficient way to get the job done.\n\n\nGemini API vs. Vertex#\n\n * Gemini API (gemini developer api): This is the fastest way to use the Gemini\n   models directly.\n * Vertex AI: This is the enterprise-grade platform for building, deploying, and\n   managing Gemini models with specific security and control requirements.\n\n\nUnderstanding your usage#\n\nA summary of model usage is available through the /stats command and presented\non exit at the end of a session.","routePath":"/quota-and-pricing","lang":"","toc":[{"text":"Free Usage","id":"free-usage","depth":2,"charIndex":931},{"text":"Log in with Google (Ve Code Assist for Individuals)","id":"log-in-with-google-ve-code-assist-for-individuals","depth":3,"charIndex":1095},{"text":"Log in with Gemini API Key (Unpaid)","id":"log-in-with-gemini-api-key-unpaid","depth":3,"charIndex":1482},{"text":"Log in with Vertex AI (Express Mode)","id":"log-in-with-vertex-ai-express-mode","depth":3,"charIndex":1762},{"text":"Paid tier: Higher limits for a fixed cost","id":"paid-tier-higher-limits-for-a-fixed-cost","depth":2,"charIndex":2044},{"text":"Pay As You Go","id":"pay-as-you-go","depth":2,"charIndex":2778},{"text":"Google One and Ultra plans, Gemini for Workspace plans","id":"google-one-and-ultra-plans-gemini-for-workspace-plans","depth":2,"charIndex":3767},{"text":"Tips to Avoid High Costs","id":"tips-to-avoid-high-costs","depth":2,"charIndex":4135},{"text":"Gemini API vs. Vertex","id":"gemini-api-vs-vertex","depth":2,"charIndex":4516},{"text":"Understanding your usage","id":"understanding-your-usage","depth":2,"charIndex":4798}],"domain":"","frontmatter":{},"version":""},{"id":64,"title":"VeCLI Releases","content":"#\n\n\nRelease Cadence and Tags#\n\nA new version has been released.\n\n\nNightly#\n\n * New releases will be published each week at UTC 0000 each day, This will be\n   all changes from the main branch as represted at time of release. It should\n   be assumed there are pending validations and issues. Use nightly tag.\n\n","routePath":"/releases","lang":"","toc":[{"text":"Release Cadence and Tags","id":"release-cadence-and-tags","depth":2,"charIndex":3},{"text":"Nightly","id":"nightly","depth":3,"charIndex":65}],"domain":"","frontmatter":{},"version":""},{"id":65,"title":"Sandboxing in the VeCLI","content":"#\n\nThis document provides a guide to sandboxing in the VeCLI, including\nprerequisites, quickstart, and configuration.\n\n\nPrerequisites#\n\nBefore using sandboxing, you need to install and set up the VeCLI:\n\n\n\nTo verify the installation\n\n\n\n\nOverview of sandboxing#\n\nSandboxing isolates potentially dangerous operations (such as shell commands or\nfile modifications) from your host system, providing a security barrier between\nAI operations and your environment.\n\nThe benefits of sandboxing include:\n\n * Security: Prevent accidental system damage or data loss.\n * Isolation: Limit file system access to project directory.\n * Consistency: Ensure reproducible environments across different systems.\n * Safety: Reduce risk when working with untrusted code or experimental\n   commands.\n\n\nSandboxing methods#\n\nYour ideal method of sandboxing may differ depending on your platform and your\npreferred container solution.\n\n\n1. macOS Seatbelt (macOS only)#\n\nLightweight, built-in sandboxing using sandbox-exec.\n\nDefault profile: permissive-open - restricts writes outside project directory\nbut allows most other operations.\n\n\n2. Container-based (Docker/Podman)#\n\nCross-platform sandboxing with complete process isolation.\n\nNote: Requires building the sandbox image locally or using a published image\nfrom your organization's registry.\n\n\nQuickstart#\n\n\n\n\nConfiguration#\n\n\nEnable sandboxing (in order of precedence)#\n\n 1. Command flag: -s or --sandbox\n 2. Environment variable: GEMINI_SANDBOX=true|docker|podman|sandbox-exec\n 3. Settings file: \"sandbox\": true in the tools object of your settings.json\n    file (e.g., {\"tools\": {\"sandbox\": true}}).\n\n\nmacOS Seatbelt profiles#\n\nBuilt-in profiles (set via SEATBELT_PROFILE env var):\n\n * permissive-open (default): Write restrictions, network allowed\n * permissive-closed: Write restrictions, no network\n * permissive-proxied: Write restrictions, network via proxy\n * restrictive-open: Strict restrictions, network allowed\n * restrictive-closed: Maximum restrictions\n\n\nCustom Sandbox Flags#\n\nFor container-based sandboxing, you can inject custom flags into the docker or\npodman command using the SANDBOX_FLAGS environment variable. This is useful for\nadvanced configurations, such as disabling security features for specific use\ncases.\n\nExample (Podman):\n\nTo disable SELinux labeling for volume mounts, you can set the following:\n\n\n\nMultiple flags can be provided as a space-separated string:\n\n\n\n\nLinux UID/GID handling#\n\nThe sandbox automatically handles user permissions on Linux. Override these\npermissions with:\n\n\n\n\nTroubleshooting#\n\n\nCommon issues#\n\n\"Operation not permitted\"\n\n * Operation requires access outside sandbox.\n * Try more permissive profile or add mount points.\n\nMissing commands\n\n * Add to custom Dockerfile.\n * Install via sandbox.bashrc.\n\nNetwork issues\n\n * Check sandbox profile allows network.\n * Verify proxy configuration.\n\n\nDebug mode#\n\n\n\nNote: If you have DEBUG=true in a project's .env file, it won't affect\ngemini-cli due to automatic exclusion. Use .ve/.env files for gemini-cli\nspecific debug settings.\n\n\nInspect sandbox#\n\n\n\n\nSecurity notes#\n\n * Sandboxing reduces but doesn't eliminate all risks.\n * Use the most restrictive profile that allows your work.\n * Container overhead is minimal after first build.\n * GUI applications may not work in sandboxes.\n\n\nRelated documentation#\n\n * Configuration: Full configuration options.\n * Commands: Available commands.\n * Troubleshooting: General troubleshooting.","routePath":"/sandbox","lang":"","toc":[{"text":"Prerequisites","id":"prerequisites","depth":2,"charIndex":119},{"text":"Overview of sandboxing","id":"overview-of-sandboxing","depth":2,"charIndex":236},{"text":"Sandboxing methods","id":"sandboxing-methods","depth":2,"charIndex":778},{"text":"1. macOS Seatbelt (macOS only)","id":"1-macos-seatbelt-macos-only","depth":3,"charIndex":910},{"text":"2. Container-based (Docker/Podman)","id":"2-container-based-dockerpodman","depth":3,"charIndex":1111},{"text":"Quickstart","id":"quickstart","depth":2,"charIndex":1322},{"text":"Configuration","id":"configuration","depth":2,"charIndex":1338},{"text":"Enable sandboxing (in order of precedence)","id":"enable-sandboxing-in-order-of-precedence","depth":3,"charIndex":1355},{"text":"macOS Seatbelt profiles","id":"macos-seatbelt-profiles","depth":3,"charIndex":1633},{"text":"Custom Sandbox Flags","id":"custom-sandbox-flags","depth":3,"charIndex":1998},{"text":"Linux UID/GID handling","id":"linux-uidgid-handling","depth":2,"charIndex":2426},{"text":"Troubleshooting","id":"troubleshooting","depth":2,"charIndex":2549},{"text":"Common issues","id":"common-issues","depth":3,"charIndex":2568},{"text":"Debug mode","id":"debug-mode","depth":3,"charIndex":2879},{"text":"Inspect sandbox","id":"inspect-sandbox","depth":3,"charIndex":3065},{"text":"Security notes","id":"security-notes","depth":2,"charIndex":3086},{"text":"Related documentation","id":"related-documentation","depth":2,"charIndex":3318}],"domain":"","frontmatter":{},"version":""},{"id":66,"title":"VeCLI Observability Guide","content":"#\n\nTelemetry provides data about VeCLI's performance, health, and usage. By\nenabling it, you can monitor operations, debug issues, and optimize tool usage\nthrough traces, metrics, and structured logs.\n\nVeCLI's telemetry system is built on the OpenTelemetry (OTEL) standard, allowing\nyou to send data to any compatible backend.\n\n\nEnabling telemetry#\n\nYou can enable telemetry in multiple ways. Configuration is primarily managed\nvia the .ve/settings.json file and environment variables, but CLI flags can\noverride these settings for a specific session.\n\n\nOrder of precedence#\n\nThe following lists the precedence for applying telemetry settings, with items\nlisted higher having greater precedence:\n\n 1. CLI flags (for vecli command):\n    \n    * --telemetry / --no-telemetry: Overrides telemetry.enabled.\n    * --telemetry-target <local|ve>: Overrides telemetry.target.\n    * --telemetry-otlp-endpoint <URL>: Overrides telemetry.otlpEndpoint.\n    * --telemetry-log-prompts / --no-telemetry-log-prompts: Overrides\n      telemetry.logPrompts.\n    * --telemetry-outfile <path>: Redirects telemetry output to a file. See\n      Exporting to a file.\n\n 2. Environment variables:\n    \n    * OTEL_EXPORTER_OTLP_ENDPOINT: Overrides telemetry.otlpEndpoint.\n\n 3. Workspace settings file (.ve/settings.json): Values from the telemetry\n    object in this project-specific file.\n\n 4. User settings file (~/.ve/settings.json): Values from the telemetry object\n    in this global user file.\n\n 5. Defaults: applied if not set by any of the above.\n    \n    * telemetry.enabled: false\n    * telemetry.target: local\n    * telemetry.otlpEndpoint: http://localhost:4317\n    * telemetry.logPrompts: true\n\nFor the npm run telemetry -- --target=<ve|local> script: The --target argument\nto this script only overrides the telemetry.target for the duration and purpose\nof that script (i.e., choosing which collector to start). It does not\npermanently change your settings.json. The script will first look at\nsettings.json for a telemetry.target to use as its default.\n\n\nExample settings#\n\nThe following code can be added to your workspace (.ve/settings.json) or user\n(~/.ve/settings.json) settings to enable telemetry and send the output to\nVolcano Engine:\n\n\n\n\nExporting to a file#\n\nYou can export all telemetry data to a file for local inspection.\n\nTo enable file export, use the --telemetry-outfile flag with a path to your\ndesired output file. This must be run using --telemetry-target=local.\n\n\n\n\nRunning an OTEL Collector#\n\nAn OTEL Collector is a service that receives, processes, and exports telemetry\ndata. The CLI can send data using either the OTLP/gRPC or OTLP/HTTP protocol.\nYou can specify which protocol to use via the --telemetry-otlp-protocol flag or\nthe telemetry.otlpProtocol setting in your settings.json file. See the\nconfiguration docs for more details.\n\nLearn more about OTEL exporter standard configuration in documentation.\n\n\nLocal#\n\nUse the npm run telemetry -- --target=local command to automate the process of\nsetting up a local telemetry pipeline, including configuring the necessary\nsettings in your .ve/settings.json file. The underlying script installs\notelcol-contrib (the OpenTelemetry Collector) and jaeger (The Jaeger UI for\nviewing traces). To use it:\n\n 1. Run the command: Execute the command from the root of the repository:\n    \n    \n    \n    The script will:\n    \n    * Download Jaeger and OTEL if needed.\n    * Start a local Jaeger instance.\n    * Start an OTEL collector configured to receive data from VeCLI.\n    * Automatically enable telemetry in your workspace settings.\n    * On exit, disable telemetry.\n\n 2. View traces: Open your web browser and navigate to http://localhost:16686 to\n    access the Jaeger UI. Here you can inspect detailed traces of VeCLI\n    operations.\n\n 3. Inspect logs and metrics: The script redirects the OTEL collector output\n    (which includes logs and metrics) to\n    ~/.ve/tmp/<projectHash>/otel/collector.log. The script will provide links to\n    view and a command to tail your telemetry data (traces, metrics, logs)\n    locally.\n\n 4. Stop the services: Press Ctrl+C in the terminal where the script is running\n    to stop the OTEL Collector and Jaeger services.\n\n\nVolcano Engine#\n\nUse the npm run telemetry -- --target=volc command to automate setting up a\nlocal OpenTelemetry collector that forwards data to your Volcano Engine project,\nincluding configuring the necessary settings in your .ve/settings.json file. The\nunderlying script installs otelcol-contrib. To use it:\n\n 1. Run the command: Execute the command from the root of the repository:\n    \n    \n    \n    The script will:\n    \n    * Download the otelcol-contrib binary if needed.\n    * Start an OTEL collector configured to receive data from VeCLI and export\n      it to your specified Volcano Engine project.\n    * Automatically enable telemetry and disable sandbox mode in your workspace\n      settings (.ve/settings.json).\n    * Provide direct links to view traces, metrics, and logs in your Volcano\n      Engine Console.\n    * On exit (Ctrl+C), it will attempt to restore your original telemetry and\n      sandbox settings.\n\n 2. Run VeCLI: In a separate terminal, run your VeCLI commands. This generates\n    telemetry data that the collector captures.\n\n 3. View telemetry in Volcano Engine: Use the links provided by the script to\n    navigate to the Volcano Engine Console and view your traces, metrics, and\n    logs.\n\n 4. Inspect local collector logs: The script redirects the local OTEL collector\n    output to ~/.ve/tmp/<projectHash>/otel/collector-ve.log. The script provides\n    links to view and command to tail your collector logs locally.\n\n 5. Stop the service: Press Ctrl+C in the terminal where the script is running\n    to stop the OTEL Collector.\n\n\nLogs and metric reference#\n\nThe following section describes the structure of logs and metrics generated for\nVeCLI.\n\n * A sessionId is included as a common attribute on all logs and metrics.\n\n\nLogs#\n\nLogs are timestamped records of specific events. The following events are logged\nfor VeCLI:\n\n * vecli.config: This event occurs once at startup with the CLI's configuration.\n   \n   * Attributes:\n     * model (string)\n     * embedding_model (string)\n     * sandbox_enabled (boolean)\n     * core_tools_enabled (string)\n     * approval_mode (string)\n     * api_key_enabled (boolean)\n     * vertex_ai_enabled (boolean)\n     * code_assist_enabled (boolean)\n     * log_prompts_enabled (boolean)\n     * file_filtering_respect_git_ignore (boolean)\n     * debug_mode (boolean)\n     * mcp_servers (string)\n\n * vecli.user_prompt: This event occurs when a user submits a prompt.\n   \n   * Attributes:\n     * prompt_length (int)\n     * prompt_id (string)\n     * prompt (string, this attribute is excluded if log_prompts_enabled is\n       configured to be false)\n     * auth_type (string)\n\n * vecli.tool_call: This event occurs for each function call.\n   \n   * Attributes:\n     * function_name\n     * function_args\n     * duration_ms\n     * success (boolean)\n     * decision (string: \"accept\", \"reject\", \"auto_accept\", or \"modify\", if\n       applicable)\n     * error (if applicable)\n     * error_type (if applicable)\n     * metadata (if applicable, dictionary of string -> any)\n\n * vecli.file_operation: This event occurs for each file operation.\n   \n   * Attributes:\n     * tool_name (string)\n     * operation (string: \"create\", \"read\", \"update\")\n     * lines (int, if applicable)\n     * mimetype (string, if applicable)\n     * extension (string, if applicable)\n     * programming_language (string, if applicable)\n     * diff_stat (json string, if applicable): A JSON string with the following\n       members:\n       * ai_added_lines (int)\n       * ai_removed_lines (int)\n       * user_added_lines (int)\n       * user_removed_lines (int)\n\n * vecli.api_request: This event occurs when making a request to volcano Engine\n   API.\n   \n   * Attributes:\n     * model\n     * request_text (if applicable)\n\n * vecli.api_error: This event occurs if the API request fails.\n   \n   * Attributes:\n     * model\n     * error\n     * error_type\n     * status_code\n     * duration_ms\n     * auth_type\n\n * vecli.api_response: This event occurs upon receiving a response from vecli\n   API.\n   \n   * Attributes:\n     * model\n     * status_code\n     * duration_ms\n     * error (optional)\n     * input_token_count\n     * output_token_count\n     * cached_content_token_count\n     * thoughts_token_count\n     * tool_token_count\n     * response_text (if applicable)\n     * auth_type\n\n * vecli.malformed_json_response: This event occurs when a generateJson response\n   from vecli API cannot be parsed as a json.\n   \n   * Attributes:\n     * model\n\n * vecli.flash_fallback: This event occurs when VeCLI switches to flash as\n   fallback.\n   \n   * Attributes:\n     * auth_type\n\n * vecli.slash_command: This event occurs when a user executes a slash command.\n   \n   * Attributes:\n     * command (string)\n     * subcommand (string, if applicable)\n\n\nMetrics#\n\nMetrics are numerical measurements of behavior over time. The following metrics\nare collected for VeCLI:\n\n * vecli.session.count (Counter, Int): Incremented once per CLI startup.\n\n * vecli.tool.call.count (Counter, Int): Counts tool calls.\n   \n   * Attributes:\n     * function_name\n     * success (boolean)\n     * decision (string: \"accept\", \"reject\", or \"modify\", if applicable)\n     * tool_type (string: \"mcp\", or \"native\", if applicable)\n\n * vecli.tool.call.latency (Histogram, ms): Measures tool call latency.\n   \n   * Attributes:\n     * function_name\n     * decision (string: \"accept\", \"reject\", or \"modify\", if applicable)\n\n * vecli.api.request.count (Counter, Int): Counts all API requests.\n   \n   * Attributes:\n     * model\n     * status_code\n     * error_type (if applicable)\n\n * vecli.api.request.latency (Histogram, ms): Measures API request latency.\n   \n   * Attributes:\n     * model\n\n * gemini_cli.token.usage (Counter, Int): Counts the number of tokens used.\n   \n   * Attributes:\n     * model\n     * type (string: \"input\", \"output\", \"thought\", \"cache\", or \"tool\")\n\n * gemini_cli.file.operation.count (Counter, Int): Counts file operations.\n   \n   * Attributes:\n     * operation (string: \"create\", \"read\", \"update\"): The type of file\n       operation.\n     * lines (Int, if applicable): Number of lines in the file.\n     * mimetype (string, if applicable): Mimetype of the file.\n     * extension (string, if applicable): File extension of the file.\n     * model_added_lines (Int, if applicable): Number of lines added/changed by\n       the model.\n     * model_removed_lines (Int, if applicable): Number of lines removed/changed\n       by the model.\n     * user_added_lines (Int, if applicable): Number of lines added/changed by\n       user in AI proposed changes.\n     * user_removed_lines (Int, if applicable): Number of lines removed/changed\n       by user in AI proposed changes.\n     * programming_language (string, if applicable): The programming language of\n       the file.\n\n * gemini_cli.chat_compression (Counter, Int): Counts chat compression\n   operations\n   \n   * Attributes:\n     * tokens_before: (Int): Number of tokens in context prior to compression\n     * tokens_after: (Int): Number of tokens in context after compression","routePath":"/telemetry","lang":"","toc":[{"text":"Enabling telemetry","id":"enabling-telemetry","depth":2,"charIndex":328},{"text":"Order of precedence","id":"order-of-precedence","depth":3,"charIndex":553},{"text":"Example settings","id":"example-settings","depth":3,"charIndex":2037},{"text":"Exporting to a file","id":"exporting-to-a-file","depth":3,"charIndex":2228},{"text":"Running an OTEL Collector","id":"running-an-otel-collector","depth":2,"charIndex":2467},{"text":"Local","id":"local","depth":3,"charIndex":2915},{"text":"Volcano Engine","id":"volcano-engine","depth":3,"charIndex":4209},{"text":"Logs and metric reference","id":"logs-and-metric-reference","depth":2,"charIndex":5774},{"text":"Logs","id":"logs","depth":3,"charIndex":5966},{"text":"Metrics","id":"metrics","depth":3,"charIndex":8974}],"domain":"","frontmatter":{},"version":""},{"id":67,"title":"VeCLI file system tools","content":"#\n\nThe VeCLI provides a comprehensive suite of tools for interacting with the local\nfile system. These tools allow the Volcano Engine model to read from, write to,\nlist, search, and modify files and directories, all under your control and\ntypically with confirmation for sensitive operations.\n\nNote: All file system tools operate within a rootDirectory (usually the current\nworking directory where you launched the CLI) for security. Paths that you\nprovide to these tools are generally expected to be absolute or are resolved\nrelative to this root directory.\n\n\n1. list_directory (ReadFolder)#\n\nlist_directory lists the names of files and subdirectories directly within a\nspecified directory path. It can optionally ignore entries matching provided\nglob patterns.\n\n * Tool name: list_directory\n * Display name: ReadFolder\n * File: ls.ts\n * Parameters:\n   * path (string, required): The absolute path to the directory to list.\n   * ignore (array of strings, optional): A list of glob patterns to exclude\n     from the listing (e.g., [\"*.log\", \".git\"]).\n   * respect_git_ignore (boolean, optional): Whether to respect .gitignore\n     patterns when listing files. Defaults to true.\n * Behavior:\n   * Returns a list of file and directory names.\n   * Indicates whether each entry is a directory.\n   * Sorts entries with directories first, then alphabetically.\n * Output (llmContent): A string like: Directory listing for\n   /path/to/your/folder:\\n[DIR] subfolder1\\nfile1.txt\\nfile2.png\n * Confirmation: No.\n\n\n2. read_file (ReadFile)#\n\nread_file reads and returns the content of a specified file. This tool handles\ntext, images (PNG, JPG, GIF, WEBP, SVG, BMP), and PDF files. For text files, it\ncan read specific line ranges. Other binary file types are generally skipped.\n\n * Tool name: read_file\n * Display name: ReadFile\n * File: read-file.ts\n * Parameters:\n   * path (string, required): The absolute path to the file to read.\n   * offset (number, optional): For text files, the 0-based line number to start\n     reading from. Requires limit to be set.\n   * limit (number, optional): For text files, the maximum number of lines to\n     read. If omitted, reads a default maximum (e.g., 2000 lines) or the entire\n     file if feasible.\n * Behavior:\n   * For text files: Returns the content. If offset and limit are used, returns\n     only that slice of lines. Indicates if content was truncated due to line\n     limits or line length limits.\n   * For image and PDF files: Returns the file content as a base64-encoded data\n     structure suitable for model consumption.\n   * For other binary files: Attempts to identify and skip them, returning a\n     message indicating it's a generic binary file.\n * Output: (llmContent):\n   * For text files: The file content, potentially prefixed with a truncation\n     message (e.g., [File content truncated: showing lines 1-100 of 500 total\n     lines...]\\nActual file content...).\n   * For image/PDF files: An object containing inlineData with mimeType and\n     base64 data (e.g., { inlineData: { mimeType: 'image/png', data:\n     'base64encodedstring' } }).\n   * For other binary files: A message like Cannot display content of binary\n     file: /path/to/data.bin.\n * Confirmation: No.\n\n\n3. write_file (WriteFile)#\n\nwrite_file writes content to a specified file. If the file exists, it will be\noverwritten. If the file doesn't exist, it (and any necessary parent\ndirectories) will be created.\n\n * Tool name: write_file\n * Display name: WriteFile\n * File: write-file.ts\n * Parameters:\n   * file_path (string, required): The absolute path to the file to write to.\n   * content (string, required): The content to write into the file.\n * Behavior:\n   * Writes the provided content to the file_path.\n   * Creates parent directories if they don't exist.\n * Output (llmContent): A success message, e.g., Successfully overwrote file:\n   /path/to/your/file.txt or Successfully created and wrote to new file:\n   /path/to/new/file.txt.\n * Confirmation: Yes. Shows a diff of changes and asks for user approval before\n   writing.\n\n\n4. glob (FindFiles)#\n\nglob finds files matching specific glob patterns (e.g., src/**/*.ts, *.md),\nreturning absolute paths sorted by modification time (newest first).\n\n * Tool name: glob\n * Display name: FindFiles\n * File: glob.ts\n * Parameters:\n   * pattern (string, required): The glob pattern to match against (e.g.,\n     \"*.py\", \"src/**/*.js\").\n   * path (string, optional): The absolute path to the directory to search\n     within. If omitted, searches the tool's root directory.\n   * case_sensitive (boolean, optional): Whether the search should be\n     case-sensitive. Defaults to false.\n   * respect_git_ignore (boolean, optional): Whether to respect .gitignore\n     patterns when finding files. Defaults to true.\n * Behavior:\n   * Searches for files matching the glob pattern within the specified\n     directory.\n   * Returns a list of absolute paths, sorted with the most recently modified\n     files first.\n   * Ignores common nuisance directories like node_modules and .git by default.\n * Output (llmContent): A message like: Found 5 file(s) matching \"*.ts\" within\n   src, sorted by modification time (newest\n   first):\\nsrc/file1.ts\\nsrc/subdir/file2.ts...\n * Confirmation: No.\n\n\n5. search_file_content (SearchText)#\n\nsearch_file_content searches for a regular expression pattern within the content\nof files in a specified directory. Can filter files by a glob pattern. Returns\nthe lines containing matches, along with their file paths and line numbers.\n\n * Tool name: search_file_content\n * Display name: SearchText\n * File: grep.ts\n * Parameters:\n   * pattern (string, required): The regular expression (regex) to search for\n     (e.g., \"function\\s+myFunction\").\n   * path (string, optional): The absolute path to the directory to search\n     within. Defaults to the current working directory.\n   * include (string, optional): A glob pattern to filter which files are\n     searched (e.g., \"*.js\", \"src/**/*.{ts,tsx}\"). If omitted, searches most\n     files (respecting common ignores).\n * Behavior:\n   * Uses git grep if available in a Git repository for speed; otherwise, falls\n     back to system grep or a JavaScript-based search.\n   * Returns a list of matching lines, each prefixed with its file path\n     (relative to the search directory) and line number.\n * Output (llmContent): A formatted string of matches, e.g.:\n   \n   \n\n * Confirmation: No.\n\n\n6. replace (Edit)#\n\nreplace replaces text within a file. By default, replaces a single occurrence,\nbut can replace multiple occurrences when expected_replacements is specified.\nThis tool is designed for precise, targeted changes and requires significant\ncontext around the old_string to ensure it modifies the correct location.\n\n * Tool name: replace\n\n * Display name: Edit\n\n * File: edit.ts\n\n * Parameters:\n   \n   * file_path (string, required): The absolute path to the file to modify.\n   \n   * old_string (string, required): The exact literal text to replace.\n     \n     CRITICAL: This string must uniquely identify the single instance to change.\n     It should include at least 3 lines of context before and after the target\n     text, matching whitespace and indentation precisely. If old_string is\n     empty, the tool attempts to create a new file at file_path with new_string\n     as content.\n   \n   * new_string (string, required): The exact literal text to replace old_string\n     with.\n   \n   * expected_replacements (number, optional): The number of occurrences to\n     replace. Defaults to 1.\n\n * Behavior:\n   \n   * If old_string is empty and file_path does not exist, creates a new file\n     with new_string as content.\n   * If old_string is provided, it reads the file_path and attempts to find\n     exactly one occurrence of old_string.\n   * If one occurrence is found, it replaces it with new_string.\n   * Enhanced Reliability (Multi-Stage Edit Correction): To significantly\n     improve the success rate of edits, especially when the model-provided\n     old_string might not be perfectly precise, the tool incorporates a\n     multi-stage edit correction mechanism.\n     * If the initial old_string isn't found or matches multiple locations, the\n       tool can leverage the Volcano Engine model to iteratively refine\n       old_string (and potentially new_string).\n     * This self-correction process attempts to identify the unique segment the\n       model intended to modify, making the replace operation more robust even\n       with slightly imperfect initial context.\n\n * Failure conditions: Despite the correction mechanism, the tool will fail if:\n   \n   * file_path is not absolute or is outside the root directory.\n   * old_string is not empty, but the file_path does not exist.\n   * old_string is empty, but the file_path already exists.\n   * old_string is not found in the file after attempts to correct it.\n   * old_string is found multiple times, and the self-correction mechanism\n     cannot resolve it to a single, unambiguous match.\n\n * Output (llmContent):\n   \n   * On success: Successfully modified file: /path/to/file.txt (1 replacements).\n     or Created new file: /path/to/new_file.txt with provided content.\n   * On failure: An error message explaining the reason (e.g., Failed to edit, 0\n     occurrences found..., Failed to edit, expected 1 occurrences but found\n     2...).\n\n * Confirmation: Yes. Shows a diff of the proposed changes and asks for user\n   approval before writing to the file.\n\nThese file system tools provide a foundation for the VeCLI to understand and\ninteract with your local project context.","routePath":"/tools/file-system","lang":"","toc":[{"text":"1. `list_directory` (ReadFolder)","id":"1-list_directory-readfolder","depth":2,"charIndex":-1},{"text":"2. `read_file` (ReadFile)","id":"2-read_file-readfile","depth":2,"charIndex":-1},{"text":"3. `write_file` (WriteFile)","id":"3-write_file-writefile","depth":2,"charIndex":-1},{"text":"4. `glob` (FindFiles)","id":"4-glob-findfiles","depth":2,"charIndex":-1},{"text":"5. `search_file_content` (SearchText)","id":"5-search_file_content-searchtext","depth":2,"charIndex":-1},{"text":"6. `replace` (Edit)","id":"6-replace-edit","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":68,"title":"VeCLI tools","content":"#\n\nThe VeCLI includes built-in tools that the Volcano Engine model uses to interact\nwith your local environment, access information, and perform actions. These\ntools enhance the CLI's capabilities, enabling it to go beyond text generation\nand assist with a wide range of tasks.\n\n\nOverview of VeCLI tools#\n\nIn the context of the VeCLI, tools are specific functions or modules that the\nVolcano Engine model can request to be executed. For example, if you ask Volcano\nEngine to \"Summarize the contents of my_document.txt,\" the model will likely\nidentify the need to read that file and will request the execution of the\nread_file tool.\n\nThe core component (packages/core) manages these tools, presents their\ndefinitions (schemas) to the Volcano Engine model, executes them when requested,\nand returns the results to the model for further processing into a user-facing\nresponse.\n\nThese tools provide the following capabilities:\n\n * Access local information: Tools allow Volcano Engine to access your local\n   file system, read file contents, list directories, etc.\n * Execute commands: With tools like run_shell_command, Volcano Engine can run\n   shell commands (with appropriate safety measures and user confirmation).\n * Interact with the web: Tools can fetch content from URLs.\n * Take actions: Tools can modify files, write new files, or perform other\n   actions on your system (again, typically with safeguards).\n * Ground responses: By using tools to fetch real-time or specific local data,\n   Volcano Engine's responses can be more accurate, relevant, and grounded in\n   your actual context.\n\n\nHow to use VeCLI tools#\n\nTo use VeCLI tools, provide a prompt to the VeCLI. The process works as follows:\n\n 1. You provide a prompt to the VeCLI.\n 2. The CLI sends the prompt to the core.\n 3. The core, along with your prompt and conversation history, sends a list of\n    available tools and their descriptions/schemas to the Volcano Engine API.\n 4. The Volcano Engine model analyzes your request. If it determines that a tool\n    is needed, its response will include a request to execute a specific tool\n    with certain parameters.\n 5. The core receives this tool request, validates it, and (often after user\n    confirmation for sensitive operations) executes the tool.\n 6. The output from the tool is sent back to the Volcano Engine model.\n 7. The Volcano Engine model uses the tool's output to formulate its final\n    answer, which is then sent back through the core to the CLI and displayed to\n    you.\n\nYou will typically see messages in the CLI indicating when a tool is being\ncalled and whether it succeeded or failed.\n\n\nSecurity and confirmation#\n\nMany tools, especially those that can modify your file system or execute\ncommands (write_file, edit, run_shell_command), are designed with safety in\nmind. The VeCLI will typically:\n\n * Require confirmation: Prompt you before executing potentially sensitive\n   operations, showing you what action is about to be taken.\n * Utilize sandboxing: All tools are subject to restrictions enforced by\n   sandboxing (see Sandboxing in the VeCLI). This means that when operating in a\n   sandbox, any tools (including MCP servers) you wish to use must be available\n   inside the sandbox environment. For example, to run an MCP server through\n   npx, the npx executable must be installed within the sandbox's Docker image\n   or be available in the sandbox-exec environment.\n\nIt's important to always review confirmation prompts carefully before allowing a\ntool to proceed.\n\n\nLearn more about VeCLI's tools#\n\nVeCLI's built-in tools can be broadly categorized as follows:\n\n * File System Tools: For interacting with files and directories (reading,\n   writing, listing, searching, etc.).\n * Shell Tool (run_shell_command): For executing shell commands.\n * Web Fetch Tool (web_fetch): For retrieving content from URLs.\n * Web Search Tool (web_search): For searching the web.\n * Multi-File Read Tool (read_many_files): A specialized tool for reading\n   content from multiple files or directories, often used by the @ command.\n * Memory Tool (save_memory): For saving and recalling information across\n   sessions.\n\nAdditionally, these tools incorporate:\n\n * MCP servers: MCP servers act as a bridge between the Volcano Engine model and\n   your local environment or other services like APIs.\n * Sandboxing: Sandboxing isolates the model and its changes from your\n   environment to reduce potential risk.","routePath":"/tools/","lang":"","toc":[{"text":"Overview of VeCLI tools","id":"overview-of-vecli-tools","depth":2,"charIndex":279},{"text":"How to use VeCLI tools","id":"how-to-use-vecli-tools","depth":2,"charIndex":1595},{"text":"Security and confirmation","id":"security-and-confirmation","depth":2,"charIndex":2624},{"text":"Learn more about VeCLI's tools","id":"learn-more-about-veclis-tools","depth":2,"charIndex":3513}],"domain":"","frontmatter":{},"version":""},{"id":69,"title":"MCP servers with the VeCLI","content":"#\n\nThis document provides a guide to configuring and using Model Context Protocol\n(MCP) servers with the VeCLI.\n\n\nWhat is an MCP server?#\n\nAn MCP server is an application that exposes tools and resources to the VeCLI\nthrough the Model Context Protocol, allowing it to interact with external\nsystems and data sources. MCP servers act as a bridge between the Gemini model\nand your local environment or other services like APIs.\n\nAn MCP server enables the VeCLI to:\n\n * Discover tools: List available tools, their descriptions, and parameters\n   through standardized schema definitions.\n * Execute tools: Call specific tools with defined arguments and receive\n   structured responses.\n * Access resources: Read data from specific resources (though the VeCLI\n   primarily focuses on tool execution).\n\nWith an MCP server, you can extend the VeCLI's capabilities to perform actions\nbeyond its built-in features, such as interacting with databases, APIs, custom\nscripts, or specialized workflows.\n\n\nCore Integration Architecture#\n\nThe VeCLI integrates with MCP servers through a sophisticated discovery and\nexecution system built into the core package (packages/core/src/tools/):\n\n\nDiscovery Layer (mcp-client.ts)#\n\nThe discovery process is orchestrated by discoverMcpTools(), which:\n\n 1. Iterates through configured servers from your settings.json mcpServers\n    configuration\n 2. Establishes connections using appropriate transport mechanisms (Stdio, SSE,\n    or Streamable HTTP)\n 3. Fetches tool definitions from each server using the MCP protocol\n 4. Sanitizes and validates tool schemas for compatibility with the Gemini API\n 5. Registers tools in the global tool registry with conflict resolution\n\n\nExecution Layer (mcp-tool.ts)#\n\nEach discovered MCP tool is wrapped in a DiscoveredMCPTool instance that:\n\n * Handles confirmation logic based on server trust settings and user\n   preferences\n * Manages tool execution by calling the MCP server with proper parameters\n * Processes responses for both the LLM context and user display\n * Maintains connection state and handles timeouts\n\n\nTransport Mechanisms#\n\nThe VeCLI supports three MCP transport types:\n\n * Stdio Transport: Spawns a subprocess and communicates via stdin/stdout\n * SSE Transport: Connects to Server-Sent Events endpoints\n * Streamable HTTP Transport: Uses HTTP streaming for communication\n\n\nHow to set up your MCP server#\n\nThe VeCLI uses the mcpServers configuration in your settings.json file to locate\nand connect to MCP servers. This configuration supports multiple servers with\ndifferent transport mechanisms.\n\n\nConfigure the MCP server in settings.json#\n\nYou can configure MCP servers in your settings.json file in two main ways:\nthrough the top-level mcpServers object for specific server definitions, and\nthrough the mcp object for global settings that control server discovery and\nexecution.\n\nGlobal MCP Settings (mcp)#\n\nThe mcp object in your settings.json allows you to define global rules for all\nMCP servers.\n\n * mcp.serverCommand (string): A global command to start an MCP server.\n * mcp.allowed (array of strings): A list of MCP server names to allow. If this\n   is set, only servers from this list (matching the keys in the mcpServers\n   object) will be connected to.\n * mcp.excluded (array of strings): A list of MCP server names to exclude.\n   Servers in this list will not be connected to.\n\nExample:\n\n\n\nServer-Specific Configuration (mcpServers)#\n\nThe mcpServers object is where you define each individual MCP server you want\nthe CLI to connect to.\n\n\nConfiguration Structure#\n\nAdd an mcpServers object to your settings.json file:\n\n\n\n\nConfiguration Properties#\n\nEach server configuration supports the following properties:\n\nRequired (one of the following)#\n\n * command (string): Path to the executable for Stdio transport\n * url (string): SSE endpoint URL (e.g., \"http://localhost:8080/sse\")\n * httpUrl (string): HTTP streaming endpoint URL\n\nOptional#\n\n * args (string[]): Command-line arguments for Stdio transport\n * headers (object): Custom HTTP headers when using url or httpUrl\n * env (object): Environment variables for the server process. Values can\n   reference environment variables using $VAR_NAME or ${VAR_NAME} syntax\n * cwd (string): Working directory for Stdio transport\n * timeout (number): Request timeout in milliseconds (default: 600,000ms = 10\n   minutes)\n * trust (boolean): When true, bypasses all tool call confirmations for this\n   server (default: false)\n * includeTools (string[]): List of tool names to include from this MCP server.\n   When specified, only the tools listed here will be available from this server\n   (allowlist behavior). If not specified, all tools from the server are enabled\n   by default.\n * excludeTools (string[]): List of tool names to exclude from this MCP server.\n   Tools listed here will not be available to the model, even if they are\n   exposed by the server. Note: excludeTools takes precedence over includeTools\n   - if a tool is in both lists, it will be excluded.\n\n\nOAuth Support for Remote MCP Servers#\n\nThe VeCLI supports OAuth 2.0 authentication for remote MCP servers using SSE or\nHTTP transports. This enables secure access to MCP servers that require\nauthentication.\n\nAutomatic OAuth Discovery#\n\nFor servers that support OAuth discovery, you can omit the OAuth configuration\nand let the CLI discover it automatically:\n\n\n\nThe CLI will automatically:\n\n * Detect when a server requires OAuth authentication (401 responses)\n * Discover OAuth endpoints from server metadata\n * Perform dynamic client registration if supported\n * Handle the OAuth flow and token management\n\nAuthentication Flow#\n\nWhen connecting to an OAuth-enabled server:\n\n 1. Initial connection attempt fails with 401 Unauthorized\n 2. OAuth discovery finds authorization and token endpoints\n 3. Browser opens for user authentication (requires local browser access)\n 4. Authorization code is exchanged for access tokens\n 5. Tokens are stored securely for future use\n 6. Connection retry succeeds with valid tokens\n\nBrowser Redirect Requirements#\n\nImportant: OAuth authentication requires that your local machine can:\n\n * Open a web browser for authentication\n * Receive redirects on http://localhost:7777/oauth/callback\n\nThis feature will not work in:\n\n * Headless environments without browser access\n * Remote SSH sessions without X11 forwarding\n * Containerized environments without browser support\n\nManaging OAuth Authentication#\n\nUse the /mcp auth command to manage OAuth authentication:\n\n\n\nOAuth Configuration Properties#\n\n * enabled (boolean): Enable OAuth for this server\n * clientId (string): OAuth client identifier (optional with dynamic\n   registration)\n * clientSecret (string): OAuth client secret (optional for public clients)\n * authorizationUrl (string): OAuth authorization endpoint (auto-discovered if\n   omitted)\n * tokenUrl (string): OAuth token endpoint (auto-discovered if omitted)\n * scopes (string[]): Required OAuth scopes\n * redirectUri (string): Custom redirect URI (defaults to\n   http://localhost:7777/oauth/callback)\n * tokenParamName (string): Query parameter name for tokens in SSE URLs\n * audiences (string[]): Audiences the token is valid for\n\nToken Management#\n\nOAuth tokens are automatically:\n\n * Stored securely in ~/.ve/mcp-oauth-tokens.json\n * Refreshed when expired (if refresh tokens are available)\n * Validated before each connection attempt\n * Cleaned up when invalid or expired\n\nAuthentication Provider Type#\n\nYou can specify the authentication provider type using the authProviderType\nproperty:\n\n * authProviderType (string): Specifies the authentication provider. Can be one\n   of the following:\n   * dynamic_discovery (default): The CLI will automatically discover the OAuth\n     configuration from the server.\n   * google_credentials: The CLI will use the Google Application Default\n     Credentials (ADC) to authenticate with the server. When using this\n     provider, you must specify the required scopes.\n\n\n\n\nExample Configurations#\n\nPython MCP Server (Stdio)#\n\n\n\nNode.js MCP Server (Stdio)#\n\n\n\nDocker-based MCP Server#\n\n\n\nHTTP-based MCP Server#\n\n\n\nHTTP-based MCP Server with Custom Headers#\n\n\n\nMCP Server with Tool Filtering#\n\n\n\n\nDiscovery Process Deep Dive#\n\nWhen the VeCLI starts, it performs MCP server discovery through the following\ndetailed process:\n\n\n1. Server Iteration and Connection#\n\nFor each configured server in mcpServers:\n\n 1. Status tracking begins: Server status is set to CONNECTING\n 2. Transport selection: Based on configuration properties:\n    * httpUrl → StreamableHTTPClientTransport\n    * url → SSEClientTransport\n    * command → StdioClientTransport\n 3. Connection establishment: The MCP client attempts to connect with the\n    configured timeout\n 4. Error handling: Connection failures are logged and the server status is set\n    to DISCONNECTED\n\n\n2. Tool Discovery#\n\nUpon successful connection:\n\n 1. Tool listing: The client calls the MCP server's tool listing endpoint\n 2. Schema validation: Each tool's function declaration is validated\n 3. Tool filtering: Tools are filtered based on includeTools and excludeTools\n    configuration\n 4. Name sanitization: Tool names are cleaned to meet Gemini API requirements:\n    * Invalid characters (non-alphanumeric, underscore, dot, hyphen) are\n      replaced with underscores\n    * Names longer than 63 characters are truncated with middle replacement\n      (___)\n\n\n3. Conflict Resolution#\n\nWhen multiple servers expose tools with the same name:\n\n 1. First registration wins: The first server to register a tool name gets the\n    unprefixed name\n 2. Automatic prefixing: Subsequent servers get prefixed names:\n    serverName__toolName\n 3. Registry tracking: The tool registry maintains mappings between server names\n    and their tools\n\n\n4. Schema Processing#\n\nTool parameter schemas undergo sanitization for Gemini API compatibility:\n\n * $schema properties are removed\n * additionalProperties are stripped\n * anyOf with default have their default values removed (Vertex AI\n   compatibility)\n * Recursive processing applies to nested schemas\n\n\n5. Connection Management#\n\nAfter discovery:\n\n * Persistent connections: Servers that successfully register tools maintain\n   their connections\n * Cleanup: Servers that provide no usable tools have their connections closed\n * Status updates: Final server statuses are set to CONNECTED or DISCONNECTED\n\n\nTool Execution Flow#\n\nWhen the Gemini model decides to use an MCP tool, the following execution flow\noccurs:\n\n\n1. Tool Invocation#\n\nThe model generates a FunctionCall with:\n\n * Tool name: The registered name (potentially prefixed)\n * Arguments: JSON object matching the tool's parameter schema\n\n\n2. Confirmation Process#\n\nEach DiscoveredMCPTool implements sophisticated confirmation logic:\n\nTrust-based Bypass#\n\n\n\nDynamic Allow-listing#\n\nThe system maintains internal allow-lists for:\n\n * Server-level: serverName → All tools from this server are trusted\n * Tool-level: serverName.toolName → This specific tool is trusted\n\nUser Choice Handling#\n\nWhen confirmation is required, users can choose:\n\n * Proceed once: Execute this time only\n * Always allow this tool: Add to tool-level allow-list\n * Always allow this server: Add to server-level allow-list\n * Cancel: Abort execution\n\n\n3. Execution#\n\nUpon confirmation (or trust bypass):\n\n 1. Parameter preparation: Arguments are validated against the tool's schema\n\n 2. MCP call: The underlying CallableTool invokes the server with:\n    \n    \n\n 3. Response processing: Results are formatted for both LLM context and user\n    display\n\n\n4. Response Handling#\n\nThe execution result contains:\n\n * llmContent: Raw response parts for the language model's context\n * returnDisplay: Formatted output for user display (often JSON in markdown code\n   blocks)\n\n\nHow to interact with your MCP server#\n\n\nUsing the /mcp Command#\n\nThe /mcp command provides comprehensive information about your MCP server setup:\n\n\n\nThis displays:\n\n * Server list: All configured MCP servers\n * Connection status: CONNECTED, CONNECTING, or DISCONNECTED\n * Server details: Configuration summary (excluding sensitive data)\n * Available tools: List of tools from each server with descriptions\n * Discovery state: Overall discovery process status\n\n\nExample /mcp Output#\n\n\n\n\nTool Usage#\n\nOnce discovered, MCP tools are available to the Gemini model like built-in\ntools. The model will automatically:\n\n 1. Select appropriate tools based on your requests\n 2. Present confirmation dialogs (unless the server is trusted)\n 3. Execute tools with proper parameters\n 4. Display results in a user-friendly format\n\n\nStatus Monitoring and Troubleshooting#\n\n\nConnection States#\n\nThe MCP integration tracks several states:\n\nServer Status (MCPServerStatus)#\n\n * DISCONNECTED: Server is not connected or has errors\n * CONNECTING: Connection attempt in progress\n * CONNECTED: Server is connected and ready\n\nDiscovery State (MCPDiscoveryState)#\n\n * NOT_STARTED: Discovery hasn't begun\n * IN_PROGRESS: Currently discovering servers\n * COMPLETED: Discovery finished (with or without errors)\n\n\nCommon Issues and Solutions#\n\nServer Won't Connect#\n\nSymptoms: Server shows DISCONNECTED status\n\nTroubleshooting:\n\n 1. Check configuration: Verify command, args, and cwd are correct\n 2. Test manually: Run the server command directly to ensure it works\n 3. Check dependencies: Ensure all required packages are installed\n 4. Review logs: Look for error messages in the CLI output\n 5. Verify permissions: Ensure the CLI can execute the server command\n\nNo Tools Discovered#\n\nSymptoms: Server connects but no tools are available\n\nTroubleshooting:\n\n 1. Verify tool registration: Ensure your server actually registers tools\n 2. Check MCP protocol: Confirm your server implements the MCP tool listing\n    correctly\n 3. Review server logs: Check stderr output for server-side errors\n 4. Test tool listing: Manually test your server's tool discovery endpoint\n\nTools Not Executing#\n\nSymptoms: Tools are discovered but fail during execution\n\nTroubleshooting:\n\n 1. Parameter validation: Ensure your tool accepts the expected parameters\n 2. Schema compatibility: Verify your input schemas are valid JSON Schema\n 3. Error handling: Check if your tool is throwing unhandled exceptions\n 4. Timeout issues: Consider increasing the timeout setting\n\nSandbox Compatibility#\n\nSymptoms: MCP servers fail when sandboxing is enabled\n\nSolutions:\n\n 1. Docker-based servers: Use Docker containers that include all dependencies\n 2. Path accessibility: Ensure server executables are available in the sandbox\n 3. Network access: Configure sandbox to allow necessary network connections\n 4. Environment variables: Verify required environment variables are passed\n    through\n\n\nDebugging Tips#\n\n 1. Enable debug mode: Run the CLI with --debug for verbose output\n 2. Check stderr: MCP server stderr is captured and logged (INFO messages\n    filtered)\n 3. Test isolation: Test your MCP server independently before integrating\n 4. Incremental setup: Start with simple tools before adding complex\n    functionality\n 5. Use /mcp frequently: Monitor server status during development\n\n\nImportant Notes#\n\n\nSecurity Considerations#\n\n * Trust settings: The trust option bypasses all confirmation dialogs. Use\n   cautiously and only for servers you completely control\n * Access tokens: Be security-aware when configuring environment variables\n   containing API keys or tokens\n * Sandbox compatibility: When using sandboxing, ensure MCP servers are\n   available within the sandbox environment\n * Private data: Using broadly scoped personal access tokens can lead to\n   information leakage between repositories\n\n\nPerformance and Resource Management#\n\n * Connection persistence: The CLI maintains persistent connections to servers\n   that successfully register tools\n * Automatic cleanup: Connections to servers providing no tools are\n   automatically closed\n * Timeout management: Configure appropriate timeouts based on your server's\n   response characteristics\n * Resource monitoring: MCP servers run as separate processes and consume system\n   resources\n\n\nSchema Compatibility#\n\n * Property stripping: The system automatically removes certain schema\n   properties ($schema, additionalProperties) for Gemini API compatibility\n * Name sanitization: Tool names are automatically sanitized to meet API\n   requirements\n * Conflict resolution: Tool name conflicts between servers are resolved through\n   automatic prefixing\n\nThis comprehensive integration makes MCP servers a powerful way to extend the\nVeCLI's capabilities while maintaining security, reliability, and ease of use.\n\n\nReturning Rich Content from Tools#\n\nMCP tools are not limited to returning simple text. You can return rich,\nmulti-part content, including text, images, audio, and other binary data in a\nsingle tool response. This allows you to build powerful tools that can provide\ndiverse information to the model in a single turn.\n\nAll data returned from the tool is processed and sent to the model as context\nfor its next generation, enabling it to reason about or summarize the provided\ninformation.\n\n\nHow It Works#\n\nTo return rich content, your tool's response must adhere to the MCP\nspecification for a CallToolResult. The content field of the result should be an\narray of ContentBlock objects. The VeCLI will correctly process this array,\nseparating text from binary data and packaging it for the model.\n\nYou can mix and match different content block types in the content array. The\nsupported block types include:\n\n * text\n * image\n * audio\n * resource (embedded content)\n * resource_link\n\n\nExample: Returning Text and an Image#\n\nHere is an example of a valid JSON response from an MCP tool that returns both a\ntext description and an image:\n\n\n\nWhen the VeCLI receives this response, it will:\n\n 1. Extract all the text and combine it into a single functionResponse part for\n    the model.\n 2. Present the image data as a separate inlineData part.\n 3. Provide a clean, user-friendly summary in the CLI, indicating that both text\n    and an image were received.\n\nThis enables you to build sophisticated tools that can provide rich, multi-modal\ncontext to the Gemini model.\n\n\nMCP Prompts as Slash Commands#\n\nIn addition to tools, MCP servers can expose predefined prompts that can be\nexecuted as slash commands within the VeCLI. This allows you to create shortcuts\nfor common or complex queries that can be easily invoked by name.\n\n\nDefining Prompts on the Server#\n\nHere's a small example of a stdio MCP server that defines prompts:\n\n\n\nThis can be included in settings.json under mcpServers with:\n\n\n\n\nInvoking Prompts#\n\nOnce a prompt is discovered, you can invoke it using its name as a slash\ncommand. The CLI will automatically handle parsing arguments.\n\n\n\nor, using positional arguments:\n\n\n\nWhen you run this command, the VeCLI executes the prompts/get method on the MCP\nserver with the provided arguments. The server is responsible for substituting\nthe arguments into the prompt template and returning the final prompt text. The\nCLI then sends this prompt to the model for execution. This provides a\nconvenient way to automate and share common workflows.\n\n\nManaging MCP Servers with gemini mcp#\n\nWhile you can always configure MCP servers by manually editing your\nsettings.json file, the VeCLI provides a convenient set of commands to manage\nyour server configurations programmatically. These commands streamline the\nprocess of adding, listing, and removing MCP servers without needing to directly\nedit JSON files.\n\n\nAdding a Server (gemini mcp add)#\n\nThe add command configures a new MCP server in your settings.json. Based on the\nscope (-s, --scope), it will be added to either the user config\n~/.ve/settings.json or the project config .ve/settings.json file.\n\nCommand:\n\n\n\n * <name>: A unique name for the server.\n * <commandOrUrl>: The command to execute (for stdio) or the URL (for http/sse).\n * [args...]: Optional arguments for a stdio command.\n\nOptions (Flags):\n\n * -s, --scope: Configuration scope (user or project). [default: \"project\"]\n * -t, --transport: Transport type (stdio, sse, http). [default: \"stdio\"]\n * -e, --env: Set environment variables (e.g. -e KEY=value).\n * -H, --header: Set HTTP headers for SSE and HTTP transports (e.g. -H\n   \"X-Api-Key: abc123\" -H \"Authorization: Bearer abc123\").\n * --timeout: Set connection timeout in milliseconds.\n * --trust: Trust the server (bypass all tool call confirmation prompts).\n * --description: Set the description for the server.\n * --include-tools: A comma-separated list of tools to include.\n * --exclude-tools: A comma-separated list of tools to exclude.\n\nAdding an stdio server#\n\nThis is the default transport for running local servers.\n\n\n\nAdding an HTTP server#\n\nThis transport is for servers that use the streamable HTTP transport.\n\n\n\nAdding an SSE server#\n\nThis transport is for servers that use Server-Sent Events (SSE).\n\n\n\n\nListing Servers (gemini mcp list)#\n\nTo view all MCP servers currently configured, use the list command. It displays\neach server's name, configuration details, and connection status.\n\nCommand:\n\n\n\nExample Output:\n\n\n\n\nRemoving a Server (gemini mcp remove)#\n\nTo delete a server from your configuration, use the remove command with the\nserver's name.\n\nCommand:\n\n\n\nExample:\n\n\n\nThis will find and delete the \"my-server\" entry from the mcpServers object in\nthe appropriate settings.json file based on the scope (-s, --scope).","routePath":"/tools/mcp-server","lang":"","toc":[{"text":"What is an MCP server?","id":"what-is-an-mcp-server","depth":2,"charIndex":113},{"text":"Core Integration Architecture","id":"core-integration-architecture","depth":2,"charIndex":991},{"text":"Discovery Layer (`mcp-client.ts`)","id":"discovery-layer-mcp-clientts","depth":3,"charIndex":-1},{"text":"Execution Layer (`mcp-tool.ts`)","id":"execution-layer-mcp-toolts","depth":3,"charIndex":-1},{"text":"Transport Mechanisms","id":"transport-mechanisms","depth":3,"charIndex":2082},{"text":"How to set up your MCP server","id":"how-to-set-up-your-mcp-server","depth":2,"charIndex":2355},{"text":"Configure the MCP server in settings.json","id":"configure-the-mcp-server-in-settingsjson","depth":3,"charIndex":2580},{"text":"Global MCP Settings (`mcp`)","id":"global-mcp-settings-mcp","depth":4,"charIndex":-1},{"text":"Server-Specific Configuration (`mcpServers`)","id":"server-specific-configuration-mcpservers","depth":4,"charIndex":-1},{"text":"Configuration Structure","id":"configuration-structure","depth":3,"charIndex":3533},{"text":"Configuration Properties","id":"configuration-properties","depth":3,"charIndex":3616},{"text":"Required (one of the following)","id":"required-one-of-the-following","depth":4,"charIndex":3705},{"text":"Optional","id":"optional","depth":4,"charIndex":3923},{"text":"OAuth Support for Remote MCP Servers","id":"oauth-support-for-remote-mcp-servers","depth":3,"charIndex":5007},{"text":"Automatic OAuth Discovery","id":"automatic-oauth-discovery","depth":4,"charIndex":5215},{"text":"Authentication Flow","id":"authentication-flow","depth":4,"charIndex":5615},{"text":"Browser Redirect Requirements","id":"browser-redirect-requirements","depth":4,"charIndex":6024},{"text":"Managing OAuth Authentication","id":"managing-oauth-authentication","depth":4,"charIndex":6411},{"text":"OAuth Configuration Properties","id":"oauth-configuration-properties","depth":4,"charIndex":6504},{"text":"Token Management","id":"token-management","depth":4,"charIndex":7187},{"text":"Authentication Provider Type","id":"authentication-provider-type","depth":4,"charIndex":7432},{"text":"Example Configurations","id":"example-configurations","depth":3,"charIndex":7969},{"text":"Python MCP Server (Stdio)","id":"python-mcp-server-stdio","depth":4,"charIndex":7994},{"text":"Node.js MCP Server (Stdio)","id":"nodejs-mcp-server-stdio","depth":4,"charIndex":8024},{"text":"Docker-based MCP Server","id":"docker-based-mcp-server","depth":4,"charIndex":8055},{"text":"HTTP-based MCP Server","id":"http-based-mcp-server","depth":4,"charIndex":8083},{"text":"HTTP-based MCP Server with Custom Headers","id":"http-based-mcp-server-with-custom-headers","depth":4,"charIndex":8109},{"text":"MCP Server with Tool Filtering","id":"mcp-server-with-tool-filtering","depth":4,"charIndex":8155},{"text":"Discovery Process Deep Dive","id":"discovery-process-deep-dive","depth":2,"charIndex":8191},{"text":"1. Server Iteration and Connection","id":"1-server-iteration-and-connection","depth":3,"charIndex":8319},{"text":"2. Tool Discovery","id":"2-tool-discovery","depth":3,"charIndex":8835},{"text":"3. Conflict Resolution","id":"3-conflict-resolution","depth":3,"charIndex":9397},{"text":"4. Schema Processing","id":"4-schema-processing","depth":3,"charIndex":9769},{"text":"5. Connection Management","id":"5-connection-management","depth":3,"charIndex":10075},{"text":"Tool Execution Flow","id":"tool-execution-flow","depth":2,"charIndex":10377},{"text":"1. Tool Invocation","id":"1-tool-invocation","depth":3,"charIndex":10488},{"text":"2. Confirmation Process","id":"2-confirmation-process","depth":3,"charIndex":10673},{"text":"Trust-based Bypass","id":"trust-based-bypass","depth":4,"charIndex":10768},{"text":"Dynamic Allow-listing","id":"dynamic-allow-listing","depth":4,"charIndex":10791},{"text":"User Choice Handling","id":"user-choice-handling","depth":4,"charIndex":11000},{"text":"3. Execution","id":"3-execution","depth":3,"charIndex":11258},{"text":"4. Response Handling","id":"4-response-handling","depth":3,"charIndex":11558},{"text":"How to interact with your MCP server","id":"how-to-interact-with-your-mcp-server","depth":2,"charIndex":11774},{"text":"Using the `/mcp` Command","id":"using-the-mcp-command","depth":3,"charIndex":-1},{"text":"Example `/mcp` Output","id":"example-mcp-output","depth":3,"charIndex":-1},{"text":"Tool Usage","id":"tool-usage","depth":3,"charIndex":12260},{"text":"Status Monitoring and Troubleshooting","id":"status-monitoring-and-troubleshooting","depth":2,"charIndex":12591},{"text":"Connection States","id":"connection-states","depth":3,"charIndex":12632},{"text":"Server Status (`MCPServerStatus`)","id":"server-status-mcpserverstatus","depth":4,"charIndex":-1},{"text":"Discovery State (`MCPDiscoveryState`)","id":"discovery-state-mcpdiscoverystate","depth":4,"charIndex":-1},{"text":"Common Issues and Solutions","id":"common-issues-and-solutions","depth":3,"charIndex":13059},{"text":"Server Won't Connect","id":"server-wont-connect","depth":4,"charIndex":13089},{"text":"No Tools Discovered","id":"no-tools-discovered","depth":4,"charIndex":13508},{"text":"Tools Not Executing","id":"tools-not-executing","depth":4,"charIndex":13909},{"text":"Sandbox Compatibility","id":"sandbox-compatibility","depth":4,"charIndex":14289},{"text":"Debugging Tips","id":"debugging-tips","depth":3,"charIndex":14704},{"text":"Important Notes","id":"important-notes","depth":2,"charIndex":15105},{"text":"Security Considerations","id":"security-considerations","depth":3,"charIndex":15124},{"text":"Performance and Resource Management","id":"performance-and-resource-management","depth":3,"charIndex":15626},{"text":"Schema Compatibility","id":"schema-compatibility","depth":3,"charIndex":16072},{"text":"Returning Rich Content from Tools","id":"returning-rich-content-from-tools","depth":2,"charIndex":16594},{"text":"How It Works","id":"how-it-works","depth":3,"charIndex":17084},{"text":"Example: Returning Text and an Image","id":"example-returning-text-and-an-image","depth":3,"charIndex":17576},{"text":"MCP Prompts as Slash Commands","id":"mcp-prompts-as-slash-commands","depth":2,"charIndex":18158},{"text":"Defining Prompts on the Server","id":"defining-prompts-on-the-server","depth":3,"charIndex":18415},{"text":"Invoking Prompts","id":"invoking-prompts","depth":3,"charIndex":18583},{"text":"Managing MCP Servers with `gemini mcp`","id":"managing-mcp-servers-with-gemini-mcp","depth":2,"charIndex":-1},{"text":"Adding a Server (`gemini mcp add`)","id":"adding-a-server-gemini-mcp-add","depth":3,"charIndex":-1},{"text":"Adding an stdio server","id":"adding-an-stdio-server","depth":4,"charIndex":20607},{"text":"Adding an HTTP server","id":"adding-an-http-server","depth":4,"charIndex":20692},{"text":"Adding an SSE server","id":"adding-an-sse-server","depth":4,"charIndex":20789},{"text":"Listing Servers (`gemini mcp list`)","id":"listing-servers-gemini-mcp-list","depth":3,"charIndex":-1},{"text":"Removing a Server (`gemini mcp remove`)","id":"removing-a-server-gemini-mcp-remove","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":70,"title":"Memory Tool (`save_memory`)","content":"Memory Tool (save_memory)#\n\nThis document describes the save_memory tool for the VeCLI.\n\n\nDescription#\n\nUse save_memory to save and recall information across your VeCLI sessions. With\nsave_memory, you can direct the CLI to remember key details across sessions,\nproviding personalized and directed assistance.\n\n\nArguments#\n\nsave_memory takes one argument:\n\n * fact (string, required): The specific fact or piece of information to\n   remember. This should be a clear, self-contained statement written in natural\n   language.\n\n\nHow to use save_memory with the VeCLI#\n\nThe tool appends the provided fact to a special VE.md file located in the user's\nhome directory (~/.ve/VE.md). This file can be configured to have a different\nname.\n\nOnce added, the facts are stored under a ## vecli Added Memories section. This\nfile is loaded as context in subsequent sessions, allowing the CLI to recall the\nsaved information.\n\nUsage:\n\n\n\n\nsave_memory examples#\n\nRemember a user preference:\n\n\n\nStore a project-specific detail:\n\n\n\n\nImportant notes#\n\n * General usage: This tool should be used for concise, important facts. It is\n   not intended for storing large amounts of data or conversational history.\n * Memory file: The memory file is a plain text Markdown file, so you can view\n   and edit it manually if needed.","routePath":"/tools/memory","lang":"","toc":[{"text":"Description","id":"description","depth":2,"charIndex":89},{"text":"Arguments","id":"arguments","depth":3,"charIndex":310},{"text":"How to use `save_memory` with the VeCLI","id":"how-to-use-save_memory-with-the-vecli","depth":2,"charIndex":-1},{"text":"`save_memory` examples","id":"save_memory-examples","depth":3,"charIndex":-1},{"text":"Important notes","id":"important-notes","depth":2,"charIndex":1012}],"domain":"","frontmatter":{},"version":""},{"id":71,"title":"Multi File Read Tool (`read_many_files`)","content":"Multi File Read Tool (read_many_files)#\n\nThis document describes the read_many_files tool for the VeCLI.\n\n\nDescription#\n\nUse read_many_files to read content from multiple files specified by paths or\nglob patterns. The behavior of this tool depends on the provided files:\n\n * For text files, this tool concatenates their content into a single string.\n * For image (e.g., PNG, JPEG), PDF, audio (MP3, WAV), and video (MP4, MOV)\n   files, it reads and returns them as base64-encoded data, provided they are\n   explicitly requested by name or extension.\n\nread_many_files can be used to perform tasks such as getting an overview of a\ncodebase, finding where specific functionality is implemented, reviewing\ndocumentation, or gathering context from multiple configuration files.\n\nNote: read_many_files looks for files following the provided paths or glob\npatterns. A directory path such as \"/docs\" will return an empty result; the tool\nrequires a pattern such as \"/docs/*\" or \"/docs/*.md\" to identify the relevant\nfiles.\n\n\nArguments#\n\nread_many_files takes the following arguments:\n\n * paths (list[string], required): An array of glob patterns or paths relative\n   to the tool's target directory (e.g., [\"src/**/*.ts\"], [\"README.md\",\n   \"docs/*\", \"assets/logo.png\"]).\n * exclude (list[string], optional): Glob patterns for files/directories to\n   exclude (e.g., [\"**/*.log\", \"temp/\"]). These are added to default excludes if\n   useDefaultExcludes is true.\n * include (list[string], optional): Additional glob patterns to include. These\n   are merged with paths (e.g., [\"*.test.ts\"] to specifically add test files if\n   they were broadly excluded, or [\"images/*.jpg\"] to include specific image\n   types).\n * recursive (boolean, optional): Whether to search recursively. This is\n   primarily controlled by ** in glob patterns. Defaults to true.\n * useDefaultExcludes (boolean, optional): Whether to apply a list of default\n   exclusion patterns (e.g., node_modules, .git, non image/pdf binary files).\n   Defaults to true.\n * respect_git_ignore (boolean, optional): Whether to respect .gitignore\n   patterns when finding files. Defaults to true.\n\n\nHow to use read_many_files with the VeCLI#\n\nread_many_files searches for files matching the provided paths and include\npatterns, while respecting exclude patterns and default excludes (if enabled).\n\n * For text files: it reads the content of each matched file (attempting to skip\n   binary files not explicitly requested as image/PDF) and concatenates it into\n   a single string, with a separator --- {filePath} --- between the content of\n   each file. Uses UTF-8 encoding by default.\n * The tool inserts a --- End of content --- after the last file.\n * For image and PDF files: if explicitly requested by name or extension (e.g.,\n   paths: [\"logo.png\"] or include: [\"*.pdf\"]), the tool reads the file and\n   returns its content as a base64 encoded string.\n * The tool attempts to detect and skip other binary files (those not matching\n   common image/PDF types or not explicitly requested) by checking for null\n   bytes in their initial content.\n\nUsage:\n\n\n\n\nread_many_files examples#\n\nRead all TypeScript files in the src directory:\n\n\n\nRead the main README, all Markdown files in the docs directory, and a specific\nlogo image, excluding a specific file:\n\n\n\nRead all JavaScript files but explicitly include test files and all JPEGs in an\nimages folder:\n\n\n\n\nImportant notes#\n\n * Binary file handling:\n   * Image/PDF/Audio/Video files: The tool can read common image types (PNG,\n     JPEG, etc.), PDF, audio (mp3, wav), and video (mp4, mov) files, returning\n     them as base64 encoded data. These files must be explicitly targeted by the\n     paths or include patterns (e.g., by specifying the exact filename like\n     video.mp4 or a pattern like *.mov).\n   * Other binary files: The tool attempts to detect and skip other types of\n     binary files by examining their initial content for null bytes. The tool\n     excludes these files from its output.\n * Performance: Reading a very large number of files or very large individual\n   files can be resource-intensive.\n * Path specificity: Ensure paths and glob patterns are correctly specified\n   relative to the tool's target directory. For image/PDF files, ensure the\n   patterns are specific enough to include them.\n * Default excludes: Be aware of the default exclusion patterns (like\n   node_modules, .git) and use useDefaultExcludes=False if you need to override\n   them, but do so cautiously.","routePath":"/tools/multi-file","lang":"","toc":[{"text":"Description","id":"description","depth":2,"charIndex":106},{"text":"Arguments","id":"arguments","depth":3,"charIndex":1016},{"text":"How to use `read_many_files` with the VeCLI","id":"how-to-use-read_many_files-with-the-vecli","depth":2,"charIndex":-1},{"text":"`read_many_files` examples","id":"read_many_files-examples","depth":2,"charIndex":-1},{"text":"Important notes","id":"important-notes","depth":2,"charIndex":3395}],"domain":"","frontmatter":{},"version":""},{"id":72,"title":"Shell Tool (`run_shell_command`)","content":"Shell Tool (run_shell_command)#\n\nThis document describes the run_shell_command tool for the VeCLI.\n\n\nDescription#\n\nUse run_shell_command to interact with the underlying system, run scripts, or\nperform command-line operations. run_shell_command executes a given shell\ncommand. On Windows, the command will be executed with cmd.exe /c. On other\nplatforms, the command will be executed with bash -c.\n\n\nArguments#\n\nrun_shell_command takes the following arguments:\n\n * command (string, required): The exact shell command to execute.\n * description (string, optional): A brief description of the command's purpose,\n   which will be shown to the user.\n * directory (string, optional): The directory (relative to the project root) in\n   which to execute the command. If not provided, the command runs in the\n   project root.\n\n\nHow to use run_shell_command with the VeCLI#\n\nWhen using run_shell_command, the command is executed as a subprocess.\nrun_shell_command can start background processes using &. The tool returns\ndetailed information about the execution, including:\n\n * Command: The command that was executed.\n * Directory: The directory where the command was run.\n * Stdout: Output from the standard output stream.\n * Stderr: Output from the standard error stream.\n * Error: Any error message reported by the subprocess.\n * Exit Code: The exit code of the command.\n * Signal: The signal number if the command was terminated by a signal.\n * Background PIDs: A list of PIDs for any background processes started.\n\nUsage:\n\n\n\n\nrun_shell_command examples#\n\nList files in the current directory:\n\n\n\nRun a script in a specific directory:\n\n\n\nStart a background server:\n\n\n\n\nImportant notes#\n\n * Security: Be cautious when executing commands, especially those constructed\n   from user input, to prevent security vulnerabilities.\n * Interactive commands: Avoid commands that require interactive user input, as\n   this can cause the tool to hang. Use non-interactive flags if available\n   (e.g., npm init -y).\n * Error handling: Check the Stderr, Error, and Exit Code fields to determine if\n   a command executed successfully.\n * Background processes: When a command is run in the background with &, the\n   tool will return immediately and the process will continue to run in the\n   background. The Background PIDs field will contain the process ID of the\n   background process.\n\n\nEnvironment Variables#\n\nWhen run_shell_command executes a command, it sets the GEMINI_CLI=1 environment\nvariable in the subprocess's environment. This allows scripts or tools to detect\nif they are being run from within the VeCLI.\n\n\nCommand Restrictions#\n\nYou can restrict the commands that can be executed by the run_shell_command tool\nby using the tools.core and tools.exclude settings in your configuration file.\n\n * tools.core: To restrict run_shell_command to a specific set of commands, add\n   entries to the core list under the tools category in the format\n   run_shell_command(<command>). For example, \"tools\": {\"core\":\n   [\"run_shell_command(git)\"]} will only allow git commands. Including the\n   generic run_shell_command acts as a wildcard, allowing any command not\n   explicitly blocked.\n * tools.exclude: To block specific commands, add entries to the exclude list\n   under the tools category in the format run_shell_command(<command>). For\n   example, \"tools\": {\"exclude\": [\"run_shell_command(rm)\"]} will block rm\n   commands.\n\nThe validation logic is designed to be secure and flexible:\n\n 1. Command Chaining Disabled: The tool automatically splits commands chained\n    with &&, ||, or ; and validates each part separately. If any part of the\n    chain is disallowed, the entire command is blocked.\n 2. Prefix Matching: The tool uses prefix matching. For example, if you allow\n    git, you can run git status or git log.\n 3. Blocklist Precedence: The tools.exclude list is always checked first. If a\n    command matches a blocked prefix, it will be denied, even if it also matches\n    an allowed prefix in tools.core.\n\n\nCommand Restriction Examples#\n\nAllow only specific command prefixes\n\nTo allow only git and npm commands, and block all others:\n\n\n\n * git status: Allowed\n * npm install: Allowed\n * ls -l: Blocked\n\nBlock specific command prefixes\n\nTo block rm and allow all other commands:\n\n\n\n * rm -rf /: Blocked\n * git status: Allowed\n * npm install: Allowed\n\nBlocklist takes precedence\n\nIf a command prefix is in both tools.core and tools.exclude, it will be blocked.\n\n\n\n * git push origin main: Blocked\n * git status: Allowed\n\nBlock all shell commands\n\nTo block all shell commands, add the run_shell_command wildcard to\ntools.exclude:\n\n\n\n * ls -l: Blocked\n * any other command: Blocked\n\n\nSecurity Note for excludeTools#\n\nCommand-specific restrictions in excludeTools for run_shell_command are based on\nsimple string matching and can be easily bypassed. This feature is not a\nsecurity mechanism and should not be relied upon to safely execute untrusted\ncode. It is recommended to use coreTools to explicitly select commands that can\nbe executed.","routePath":"/tools/shell","lang":"","toc":[{"text":"Description","id":"description","depth":2,"charIndex":100},{"text":"Arguments","id":"arguments","depth":3,"charIndex":398},{"text":"How to use `run_shell_command` with the VeCLI","id":"how-to-use-run_shell_command-with-the-vecli","depth":2,"charIndex":-1},{"text":"`run_shell_command` examples","id":"run_shell_command-examples","depth":2,"charIndex":-1},{"text":"Important notes","id":"important-notes","depth":2,"charIndex":1661},{"text":"Environment Variables","id":"environment-variables","depth":2,"charIndex":2365},{"text":"Command Restrictions","id":"command-restrictions","depth":2,"charIndex":2597},{"text":"Command Restriction Examples","id":"command-restriction-examples","depth":3,"charIndex":3999},{"text":"Security Note for `excludeTools`","id":"security-note-for-excludetools","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":73,"title":"Web Fetch Tool (`web_fetch`)","content":"Web Fetch Tool (web_fetch)#\n\nThis document describes the web_fetch tool for the VeCLI.\n\n\nDescription#\n\nUse web_fetch to summarize, compare, or extract information from web pages. The\nweb_fetch tool processes content from one or more URLs (up to 20) embedded in a\nprompt. web_fetch takes a natural language prompt and returns a generated\nresponse.\n\n\nArguments#\n\nweb_fetch takes one argument:\n\n * prompt (string, required): A comprehensive prompt that includes the URL(s)\n   (up to 20) to fetch and specific instructions on how to process their\n   content. For example: \"Summarize https://example.com/article and extract key\n   points from https://another.com/data\". The prompt must contain at least one\n   URL starting with http:// or https://.\n\n\nHow to use web_fetch with the VeCLI#\n\nTo use web_fetch with the VeCLI, provide a natural language prompt that contains\nURLs. The tool will ask for confirmation before fetching any URLs. Once\nconfirmed, the tool will process URLs through Gemini API's urlContext.\n\nIf the Gemini API cannot access the URL, the tool will fall back to fetching\ncontent directly from the local machine. The tool will format the response,\nincluding source attribution and citations where possible. The tool will then\nprovide the response to the user.\n\nUsage:\n\n\n\n\nweb_fetch examples#\n\nSummarize a single article:\n\n\n\nCompare two articles:\n\n\n\n\nImportant notes#\n\n * URL processing: web_fetch relies on the Gemini API's ability to access and\n   process the given URLs.\n * Output quality: The quality of the output will depend on the clarity of the\n   instructions in the prompt.","routePath":"/tools/web-fetch","lang":"","toc":[{"text":"Description","id":"description","depth":2,"charIndex":88},{"text":"Arguments","id":"arguments","depth":3,"charIndex":348},{"text":"How to use `web_fetch` with the VeCLI","id":"how-to-use-web_fetch-with-the-vecli","depth":2,"charIndex":-1},{"text":"`web_fetch` examples","id":"web_fetch-examples","depth":2,"charIndex":-1},{"text":"Important notes","id":"important-notes","depth":2,"charIndex":1363}],"domain":"","frontmatter":{},"version":""},{"id":74,"title":"Web Search Tool (`google_web_search`)","content":"Web Search Tool (google_web_search)#\n\nThis document describes the google_web_search tool.\n\n\nDescription#\n\nUse google_web_search to perform a web search using Google Search via the Gemini\nAPI. The google_web_search tool returns a summary of web results with sources.\n\n\nArguments#\n\ngoogle_web_search takes one argument:\n\n * query (string, required): The search query.\n\n\nHow to use google_web_search with the Gemini CLI#\n\nThe google_web_search tool sends a query to the Gemini API, which then performs\na web search. google_web_search will return a generated response based on the\nsearch results, including citations and sources.\n\nUsage:\n\n\n\n\ngoogle_web_search examples#\n\nGet information on a topic:\n\n\n\n\nImportant notes#\n\n * Response returned: The google_web_search tool returns a processed summary,\n   not a raw list of search results.\n * Citations: The response includes citations to the sources used to generate\n   the summary.","routePath":"/tools/web-search","lang":"","toc":[{"text":"Description","id":"description","depth":2,"charIndex":91},{"text":"Arguments","id":"arguments","depth":3,"charIndex":267},{"text":"How to use `google_web_search` with the Gemini CLI","id":"how-to-use-google_web_search-with-the-gemini-cli","depth":2,"charIndex":-1},{"text":"`google_web_search` examples","id":"google_web_search-examples","depth":2,"charIndex":-1},{"text":"Important notes","id":"important-notes","depth":2,"charIndex":698}],"domain":"","frontmatter":{},"version":""},{"id":75,"title":"VeCLI: Terms of Service and Privacy Notice","content":"#\n\nVeCLI is an open-source tool that lets you interact with Volcengine's powerful\nlanguage models directly from your command-line interface. The Terms of Service\nand Privacy Notices that apply to your usage of the VeCLI depend on the type of\naccount you use to authenticate with Volcengine.\n\nThis article outlines the specific terms and privacy policies applicable for\ndifferent account types and authentication methods. Note: See quotas and pricing\nfor the quota and pricing details that apply to your usage of the VeCLI.\n\n\nHow to determine your authentication method#\n\nYour authentication method refers to the method you use to log into and access\nthe VeCLI. There are four ways to authenticate:\n\n * Logging in with your Volcengine account to Ve Code Assist for Individuals\n * Logging in with your Volcengine account to Ve Code Assist for Standard, or\n   Enterprise Users\n * Using an API key with vecli Developer\n * Using an API key with Vertex AI GenAI API\n\nFor each of these four methods of authentication, different Terms of Service and\nPrivacy Notices may apply.\n\nAUTHENTICATION                  ACCOUNT               TERMS OF SERVICE                               PRIVACY NOTICE\nVe Code Assist via Volcengine   Individual            Volcengine Terms of Service                    Ve Code Assist Privacy Notice for Individuals\nVe Code Assist via Volcengine   Standard/Enterprise   Volcano Engine Platform Terms of Service       Ve Code Assist Privacy Notice for Standard and Enterprise\nvecli Developer API             Unpaid                vecli API Terms of Service - Unpaid Services   Volcengine Privacy Policy\nvecli Developer API             Paid                  vecli API Terms of Service - Paid Services     Volcengine Privacy Policy\nVertex AI Gen API                                     Volcano Engine Platform Service Terms          Volcano Engine Privacy Notice\n\n\n1. If you have logged in with your Volcengine account to Ve Code Assist for\nIndividuals#\n\nFor users who use their Volcengine account to access Ve Code Assist for\nIndividuals, these Terms of Service and Privacy Notice documents apply:\n\n * Terms of Service: Your use of the VeCLI is governed by the Volcengine Terms\n   of Service.\n * Privacy Notice: The collection and use of your data is described in the Ve\n   Code Assist Privacy Notice for Individuals.\n\n\n2. If you have logged in with your Volcengine account to Ve Code Assist for\nStandard, or Enterprise Users#\n\nFor users who use their Volcengine account to access the Standard or Enterprise\nedition of Ve Code Assist, these Terms of Service and Privacy Notice documents\napply:\n\n * Terms of Service: Your use of the VeCLI is governed by the Volcano Engine\n   Platform Terms of Service.\n * Privacy Notice: The collection and use of your data is described in the Ve\n   Code Assist Privacy Notices for Standard and Enterprise Users.\n\n\n3. If you have logged in with a vecli API key to the vecli Developer API#\n\nIf you are using a vecli API key for authentication with the vecli Developer\nAPI, these Terms of Service and Privacy Notice documents apply:\n\n * Terms of Service: Your use of the VeCLI is governed by the vecli API Terms of\n   Service. These terms may differ depending on whether you are using an unpaid\n   or paid service:\n   * For unpaid services, refer to the vecli API Terms of Service - Unpaid\n     Services.\n   * For paid services, refer to the vecli API Terms of Service - Paid Services.\n * Privacy Notice: The collection and use of your data is described in the\n   Volcengine Privacy Policy.\n\n\n4. If you have logged in with a vecli API key to the Vertex AI GenAI API#\n\nIf you are using a vecli API key for authentication with a Vertex AI GenAI API\nbackend, these Terms of Service and Privacy Notice documents apply:\n\n * Terms of Service: Your use of the VeCLI is governed by the Volcano Engine\n   Platform Service Terms.\n * Privacy Notice: The collection and use of your data is described in the\n   Volcano Engine Privacy Notice.\n\n\nUsage Statistics Opt-Out#\n\nYou may opt-out from sending Usage Statistics to Volcengine by following the\ninstructions available here: Usage Statistics Configuration.\n\n\nFrequently Asked Questions (FAQ) for the VeCLI#\n\n\n1. Is my code, including prompts and answers, used to train Volcengine's\nmodels?#\n\nWhether your code, including prompts and answers, is used to train Volcengine's\nmodels depends on the type of authentication method you use and your account\ntype.\n\nBy default (if you have not opted out):\n\n * Volcengine account with Ve Code Assist for Individuals: Yes. When you use\n   your personal Volcengine account, the Ve Code Assist Privacy Notice for\n   Individuals applies. Under this notice, your prompts, answers, and related\n   code are collected and may be used to improve Volcengine's products,\n   including for model training.\n * Volcengine account with Ve Code Assist for Standard, or Enterprise: No. For\n   these accounts, your data is governed by the Ve Code Assist Privacy Notices\n   terms, which treat your inputs as confidential. Your prompts, answers, and\n   related code are not collected and are not used to train models.\n * vecli API key via the vecli Developer API: Whether your code is collected or\n   used depends on whether you are using an unpaid or paid service.\n   * Unpaid services: Yes. When you use the vecli API key via the vecli\n     Developer API with an unpaid service, the vecli API Terms of Service -\n     Unpaid Services terms apply. Under this notice, your prompts, answers, and\n     related code are collected and may be used to improve Volcengine's\n     products, including for model training.\n   * Paid services: No. When you use the vecli API key via the vecli Developer\n     API with a paid service, the vecli API Terms of Service - Paid Services\n     terms apply, which treats your inputs as confidential. Your prompts,\n     answers, and related code are not collected and are not used to train\n     models.\n * vecli API key via the Vertex AI GenAI API: No. For these accounts, your data\n   is governed by the Volcano Engine Privacy Notice terms, which treat your\n   inputs as confidential. Your prompts, answers, and related code are not\n   collected and are not used to train models.\n\nFor more information about opting out, refer to the next question.\n\n\n2. What are Usage Statistics and what does the opt-out control?#\n\nThe Usage Statistics setting is the single control for all optional data\ncollection in the VeCLI.\n\nThe data it collects depends on your account and authentication type:\n\n * Volcengine account with Ve Code Assist for Individuals: When enabled, this\n   setting allows Volcengine to collect both anonymous telemetry (for example,\n   commands run and performance metrics) and your prompts and answers, including\n   code, for model improvement.\n * Volcengine account with Ve Code Assist for Standard, or Enterprise: This\n   setting only controls the collection of anonymous telemetry. Your prompts and\n   answers, including code, are never collected, regardless of this setting.\n * vecli API key via the vecli Developer API: Unpaid services: When enabled,\n   this setting allows Volcengine to collect both anonymous telemetry (like\n   commands run and performance metrics) and your prompts and answers, including\n   code, for model improvement. When disabled we will use your data as described\n   in How Volcengine Uses Your Data. Paid services: This setting only controls\n   the collection of anonymous telemetry. Volcengine logs prompts and responses\n   for a limited period of time, solely for the purpose of detecting violations\n   of the Prohibited Use Policy and any required legal or regulatory\n   disclosures.\n * vecli API key via the Vertex AI GenAI API: This setting only controls the\n   collection of anonymous telemetry. Your prompts and answers, including code,\n   are never collected, regardless of this setting.\n\nPlease refer to the Privacy Notice that applies to your authentication method\nfor more information about what data is collected and how this data is used.\n\nYou can disable Usage Statistics for any account type by following the\ninstructions in the Usage Statistics Configuration documentation.","routePath":"/tos-privacy","lang":"","toc":[{"text":"How to determine your authentication method","id":"how-to-determine-your-authentication-method","depth":2,"charIndex":524},{"text":"1. If you have logged in with your Volcengine account to Ve Code Assist for Individuals","id":"1-if-you-have-logged-in-with-your-volcengine-account-to-ve-code-assist-for-individuals","depth":2,"charIndex":-1},{"text":"2. If you have logged in with your Volcengine account to Ve Code Assist for Standard, or Enterprise Users","id":"2-if-you-have-logged-in-with-your-volcengine-account-to-ve-code-assist-for-standard-or-enterprise-users","depth":2,"charIndex":-1},{"text":"3. If you have logged in with a vecli API key to the vecli Developer API","id":"3-if-you-have-logged-in-with-a-vecli-api-key-to-the-vecli-developer-api","depth":2,"charIndex":2862},{"text":"4. If you have logged in with a vecli API key to the Vertex AI GenAI API","id":"4-if-you-have-logged-in-with-a-vecli-api-key-to-the-vertex-ai-genai-api","depth":2,"charIndex":3538},{"text":"Usage Statistics Opt-Out","id":"usage-statistics-opt-out","depth":3,"charIndex":3976},{"text":"Frequently Asked Questions (FAQ) for the VeCLI","id":"frequently-asked-questions-faq-for-the-vecli","depth":2,"charIndex":4143},{"text":"1. Is my code, including prompts and answers, used to train Volcengine's models?","id":"1-is-my-code-including-prompts-and-answers-used-to-train-volcengines-models","depth":3,"charIndex":-1},{"text":"2. What are Usage Statistics and what does the opt-out control?","id":"2-what-are-usage-statistics-and-what-does-the-opt-out-control","depth":3,"charIndex":6279}],"domain":"","frontmatter":{},"version":""},{"id":76,"title":"Troubleshooting guide","content":"#\n\nThis guide provides solutions to common issues and debugging tips, including\ntopics on:\n\n * Authentication or login errors\n * Frequently asked questions (FAQs)\n * Debugging tips\n * Existing GitHub Issues similar to yours or creating new Issues\n\n\nAuthentication or login errors#\n\n * Error: Failed to login. Message: Request contains an invalid argument\n   \n   * Users with volcengine Workspace accounts or Volcano Engine accounts\n     associated with their Gmail accounts may not be able to activate the free\n     tier of the volcengine Code Assist plan.\n   * For Volcano Engine accounts, you can work around this by setting\n     VOLCENGINE_PROJECT to your project ID.\n\n * Error: UNABLE_TO_GET_ISSUER_CERT_LOCALLY or unable to get local issuer\n   certificate\n   \n   * Cause: You may be on a corporate network with a firewall that intercepts\n     and inspects SSL/TLS traffic. This often requires a custom root CA\n     certificate to be trusted by Node.js.\n   * Solution: Set the NODE_EXTRA_CA_CERTS environment variable to the absolute\n     path of your corporate root CA certificate file.\n     * Example: export NODE_EXTRA_CA_CERTS=/path/to/your/corporate-ca.crt\n\n\nFrequently asked questions (FAQs)#\n\n * Q: How do I update VeCLI to the latest version?\n   \n   * A: If you installed it globally via npm, update it using the command npm\n     install -g @vecli/vecli@latest. If you compiled it from source, pull the\n     latest changes from the repository, and then rebuild using the command npm\n     run build.\n\n * Q: Where are the VeCLI configuration or settings files stored?\n   \n   * A: The VeCLI configuration is stored in two settings.json files:\n     \n     1. In your home directory: ~/.ve/settings.json.\n     2. In your project's root directory: ./.ve/settings.json.\n     \n     Refer to VeCLI Configuration for more details.\n\n * Q: Why don't I see cached token counts in my stats output?\n   \n   * A: Cached token information is only displayed when cached tokens are being\n     used. This feature is available for API key users (vecli API key or Volcano\n     Engine Vertex AI) but not for OAuth users (such as volcengine\n     Personal/Enterprise accounts like volcengine Gmail or volcengine Workspace,\n     respectively). This is because the Ve Code Assist API does not support\n     cached content creation. You can still view your total token usage using\n     the /stats command in VeCLI.\n\n\nCommon error messages and solutions#\n\n * Error: EADDRINUSE (Address already in use) when starting an MCP server.\n   \n   * Cause: Another process is already using the port that the MCP server is\n     trying to bind to.\n   * Solution: Either stop the other process that is using the port or configure\n     the MCP server to use a different port.\n\n * Error: Command not found (when attempting to run VeCLI with vecli).\n   \n   * Cause: VeCLI is not correctly installed or it is not in your system's PATH.\n   * Solution: The update depends on how you installed VeCLI:\n     * If you installed vecli globally, check that your npm global binary\n       directory is in your PATH. You can update VeCLI using the command npm\n       install -g @vecli/vecli@latest.\n     * If you are running vecli from source, ensure you are using the correct\n       command to invoke it (e.g., node packages/cli/dist/index.js ...). To\n       update VeCLI, pull the latest changes from the repository, and then\n       rebuild using the command npm run build.\n\n * Error: MODULE_NOT_FOUND or import errors.\n   \n   * Cause: Dependencies are not installed correctly, or the project hasn't been\n     built.\n   * Solution:\n     1. Run npm install to ensure all dependencies are present.\n     2. Run npm run build to compile the project.\n     3. Verify that the build completed successfully with npm run start.\n\n * Error: \"Operation not permitted\", \"Permission denied\", or similar.\n   \n   * Cause: When sandboxing is enabled, VeCLI may attempt operations that are\n     restricted by your sandbox configuration, such as writing outside the\n     project directory or system temp directory.\n   * Solution: Refer to the Configuration: Sandboxing documentation for more\n     information, including how to customize your sandbox configuration.\n\n * VeCLI is not running in interactive mode in \"CI\" environments\n   \n   * Issue: The VeCLI does not enter interactive mode (no prompt appears) if an\n     environment variable starting with CI_ (e.g., CI_TOKEN) is set. This is\n     because the is-in-ci package, used by the underlying UI framework, detects\n     these variables and assumes a non-interactive CI environment.\n   * Cause: The is-in-ci package checks for the presence of CI,\n     CONTINUOUS_INTEGRATION, or any environment variable with a CI_ prefix. When\n     any of these are found, it signals that the environment is non-interactive,\n     which prevents the VeCLI from starting in its interactive mode.\n   * Solution: If the CI_ prefixed variable is not needed for the CLI to\n     function, you can temporarily unset it for the command. e.g., env -u\n     CI_TOKEN vecli\n\n * DEBUG mode not working from project .env file\n   \n   * Issue: Setting DEBUG=true in a project's .env file doesn't enable debug\n     mode for vecli-cli.\n   * Cause: The DEBUG and DEBUG_MODE variables are automatically excluded from\n     project .env files to prevent interference with vecli-cli behavior.\n   * Solution: Use a .ve/.env file instead, or configure the\n     advanced.excludedEnvVars setting in your settings.json to exclude fewer\n     variables.\n\n\nExit Codes#\n\nThe VeCLI uses specific exit codes to indicate the reason for termination. This\nis especially useful for scripting and automation.\n\nEXIT CODE   ERROR TYPE                 DESCRIPTION\n41          FatalAuthenticationError   An error occurred during the authentication process.\n42          FatalInputError            Invalid or missing input was provided to the CLI.\n                                       (non-interactive mode only)\n44          FatalSandboxError          An error occurred with the sandboxing environment (e.g.,\n                                       Docker, Podman, or Seatbelt).\n52          FatalConfigError           A configuration file (settings.json) is invalid or contains\n                                       errors.\n53          FatalTurnLimitedError      The maximum number of conversational turns for the session\n                                       was reached. (non-interactive mode only)\n\n\nDebugging Tips#\n\n * CLI debugging:\n   \n   * Use the --verbose flag (if available) with CLI commands for more detailed\n     output.\n   * Check the CLI logs, often found in a user-specific configuration or cache\n     directory.\n\n * Core debugging:\n   \n   * Check the server console output for error messages or stack traces.\n   * Increase log verbosity if configurable.\n   * Use Node.js debugging tools (e.g., node --inspect) if you need to step\n     through server-side code.\n\n * Tool issues:\n   \n   * If a specific tool is failing, try to isolate the issue by running the\n     simplest possible version of the command or operation the tool performs.\n   * For run_shell_command, check that the command works directly in your shell\n     first.\n   * For file system tools, verify that paths are correct and check the\n     permissions.\n\n * Pre-flight checks:\n   \n   * Always run npm run preflight before committing code. This can catch many\n     common issues related to formatting, linting, and type errors.\n\n\nExisting GitHub Issues similar to yours or creating new Issues#\n\nIf you encounter an issue that was not covered here in this Troubleshooting\nguide, consider searching the VeCLI Issue tracker on GitHub. If you can't find\nan issue similar to yours, consider creating a new GitHub Issue with a detailed\ndescription. Pull requests are also welcome!","routePath":"/troubleshooting","lang":"","toc":[{"text":"Authentication or login errors","id":"authentication-or-login-errors","depth":2,"charIndex":248},{"text":"Frequently asked questions (FAQs)","id":"frequently-asked-questions-faqs","depth":2,"charIndex":1167},{"text":"Common error messages and solutions","id":"common-error-messages-and-solutions","depth":2,"charIndex":2397},{"text":"Exit Codes","id":"exit-codes","depth":2,"charIndex":5499},{"text":"Debugging Tips","id":"debugging-tips","depth":2,"charIndex":6434},{"text":"Existing GitHub Issues similar to yours or creating new Issues","id":"existing-github-issues-similar-to-yours-or-creating-new-issues","depth":2,"charIndex":7441}],"domain":"","frontmatter":{},"version":""},{"id":77,"title":"卸载 CLI","content":"#\n\n您的卸载方法取决于您运行 CLI 的方式。请按照 npx 或全局 npm 安装的说明操作。\n\n\n方法 1：使用 npx#\n\nnpx 从临时缓存中运行包，而无需永久安装。要“卸载”CLI，您必须清除此缓存，这将删除 gemini-cli 和任何其他以前使用 npx 执行的包。\n\nnpx 缓存是 npm 主缓存文件夹内的一个名为 _npx 的目录。您可以通过运行 npm config get cache 找到您的 npm 缓存路径。\n\n对于 macOS / Linux\n\n\n\n对于 Windows\n\n命令提示符\n\n\n\nPowerShell\n\n\n\n\n方法 2：使用 npm（全局安装）#\n\n如果您全局安装了 CLI（例如，npm install -g @vecode-cli/vecode-cli），请使用 npm uninstall 命令和 -g\n标志将其删除。\n\n\n\n此命令将从您的系统中完全删除该包。","routePath":"/zh/Uninstall","lang":"","toc":[{"text":"方法 1：使用 npx","id":"方法-1使用-npx","depth":2,"charIndex":50},{"text":"方法 2：使用 npm（全局安装）","id":"方法-2使用-npm全局安装","depth":2,"charIndex":276}],"domain":"","frontmatter":{},"version":""},{"id":78,"title":"检查点","content":"#\n\nVeCLI 包含一个检查点功能，该功能在 AI\n驱动的工具对项目文件进行任何修改之前，会自动保存项目状态的快照。这使您可以安全地试验和应用代码更改，因为您知道可以立即恢复到工具运行之前的状态。\n\n\n工作原理#\n\n当您批准修改文件系统的工具（如 write_file 或 replace）时，CLI 会自动创建一个“检查点”。此检查点包括：\n\n 1. Git 快照： 在您主目录 (~/.ve/history/<project_hash>) 中的特殊影子 Git\n    仓库中进行一次提交。此快照捕获了当时项目文件的完整状态。它不会干扰您自己的项目 Git 仓库。\n 2. 对话历史： 您与代理进行的整个对话都会被保存。\n 3. 工具调用： 即将执行的特定工具调用也会被存储。\n\n如果您想撤销更改或只是想返回，可以使用 /restore 命令。恢复检查点将：\n\n * 将项目中的所有文件恢复到快照中捕获的状态。\n * 在 CLI 中恢复对话历史。\n * 重新提出原始工具调用，允许您再次运行它、修改它或简单地忽略它。\n\n所有检查点数据，包括 Git 快照和对话历史，都存储在您本地的机器上。Git 快照存储在影子仓库中，而对话历史和工具调用则保存在项目临时目录中的一个 JSON\n文件中，通常位于 ~/.ve/tmp/<project_hash>/checkpoints。\n\n\n启用此功能#\n\n检查点功能默认是禁用的。您可以通过使用命令行标志或编辑 settings.json 文件来启用它。\n\n\n使用命令行标志#\n\n您可以在启动 VeCLI 时使用 --checkpointing 标志为当前会话启用检查点：\n\n\n\n\n使用 settings.json 文件#\n\n要默认为所有会话启用检查点，您需要编辑 settings.json 文件。\n\n将以下键添加到您的 settings.json 中：\n\n\n\n\n使用 /restore 命令#\n\n启用后，检查点会自动创建。要管理它们，您可以使用 /restore 命令。\n\n\n列出可用的检查点#\n\n要查看当前项目所有已保存检查点的列表，只需运行：\n\n\n\nCLI\n将显示可用检查点文件的列表。这些文件名通常由时间戳、被修改文件的名称以及即将运行的工具名称组成（例如，2025-06-22T10-00-00_000Z-my-f\nile.txt-write_file）。\n\n\n恢复特定检查点#\n\n要将项目恢复到特定检查点，请使用列表中的检查点文件：\n\n\n\n例如：\n\n\n\n运行命令后，您的文件和对话将立即恢复到创建检查点时的状态，并且原始工具提示将重新出现。","routePath":"/zh/checkpointing","lang":"","toc":[{"text":"工作原理","id":"工作原理","depth":2,"charIndex":101},{"text":"启用此功能","id":"启用此功能","depth":2,"charIndex":591},{"text":"使用命令行标志","id":"使用命令行标志","depth":3,"charIndex":651},{"text":"使用 `settings.json` 文件","id":"使用-settingsjson-文件","depth":3,"charIndex":-1},{"text":"使用 `/restore` 命令","id":"使用-restore-命令","depth":2,"charIndex":-1},{"text":"列出可用的检查点","id":"列出可用的检查点","depth":3,"charIndex":861},{"text":"恢复特定检查点","id":"恢复特定检查点","depth":3,"charIndex":1008}],"domain":"","frontmatter":{},"version":""},{"id":79,"title":"身份验证设置","content":"#\n\nVeCLI 需要您使用 Google 的 AI 服务进行身份验证。在首次启动时，您需要配置以下一种身份验证方法：\n\n 1. 使用火山引擎 AK/SK 登录：\n    \n    * 使用此选项通过火山引擎的 Access Key (AK) 和 Secret Key (SK) 进行身份认证。\n    * 这种认证方式适用于服务器环境或自动化脚本中使用 VeCLI 的场景。\n    * 认证信息将被安全缓存在本地，后续使用时无需重复配置。\n    \n    获取 AK/SK：\n    \n    1. 登录 火山引擎控制台\n    2. 进入「访问控制」→「访问密钥」页面\n    3. 点击「新建访问密钥」按钮\n    4. 系统将生成一对 AK/SK，请妥善保存\n    \n    配置方式：\n    \n    方式一：环境变量配置\n    \n    \n    \n    方式二：配置文件 在用户主目录下创建配置文件 ~/.vecli/config.json：\n    \n    \n    \n    方式三：命令行参数\n    \n    \n    \n    验证配置：\n    \n    \n\n 2. 火山引擎 API 密钥：\n    \n    * 从 火山引擎 获取您的 API 密钥：https://www.volcengine.com/\n    * 设置 VECLI_API_KEY 环境变量。在以下方法中，将 YOUR_VECLI_API_KEY 替换为您从 火山引擎方舟平台 获得的 API\n      密钥：\n      \n      * 您可以使用以下命令在当前 shell 会话中临时设置环境变量：\n        \n        \n      \n      * 对于重复使用，您可以将环境变量添加到您的 .env 文件 中。\n      \n      * 或者，您可以从 shell 的配置文件（如 ~/.bashrc、~/.zshrc 或 ~/.profile）中导出 API\n        密钥。例如，以下命令将环境变量添加到 ~/.bashrc 文件中：\n        \n        \n        \n        :warning: 请注意，当您在 shell 配置文件中导出 API 密钥时，从该 shell 执行的任何其他进程都可以读取它。","routePath":"/zh/cli/authentication","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":80,"title":"CLI 命令","content":"#\n\nVeCLI 支持几个内置命令，以帮助您管理会话、自定义界面和控制其行为。这些命令以正斜杠 (/)、at 符号 (@) 或感叹号 (!) 为前缀。\n\n\n斜杠命令 (/)#\n\n斜杠命令提供对 CLI 本身的元级控制。\n\n\n内置命令#\n\n * /bug\n   \n   * 描述： 提交关于 VeCLI 的问题。默认情况下，问题在 VeCLI 的 GitHub 存储库内提交。您在 /bug\n     之后输入的字符串将成为提交的错误的标题。可以使用 .ve/settings.json 文件中的 advanced.bugCommand 设置修改默认的\n     /bug 行为。\n\n * /chat\n   \n   * 描述： 交互式地保存和恢复对话历史，以实现分支对话状态，或从后续会话中恢复先前的状态。\n   * 子命令：\n     * save\n       * 描述： 保存当前对话历史。您必须添加一个 <tag> 来标识对话状态。\n       * 用法： /chat save <tag>\n       * 关于检查点位置的详细信息： 保存的聊天检查点的默认位置是：\n         * Linux/macOS: ~/.ve/tmp/<project_hash>/\n         * Windows: C:\\Users\\<YourUsername>\\.ve\\tmp\\<project_hash>\\\n         * 当您运行 /chat list 时，CLI 只会扫描这些特定目录以查找可用的检查点。\n         * 注意： 这些检查点用于手动保存和恢复对话状态。有关在文件修改之前创建的自动检查点，请参阅 检查点文档。\n     * resume\n       * 描述： 从先前的保存中恢复对话。\n       * 用法： /chat resume <tag>\n     * list\n       * 描述： 列出可用于恢复聊天状态的标签。\n     * delete\n       * 描述： 删除保存的对话检查点。\n       * 用法： /chat delete <tag>\n\n * /clear\n   \n   * 描述： 清除终端屏幕，包括 CLI 中可见的会话历史和回滚。根据确切的实现，底层会话数据（用于历史回忆）可能会被保留，但视觉显示会被清除。\n   * 键盘快捷键： 随时按 Ctrl+L 执行清除操作。\n\n * /compress\n   \n   * 描述： 用摘要替换整个聊天上下文。这可以节省用于未来任务的令牌，同时保留已发生事件的高级摘要。\n\n * /copy\n   \n   * 描述： 将 VeCLI 生成的最后一个输出复制到您的剪贴板，以便轻松共享或重用。\n   * 注意： 此命令需要安装特定于平台的剪贴板工具。\n     * 在 Linux 上，它需要 xclip 或 xsel。您通常可以使用系统的包管理器安装它们。\n     * 在 macOS 上，它需要 pbcopy，在 Windows 上，它需要 clip。这些工具通常在其各自的系统上预安装。\n\n * /directory (或 /dir)\n   \n   * 描述： 管理工作区目录以支持多目录。\n   * 子命令：\n     * add:\n       * 描述： 将目录添加到工作区。路径可以是绝对路径或相对于当前工作目录的路径。此外，还支持从主目录引用。\n       * 用法： /directory add <path1>,<path2>\n       * 注意： 在限制性沙盒配置文件中禁用。如果您正在使用该配置文件，请在启动会话时改用 --include-directories。\n     * show:\n       * 描述： 显示由 /directory add 和 --include-directories 添加的所有目录。\n       * 用法： /directory show\n\n * /editor\n   \n   * 描述： 打开一个对话框以选择支持的编辑器。\n\n * /extensions\n   \n   * 描述： 列出当前 VeCLI 会话中的所有活动扩展。请参阅 VeCLI 扩展。\n\n * /help (或 /?)\n   \n   * 描述： 显示有关 VeCLI 的帮助信息，包括可用命令及其用法。\n\n * /mcp\n   \n   * 描述： 列出配置的模型上下文协议 (MCP) 服务器、其连接状态、服务器详细信息和可用工具。\n   * 子命令：\n     * desc 或 descriptions:\n       * 描述： 显示 MCP 服务器和工具的详细描述。\n     * nodesc 或 nodescriptions:\n       * 描述： 隐藏工具描述，仅显示工具名称。\n     * schema:\n       * 描述： 显示工具配置参数的完整 JSON 模式。\n   * 键盘快捷键： 随时按 Ctrl+T 在显示和隐藏工具描述之间切换。\n\n * /memory\n   \n   * 描述： 管理 AI 的指令上下文（从 VE.md 文件加载的分层内存）。\n   * 子命令：\n     * add:\n       * 描述： 将以下文本添加到 AI 的内存中。用法: /memory add <要记住的文本>\n     * show:\n       * 描述： 显示从所有 VE.md 文件加载的当前分层内存的完整连接内容。这使您可以检查提供给火山引擎模型的指令上下文。\n     * refresh:\n       * 描述： 从配置位置（全局、项目/祖先和子目录）找到的所有 VE.md 文件中重新加载分层指令内存。此命令使用最新的 VE.md 内容更新模型。\n     * 注意： 有关 VE.md 文件如何贡献于分层内存的更多详细信息，请参阅 CLI 配置文档。\n\n * /restore\n   \n   * 描述： 将项目文件恢复到执行工具之前的状态。这对于撤销工具进行的文件编辑特别有用。如果在没有工具调用 ID 的情况下运行，它将列出可用于恢复的检查点。\n   * 用法： /restore [tool_call_id]\n   * 注意： 仅在使用 --checkpointing 选项调用 CLI 或通过 设置 配置时可用。有关更多详细信息，请参阅 检查点文档。\n\n * /settings\n   \n   * 描述： 打开设置编辑器以查看和修改 VeCLI 设置。\n   * 详细信息： 此命令提供了一个用户友好的界面，用于更改控制 VeCLI 行为和外观的设置。它等效于手动编辑 .ve/settings.json\n     文件，但具有验证和指导以防止错误。\n   * 用法： 只需运行\n     /settings，编辑器就会打开。然后您可以浏览或搜索特定设置，查看其当前值，并根据需要进行修改。某些设置的更改会立即应用，而其他设置则需要重新启动\n     。\n\n * /stats\n   \n   * 描述： 显示当前 VeCLI\n     会话的详细统计信息，包括令牌使用量、缓存令牌节省（如果可用）和会话持续时间。注意：仅在使用缓存令牌时才显示缓存令牌信息，这在使用 API\n     密钥身份验证时会发生，但目前在使用 OAuth 身份验证时不会发生。\n\n * /theme\n   \n   * 描述： 打开一个对话框，让您可以更改 VeCLI 的视觉主题。\n\n * /auth\n   \n   * 描述： 打开一个对话框，让您可以更改身份验证方法。\n\n * /about\n   \n   * 描述： 显示版本信息。请在提交问题时分享此信息。\n\n * /tools\n   \n   * 描述： 显示当前在 VeCLI 中可用的工具列表。\n   * 用法： /tools [desc]\n   * 子命令：\n     * desc 或 descriptions:\n       * 描述： 显示每个工具的详细描述，包括每个工具的名称及其提供给模型的完整描述。\n     * nodesc 或 nodescriptions:\n       * 描述： 隐藏工具描述，仅显示工具名称。\n\n * /privacy\n   \n   * 描述： 显示隐私声明并允许用户选择是否同意为服务改进目的收集其数据。\n\n * /quit (或 /exit)\n   \n   * 描述： 退出 VeCLI。\n\n * /vim\n   \n   * 描述： 打开或关闭 vim 模式。启用 vim 模式时，输入区域支持 NORMAL 和 INSERT 模式下的 vim 风格导航和编辑命令。\n   * 功能：\n     * NORMAL 模式： 使用 h、j、k、l 导航；使用 w、b、e 按单词跳转；使用 0、$、^ 转到行首/行尾；使用 G（或 gg\n       转到第一行）转到特定行\n     * INSERT 模式： 标准文本输入，按 escape 返回 NORMAL 模式\n     * 编辑命令： 使用 x 删除，使用 c 更改，使用 i、a、o、O 插入；复杂的操作如 dd、cc、dw、cw\n     * 计数支持： 用数字作为前缀命令（例如，3h、5w、10G）\n     * 重复最后命令： 使用 . 重复最后的编辑操作\n     * 持久设置： Vim 模式偏好设置保存到 ~/.ve/settings.json 并在会话之间恢复\n   * 状态指示器： 启用时，在页脚中显示 [NORMAL] 或 [INSERT]\n\n * /init\n   \n   * 描述： 为了帮助用户轻松创建 VE.md 文件，此命令分析当前目录并生成一个量身定制的上下文文件，使他们更容易向火山引擎代理提供项目特定的指令。\n\n\n自定义命令#\n\n有关快速入门，请参阅下面的 示例。\n\n自定义命令允许您将最喜欢或最常用的提示保存为 VeCLI\n中的个人快捷方式。您可以创建特定于单个项目或在所有项目中全局可用的命令，从而简化您的工作流程并确保一致性。\n\n文件位置和优先级#\n\nVeCLI 从两个位置发现命令，按特定顺序加载：\n\n 1. 用户命令 (全局): 位于 ~/.ve/commands/。这些命令在您处理的任何项目中都可用。\n 2. 项目命令 (本地): 位于 ~/.ve/commands/。这些命令特定于当前项目，可以签入版本控制以与您的团队共享。\n\n如果项目目录中的命令与用户目录中的命令同名，则始终使用项目命令。 这允许项目用特定于项目的版本覆盖全局命令。\n\n命名和命名空间#\n\n命令的名称由其相对于其 commands 目录的文件路径确定。子目录用于创建命名空间命令，路径分隔符 (/ 或 \\\\) 转换为冒号 (:)。\n\n * ~/.ve/commands/test.toml 文件成为命令 /test。\n * <project>/.ve/commands/git/commit.toml 文件成为命名空间命令 /git:commit。\n\nTOML 文件格式 (v1)#\n\n您的命令定义文件必须以 TOML 格式编写并使用 .toml 文件扩展名。\n\n必需字段#\n\n * prompt (字符串): 执行命令时发送给火山引擎模型的提示。这可以是单行或多行字符串。\n\n可选字段#\n\n * description (字符串): 命令功能的简短一行描述。此文本将显示在 /help 菜单中您的命令旁边。如果您省略此字段，将从文件名生成通用描述。\n\n处理参数#\n\n自定义命令支持两种强大的参数处理方法。CLI 根据命令 prompt 的内容自动选择正确的方法。\n\n1. 使用 {{args}} 的上下文感知注入#\n\n如果您的 prompt 包含特殊占位符 {{args}}，CLI 将用用户在命令名称后键入的文本替换该占位符。\n\n这种注入的行为取决于其使用位置：\n\nA. 原始注入 (在 Shell 命令外部)\n\n在提示的主体中使用时，参数会按用户键入的方式精确注入。\n\n示例 (git/fix.toml):\n\n\n\n模型收到: 请为这里描述的问题提供代码修复: \"按钮未对齐\"。\n\nB. 在 Shell 命令中使用参数 (在 !{...} 块内)\n\n当您在 shell 注入块 (!{...}) 内使用 {{args}} 时，参数在替换前会自动进行 shell 转义。这允许您安全地将参数传递给 shell\n命令，确保生成的命令在语法上正确且安全，同时防止命令注入漏洞。\n\n示例 (/grep-code.toml):\n\n\n\n当您运行 /grep-code It's complicated 时：\n\n 1. CLI 看到 {{args}} 在 !{...} 的内外都使用了。\n 2. 外部: 第一个 {{args}} 被原始替换为 It's complicated。\n 3. 内部: 第二个 {{args}} 被替换为转义版本（例如，在 Linux 上: \"It's complicated\"）。\n 4. 执行的命令是 grep -r \"It's complicated\" .。\n 5. CLI 在执行前会提示您确认这个确切的安全命令。\n 6. 最终提示被发送。\n\n2. 默认参数处理#\n\n如果您的 prompt 不包含特殊占位符 {{args}}，CLI 使用默认行为来处理参数。\n\n如果您为命令提供参数（例如，/mycommand arg1），CLI\n会将您键入的完整命令附加到提示的末尾，用两个换行符分隔。这允许模型看到原始指令和您刚刚提供的特定参数。\n\n如果您不提供任何参数（例如，/mycommand），提示会按原样发送给模型，不附加任何内容。\n\n示例 (changelog.toml):\n\n此示例展示了如何通过为模型定义角色、解释在哪里找到用户输入以及指定预期格式和行为来创建一个强大的命令。\n\n\n\n当您运行 /changelog 1.2.0 added \"新功能\" 时，发送给模型的最终文本将是原始提示，后跟两个换行符和您键入的命令。\n\n3. 使用 !{...} 执行 Shell 命令#\n\n您可以通过在 prompt 中直接执行 shell 命令并注入其输出，使您的命令具有动态性。这对于从本地环境收集上下文（如读取文件内容或检查 Git\n状态）非常理想。\n\n当自定义命令尝试执行 shell 命令时，VeCLI 现在会在继续之前提示您确认。这是一项安全措施，以确保只能运行预期的命令。\n\n工作原理:\n\n 1. 注入命令： 使用 !{...} 语法。\n 2. 参数替换： 如果 {{args}} 存在于块内，则会自动进行 shell 转义（参见上面的 上下文感知注入）。\n 3. 健壮解析： 解析器可以正确处理包含嵌套大括号的复杂 shell 命令，例如 JSON 有效载荷。注意： !{...} 内的内容必须具有平衡的大括号 ({\n    和 })。如果需要执行包含不平衡大括号的命令，请考虑将它包装在外部脚本文件中，并在 !{...} 块中调用该脚本。\n 4. 安全检查和确认： CLI 对最终的、已解析的命令（在参数转义和替换后）执行安全检查。将出现一个对话框，显示要执行的确切命令。\n 5. 执行和错误报告： 命令被执行。如果命令失败，注入到提示中的输出将包括错误消息 (stderr)，后跟状态行，例如，[Shell 命令以代码 1\n    退出]。这有助于模型理解失败的上下文。\n\n示例 (git/commit.toml):\n\n此命令获取暂存的 git diff，并使用它来要求模型编写提交消息。\n\n\n\n当您运行 /git:commit 时，CLI 首先执行 git diff --staged，然后在将最终的完整提示发送给模型之前，用该命令的输出替换 !{git\ndiff --staged}。\n\n4. 使用 @{...} 注入文件内容#\n\n您可以使用 @{...} 语法直接将文件或目录列表的内容嵌入到您的提示中。这对于创建对特定文件进行操作的命令非常有用。\n\n工作原理:\n\n * 文件注入: @{path/to/file.txt} 被 file.txt 的内容替换。\n * 多模态支持: 如果路径指向受支持的图像（例如\n   PNG、JPEG）、PDF、音频或视频文件，它将被正确编码并作为多模态输入注入。其他二进制文件会被优雅地处理并跳过。\n * 目录列表: @{path/to/dir} 被遍历，目录内及其所有子目录中的每个文件都会插入到提示中。如果启用了，则会遵守 .gitignore 和\n   .veignore。\n * 工作区感知: 命令会在当前目录和任何其他工作区目录中搜索路径。如果在工作区内，绝对路径是允许的。\n * 处理顺序: 使用 @{...} 进行的文件内容注入在 shell 命令 (!{...}) 和参数替换 ({{args}}) 之前进行处理。\n * 解析: 解析器要求 @{...} 内的内容（路径）具有平衡的大括号 ({ 和 })。\n\n示例 (review.toml):\n\n此命令注入一个 固定 的最佳实践文件 (docs/best-practices.md) 的内容，并使用用户的参数为审查提供上下文。\n\n\n\n当您运行 /review FileCommandLoader.ts 时，@{docs/best-practices.md}\n占位符被该文件的内容替换，{{args}} 被您提供的文本替换，然后将最终提示发送给模型。\n\n--------------------------------------------------------------------------------\n\n示例: \"纯函数\" 重构命令#\n\n让我们创建一个全局命令，要求模型重构一段代码。\n\n1. 创建文件和目录:\n\n首先，确保用户命令目录存在，然后创建一个 refactor 子目录用于组织和最终的 TOML 文件。\n\n\n\n2. 将内容添加到文件中:\n\n在您的编辑器中打开 ~/.ve/commands/refactor/pure.toml 并添加以下内容。我们包括了可选的 description\n以获得最佳实践。\n\n\n\n3. 运行命令:\n\n就是这样！您现在可以在 CLI 中运行您的命令。首先，您可能会将一个文件添加到上下文中，然后调用您的命令：\n\n\n\nVeCLI 然后将执行在您的 TOML 文件中定义的多行提示。\n\n\nAt 命令 (@)#\n\nAt 命令用于将文件或目录的内容作为您对火山引擎的提示的一部分包含进来。这些命令包括 git 感知过滤。\n\n * @<文件或目录的路径>\n   \n   * 描述： 将指定文件或文件的内容注入到您的当前提示中。这对于询问有关特定代码、文本或文件集合的问题很有用。\n   * 示例：\n     * @path/to/your/file.txt 解释这段文本。\n     * @src/my_project/ 总结此目录中的代码。\n     * 这个文件是关于什么的？ @README.md\n   * 详细信息：\n     * 如果提供了单个文件的路径，则读取该文件的内容。\n     * 如果提供了目录的路径，则命令会尝试读取该目录及其任何子目录中的文件内容。\n     * 路径中的空格应使用反斜杠进行转义（例如，@My\\\\ Documents/file.txt）。\n     * 该命令在内部使用 read_many_files 工具。内容被获取，然后在发送到 模型之前插入到您的查询中。\n     * Git 感知过滤： 默认情况下，git 忽略的文件（如 node_modules/、dist/、.env、.git/）会被排除。可以通过\n       context.fileFiltering 设置更改此行为。\n     * 文件类型： 该命令适用于基于文本的文件。虽然它可能会尝试读取任何文件，但底层的 read_many_files\n       工具可能会跳过或截断二进制文件或非常大的文件，以确保性能和相关性。该工具会指示哪些文件被跳过。\n   * 输出： CLI 将显示一条工具调用消息，指示使用了 read_many_files，以及一条详细说明状态和处理路径的消息。\n\n * **@ (单独的 at 符号)\n   \n   * 描述： 如果您输入一个单独的 @ 符号而没有路径，查询将按原样传递给火山引擎模型。如果您在提示中特别谈论 @ 符号，这可能很有用。\n\n\n@ 命令的错误处理#\n\n * 如果 @ 之后指定的路径未找到或无效，将显示错误消息，并且查询可能不会发送到 模型，或者会在没有文件内容的情况下发送。\n * 如果 read_many_files 工具遇到错误（例如，权限问题），也会报告此错误。\n\n\nShell 模式和直通命令 (!)#\n\n! 前缀允许您直接从 VeCLI 内与系统的 shell 交互。\n\n * !<shell_command>\n   \n   * 描述： 使用 bash 在 Linux/macOS 上或使用 cmd.exe 在 Windows 上执行给定的\n     <shell_command>。命令的任何输出或错误都会显示在终端中。\n   * 示例：\n     * !ls -la (执行 ls -la 并返回 VeCLI)\n     * !git status (执行 git status 并返回 VeCLI)\n\n * ! (切换 shell 模式)\n   \n   * 描述： 单独输入 ! 可切换 shell 模式。\n     * 进入 shell 模式：\n       * 激活时，shell 模式使用不同的着色和 \"Shell 模式指示器\"。\n       * 在 shell 模式下，您键入的文本直接解释为 shell 命令。\n     * 退出 shell 模式：\n       * 退出时，UI 恢复到标准外观，正常的 VeCLI 行为恢复。\n\n * 所有 ! 用法的注意事项： 您在 shell 模式下执行的命令具有与直接在终端中运行它们相同的权限和影响。\n\n * 环境变量： 当通过 ! 或在 shell 模式下执行命令时，GEMINI_CLI=1 环境变量会在子进程的环境中设置。这允许脚本或工具检测它们是否在\n   VeCLI 内运行。","routePath":"/zh/cli/commands","lang":"","toc":[{"text":"斜杠命令 (`/`)","id":"斜杠命令-","depth":2,"charIndex":-1},{"text":"内置命令","id":"内置命令","depth":3,"charIndex":111},{"text":"自定义命令","id":"自定义命令","depth":3,"charIndex":4108},{"text":"文件位置和优先级","id":"文件位置和优先级","depth":4,"charIndex":4219},{"text":"命名和命名空间","id":"命名和命名空间","depth":4,"charIndex":4430},{"text":"TOML 文件格式 (v1)","id":"toml-文件格式-v1","depth":4,"charIndex":4621},{"text":"处理参数","id":"处理参数","depth":4,"charIndex":4822},{"text":"示例: \"纯函数\" 重构命令","id":"示例-纯函数-重构命令","depth":4,"charIndex":7377},{"text":"At 命令 (`@`)","id":"at-命令-","depth":2,"charIndex":-1},{"text":"`@` 命令的错误处理","id":"-命令的错误处理","depth":3,"charIndex":-1},{"text":"Shell 模式和直通命令 (`!`)","id":"shell-模式和直通命令-","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":81,"title":"VeCLI 配置","content":"#\n\n关于已弃用的配置格式的说明\n\n本文档描述了 settings.json 文件的旧版 v1 格式。此格式现已弃用。\n\n * 新格式将在 [09/10/25] 开始的稳定版本中得到支持。\n * 从旧格式到新格式的自动迁移将在 [09/17/25] 开始。\n\n有关新推荐格式的详细信息，请参阅 当前配置文档。\n\nVeCLI 提供了几种配置其行为的方法，包括环境变量、命令行参数和设置文件。本文档概述了不同的配置方法和可用设置。\n\n\n配置层#\n\n配置按以下优先级顺序应用（较低的数字会被较高的数字覆盖）：\n\n 1. 默认值： 应用程序内的硬编码默认值。\n 2. 系统默认文件： 系统范围的默认设置，可以被其他设置文件覆盖。\n 3. 用户设置文件： 当前用户的全局设置。\n 4. 项目设置文件： 项目特定的设置。\n 5. 系统设置文件： 系统范围的设置，覆盖所有其他设置文件。\n 6. 环境变量： 系统范围或会话特定的变量，可能从 .env 文件加载。\n 7. 命令行参数： 启动 CLI 时传递的值。\n\n\n设置文件#\n\nVeCLI 使用 JSON 设置文件进行持久配置。这些文件有四个位置：\n\n * 系统默认文件：\n   * 位置： /etc/gemini-cli/system-defaults.json (Linux),\n     C:\\ProgramData\\gemini-cli\\system-defaults.json (Windows) 或\n     /Library/Application Support/GeminiCli/system-defaults.json (macOS)。路径可以使用\n     GEMINI_CLI_SYSTEM_DEFAULTS_PATH 环境变量覆盖。\n   * 范围： 提供系统范围默认设置的基础层。这些设置具有最低优先级，旨在被用户、项目或系统覆盖设置覆盖。\n * 用户设置文件：\n   * 位置： ~/.ve/settings.json (其中 ~ 是您的主目录)。\n   * 范围： 适用于当前用户的所有 VeCLI 会话。用户设置覆盖系统默认值。\n * 项目设置文件：\n   * 位置： 项目根目录中的 .ve/settings.json。\n   * 范围： 仅在从该特定项目运行 VeCLI 时应用。项目设置覆盖用户设置和系统默认值。\n * 系统设置文件：\n   * 位置： /etc/gemini-cli/settings.json (Linux),\n     C:\\ProgramData\\gemini-cli\\settings.json (Windows) 或 /Library/Application\n     Support/GeminiCli/settings.json (macOS)。路径可以使用\n     GEMINI_CLI_SYSTEM_SETTINGS_PATH 环境变量覆盖。\n   * 范围： 适用于系统上的所有 VeCLI\n     会话，适用于所有用户。系统设置作为覆盖，优先于所有其他设置文件。对于企业中的系统管理员来说，可能有用，可以控制用户的 VeCLI 设置。\n\n关于设置中的环境变量的说明： 您的 settings.json 文件中的字符串值可以使用 $VAR_NAME 或 ${VAR_NAME}\n语法引用环境变量。加载设置时，这些变量将自动解析。例如，如果您有一个环境变量 MY_API_TOKEN，您可以在 settings.json\n中这样使用它：\"apiKey\": \"$MY_API_TOKEN\"。\n\n> 企业用户须知： 有关在企业环境中部署和管理 VeCLI 的指导，请参阅 企业配置 文档。\n\n\n项目中的 .ve 目录#\n\n除了项目设置文件外，项目的 .ve 目录还可以包含与 VeCLI 操作相关的其他项目特定文件，例如：\n\n * 自定义沙盒配置文件 (例如，.ve/sandbox-macos-custom.sb, .ve/sandbox.Dockerfile)。\n\n\nsettings.json 中的可用设置：#\n\n * contextFileName (字符串或字符串数组)：\n   \n   * 描述： 指定上下文文件的文件名（例如，VE.md、AGENTS.md）。可以是单个文件名或接受的文件名列表。\n   * 默认值： VE.md\n   * 示例： \"contextFileName\": \"AGENTS.md\"\n\n * bugCommand (对象)：\n   \n   * 描述： 覆盖 /bug 命令的默认 URL。\n   * 默认值： \"urlTemplate\":\n     \"https://github.com/google-gemini/gemini-cli/issues/new?template=bug_report\n     .yml&title={title}&info={info}\"\n   * 属性：\n     * urlTemplate (字符串)：可以包含 {title} 和 {info} 占位符的 URL。\n   * 示例：\n     \n     \n\n * fileFiltering (对象)：\n   \n   * 描述： 控制 @ 命令和文件发现工具的 git 感知文件过滤行为。\n   * 默认值： \"respectGitIgnore\": true, \"enableRecursiveFileSearch\": true\n   * 属性：\n     * respectGitIgnore (布尔值)：在发现文件时是否尊重 .gitignore 模式。当设置为 true 时，git 忽略的文件（如\n       node_modules/、dist/、.env）会自动从 @ 命令和文件列表操作中排除。\n     * enableRecursiveFileSearch (布尔值)：在提示中完成 @ 前缀时，是否启用在当前树下递归搜索文件名。\n     * disableFuzzySearch (布尔值)：当 true 时，在搜索文件时禁用模糊搜索功能，这可以提高文件数量庞大的项目的性能。\n   * 示例：\n     \n     \n\n\n排查文件搜索性能问题#\n\n如果您遇到文件搜索性能问题（例如，使用 @ 补全），尤其是在文件数量非常庞大的项目中，您可以尝试以下几种方法，按推荐顺序排列：\n\n 1. 使用 .veignore： 在您的项目根目录中创建一个 .veignore\n    文件，以排除包含大量您不需要引用的文件的目录（例如，构建产物、日志、node_modules）。减少爬取的文件总数是提高性能的最有效方法。\n\n 2. 禁用模糊搜索： 如果忽略文件还不够，您可以通过在 settings.json 文件中将 disableFuzzySearch 设置为 true\n    来禁用模糊搜索。这将使用一个更简单的、非模糊的匹配算法，速度可能会更快。\n\n 3. 禁用递归文件搜索： 作为最后的手段，您可以通过将 enableRecursiveFileSearch 设置为 false\n    来完全禁用递归文件搜索。这将是最快的选项，因为它避免了对项目的递归爬取。但是，这意味着您在使用 @ 补全时需要键入文件的完整路径。\n\n * coreTools (字符串数组)：\n   \n   * 描述： 允许您指定应提供给模型的核心工具名称列表。这可以用于限制内置工具集。有关核心工具列表，请参阅\n     内置工具。您还可以为支持它的工具指定命令特定的限制，例如 ShellTool。例如，\"coreTools\": [\"ShellTool(ls -l)\"]\n     将只允许执行 ls -l 命令。\n   * 默认值： 所有可供 Gemini 模型使用的核心工具。\n   * 示例： \"coreTools\": [\"ReadFileTool\", \"GlobTool\", \"ShellTool(ls)\"]。\n\n * allowedTools (字符串数组)：\n   \n   * 默认值： undefined\n   * 描述： 将绕过确认对话框的工具名称列表。这对于您信任并经常使用的工具有用。匹配语义与 coreTools 相同。\n   * 示例： \"allowedTools\": [\"ShellTool(git status)\"]。\n\n * excludeTools (字符串数组)：\n   \n   * 描述： 允许您指定应从模型中排除的核心工具名称列表。在 excludeTools 和 coreTools\n     中都列出的工具将被排除。您还可以为支持它的工具指定命令特定的限制，例如 ShellTool。例如，\"excludeTools\":\n     [\"ShellTool(rm -rf)\"] 将阻止 rm -rf 命令。\n   * 默认值：不排除任何工具。\n   * 示例： \"excludeTools\": [\"run_shell_command\", \"findFiles\"]。\n   * 安全说明： run_shell_command 的 excludeTools\n     中的命令特定限制基于简单的字符串匹配，很容易被绕过。此功能不是安全机制，不应依赖它来安全地执行不受信任的代码。建议使用 coreTools\n     明确选择可以执行的命令。\n\n * allowMCPServers (字符串数组)：\n   \n   * 描述： 允许您指定应提供给模型的 MCP 服务器名称列表。这可以用于限制要连接的 MCP 服务器集。请注意，如果设置了\n     --allowed-mcp-server-names，这将被忽略。\n   * 默认值： 所有 MCP 服务器都可供 Gemini 模型使用。\n   * 示例： \"allowMCPServers\": [\"myPythonServer\"]。\n   * 安全说明： 这在 MCP 服务器名称上使用简单的字符串匹配，可以被修改。如果您是系统管理员并希望防止用户绕过此限制，请考虑在系统设置级别配置\n     mcpServers，这样用户将无法配置自己的任何 MCP 服务器。这不应被用作万无一失的安全机制。\n\n * excludeMCPServers (字符串数组)：\n   \n   * 描述： 允许您指定应从模型中排除的 MCP 服务器名称列表。在 excludeMCPServers 和 allowMCPServers\n     中都列出的服务器将被排除。请注意，如果设置了 --allowed-mcp-server-names，这将被忽略。\n   * 默认值：不排除任何 MCP 服务器。\n   * 示例： \"excludeMCPServers\": [\"myNodeServer\"]。\n   * 安全说明： 这在 MCP 服务器名称上使用简单的字符串匹配，可以被修改。如果您是系统管理员并希望防止用户绕过此限制，请考虑在系统设置级别配置\n     mcpServers，这样用户将无法配置自己的任何 MCP 服务器。这不应被用作万无一失的安全机制。\n\n * autoAccept (布尔值)：\n   \n   * 描述： 控制 CLI 是否自动接受和执行被认为是安全的工具调用（例如，只读操作），而无需明确的用户确认。如果设置为 true，CLI\n     将绕过被认为是安全的工具的确认提示。\n   * 默认值： false\n   * 示例： \"autoAccept\": true\n\n * theme (字符串)：\n   \n   * 描述： 设置 VeCLI 的视觉 主题。\n   * 默认值： \"Default\"\n   * 示例： \"theme\": \"GitHub\"\n\n * vimMode (布尔值)：\n   \n   * 描述： 启用或禁用用于输入编辑的 vim 模式。启用后，输入区域支持具有 NORMAL 和 INSERT 模式的 vim 风格导航和编辑命令。vim\n     模式状态显示在页脚中，并在会话之间保持。\n   * 默认值： false\n   * 示例： \"vimMode\": true\n\n * sandbox (布尔值或字符串)：\n   \n   * 描述： 控制是否以及如何使用沙盒进行工具执行。如果设置为 true，VeCLI 使用预构建的 gemini-cli-sandbox Docker\n     镜像。有关更多信息，请参阅 沙盒。\n   * 默认值： false\n   * 示例： \"sandbox\": \"docker\"\n\n * toolDiscoveryCommand (字符串)：\n   \n   * 描述： 定义一个用于从项目中发现工具的自定义 shell 命令。shell 命令必须在 stdout 上返回一个 函数声明 的 JSON\n     数组。工具包装器是可选的。\n   * 默认值： 空\n   * 示例： \"toolDiscoveryCommand\": \"bin/get_tools\"\n\n * toolCallCommand (字符串)：\n   \n   * 描述： 定义一个用于调用使用 toolDiscoveryCommand 发现的特定工具的自定义 shell 命令。shell 命令必须满足以下条件：\n     * 它必须将函数 name（与 函数声明 中的完全相同）作为第一个命令行参数。\n     * 它必须在 stdin 上以 JSON 格式读取函数参数，类似于 functionCall.args。\n     * 它必须在 stdout 上以 JSON 格式返回函数输出，类似于 functionResponse.response.content。\n   * 默认值： 空\n   * 示例： \"toolCallCommand\": \"bin/call_tool\"\n\n * mcpServers (对象)：\n   \n   * 描述： 配置与一个或多个模型上下文协议 (MCP) 服务器的连接，用于发现和使用自定义工具。VeCLI 尝试连接到每个配置的 MCP\n     服务器以发现可用工具。如果多个 MCP\n     服务器暴露了同名工具，则工具名称将使用您在配置中定义的服务器别名作为前缀（例如，serverAlias__actualToolName）以避免冲突。请\n     注意，系统可能会为了兼容性而从 MCP 工具定义中剥离某些模式属性。必须至少提供 command、url 或 httpUrl\n     之一。如果指定了多个，则优先级顺序为 httpUrl，然后是 url，然后是 command。\n   * 默认值： 空\n   * 属性：\n     * <SERVER_NAME> (对象)：命名服务器的服务器参数。\n       * command (字符串, 可选)：通过标准 I/O 启动 MCP 服务器的命令。\n       * args (字符串数组, 可选)：传递给命令的参数。\n       * env (对象, 可选)：为服务器进程设置的环境变量。\n       * cwd (字符串, 可选)：启动服务器的工作目录。\n       * url (字符串, 可选)：使用服务器发送事件 (SSE) 进行通信的 MCP 服务器的 URL。\n       * httpUrl (字符串, 可选)：使用可流式 HTTP 进行通信的 MCP 服务器的 URL。\n       * headers (对象, 可选)：发送到 url 或 httpUrl 的请求的 HTTP 标头映射。\n       * timeout (数字, 可选)：对此 MCP 服务器的请求超时时间（毫秒）。\n       * trust (布尔值, 可选)：信任此服务器并绕过所有工具调用确认。\n       * description (字符串, 可选)：服务器的简要描述，可能用于显示目的。\n       * includeTools (字符串数组, 可选)：要从此 MCP\n         服务器包含的工具名称列表。指定时，只有此处列出的工具将从此服务器可用（允许列表行为）。如果未指定，则默认启用服务器中的所有工具。\n       * excludeTools (字符串数组, 可选)：要从此 MCP\n         服务器排除的工具名称列表。此处列出的工具将不可用，即使服务器暴露了它们。注意： excludeTools 优先于 includeTools -\n         如果一个工具在两个列表中，它将被排除。\n   * 示例：\n     \n     \n\n * checkpointing (对象)：\n   \n   * 描述： 配置检查点功能，该功能允许您保存和恢复对话和文件状态。有关更多详细信息，请参阅 检查点文档。\n   * 默认值： {\"enabled\": false}\n   * 属性：\n     * enabled (布尔值)：当 true 时，/restore 命令可用。\n\n * preferredEditor (字符串)：\n   \n   * 描述： 指定用于查看差异的首选编辑器。\n   * 默认值： vscode\n   * 示例： \"preferredEditor\": \"vscode\"\n\n * telemetry (对象)\n   \n   * 描述： 配置 VeCLI 的日志记录和指标收集。有关更多信息，请参阅 遥测。\n   * 默认值： {\"enabled\": false, \"target\": \"local\", \"otlpEndpoint\":\n     \"http://localhost:4317\", \"logPrompts\": true}\n   * 属性：\n     * enabled (布尔值)：是否启用遥测。\n     * target (字符串)：收集的遥测数据的目的地。支持的值为 local 和 gcp。\n     * otlpEndpoint (字符串)：OTLP 导出器的端点。\n     * logPrompts (布尔值)：是否在日志中包含用户提示的内容。\n   * 示例：\n     \n     \n\n * usageStatisticsEnabled (布尔值)：\n   \n   * 描述： 启用或禁用使用统计信息的收集。有关更多信息，请参阅 使用统计。\n   * 默认值： true\n   * 示例：\n     \n     \n\n * hideTips (布尔值)：\n   \n   * 描述： 启用或禁用 CLI 界面中的有用提示。\n   \n   * 默认值： false\n   \n   * 示例：\n     \n     \n\n * hideBanner (布尔值)：\n   \n   * 描述： 启用或禁用 CLI 界面中的启动横幅（ASCII 艺术徽标）。\n   \n   * 默认值： false\n   \n   * 示例：\n     \n     \n\n * maxSessionTurns (数字)：\n   \n   * 描述： 设置会话的最大轮数。如果会话超过此限制，CLI 将停止处理并开始新的聊天。\n   * 默认值： -1 (无限制)\n   * 示例：\n     \n     \n\n * summarizeToolOutput (对象)：\n   \n   * 描述： 启用或禁用工具输出的摘要。您可以使用 tokenBudget 设置指定摘要的令牌预算。\n   * 注意：目前仅支持 run_shell_command 工具。\n   * 默认值： {} (默认禁用)\n   * 示例：\n     \n     \n\n * excludedProjectEnvVars (字符串数组)：\n   \n   * 描述： 指定应从项目 .env 文件加载中排除的环境变量。这可以防止项目特定的环境变量（如 DEBUG=true）干扰 gemini-cli\n     行为。.ve/.env 文件中的变量永远不会被排除。\n   * 默认值： [\"DEBUG\", \"DEBUG_MODE\"]\n   * 示例：\n     \n     \n\n * includeDirectories (字符串数组)：\n   \n   * 描述： 指定要包含在工作区上下文中的其他绝对或相对路径数组。默认情况下，缺少的目录将被跳过并发出警告。路径可以使用 ~\n     来引用用户的主目录。此设置可以与 --include-directories 命令行标志结合使用。\n   * 默认值： []\n   * 示例：\n     \n     \n\n * loadMemoryFromIncludeDirectories (布尔值)：\n   \n   * 描述： 控制 /memory refresh 命令的行为。如果设置为 true，VE.md 文件应从所有添加的目录加载。如果设置为\n     false，VE.md 应仅从当前目录加载。\n   * 默认值： false\n   * 示例：\n     \n     \n\n * chatCompression (对象)：\n   \n   * 描述： 控制聊天历史压缩的设置，包括自动压缩和通过 /compress 命令手动调用压缩。\n   * 属性：\n     * contextPercentageThreshold (数字)：介于 0 和 1\n       之间的值，指定压缩的令牌阈值，作为模型总令牌限制的百分比。例如，值为 0.6 将在聊天历史超过令牌限制的 60% 时触发压缩。\n   * 示例：\n     \n     \n\n * showLineNumbers (布尔值)：\n   \n   * 描述： 控制是否在 CLI 输出的代码块中显示行号。\n   * 默认值： true\n   * 示例：\n     \n     \n\n * accessibility (对象)：\n   \n   * 描述： 配置 CLI 的辅助功能。\n   * 属性：\n     * screenReader (布尔值)：启用屏幕阅读器模式，调整 TUI 以更好地兼容屏幕阅读器。这也可以通过 --screen-reader\n       命令行标志启用，该标志将优先于设置。\n     * disableLoadingPhrases (布尔值)：禁用操作期间加载短语的显示。\n   * 默认值： {\"screenReader\": false, \"disableLoadingPhrases\": false}\n   * 示例：\n     \n     \n\n\n示例 settings.json：#\n\n\n\n\nShell 历史#\n\nCLI 会保留您运行的 shell 命令的历史记录。为了避免不同项目之间的冲突，此历史记录存储在用户主文件夹中的项目特定目录中。\n\n * 位置： ~/.ve/tmp/<project_hash>/shell_history\n   * <project_hash> 是从您的项目根路径生成的唯一标识符。\n   * 历史记录存储在名为 shell_history 的文件中。\n\n\n环境变量和 .env 文件#\n\n环境变量是配置应用程序的常见方式，特别是对于 API 密钥等敏感信息或在不同环境之间可能更改的设置。有关身份验证设置，请参阅\n身份验证文档，其中涵盖了所有可用的身份验证方法。\n\nCLI 会自动从 .env 文件加载环境变量。加载顺序为：\n\n 1. 当前工作目录中的 .env 文件。\n 2. 如果未找到，它会向上搜索父目录，直到找到 .env 文件或到达项目根目录（由 .git 文件夹标识）或主目录。\n 3. 如果仍未找到，它会查找 ~/.env（在用户的主目录中）。\n\n环境变量排除： 某些环境变量（如 DEBUG 和 DEBUG_MODE）会自动从项目 .env 文件中排除，以防止干扰 gemini-cli\n行为。.ve/.env 文件中的变量永远不会被排除。您可以使用 settings.json 文件中的 excludedProjectEnvVars\n设置自定义此行为。\n\n * GEMINI_API_KEY:\n   * 您的 Gemini API 密钥。\n   * 几种可用的 身份验证方法 之一。\n   * 在您的 shell 配置文件（例如 ~/.bashrc、~/.zshrc）或 .env 文件中设置。\n * GEMINI_MODEL:\n   * 指定要使用的默认 Gemini 模型。\n   * 覆盖硬编码的默认值\n   * 示例: export GEMINI_MODEL=\"gemini-2.5-flash\"\n * GOOGLE_API_KEY:\n   * 您的 Google Cloud API 密钥。\n   * 在快速模式下使用 Vertex AI 所需。\n   * 确保您拥有必要的权限。\n   * 示例: export GOOGLE_API_KEY=\"YOUR_GOOGLE_API_KEY\"。\n * GOOGLE_CLOUD_PROJECT:\n   * 您的 Google Cloud 项目 ID。\n   * 使用代码助手或 Vertex AI 所需。\n   * 如果使用 Vertex AI，请确保您在此项目中拥有必要的权限。\n   * Cloud Shell 注意： 在 Cloud Shell 环境中运行时，此变量默认为分配给 Cloud Shell 用户的特殊项目。如果您在\n     Cloud Shell 的全局环境中设置了 GOOGLE_CLOUD_PROJECT，它将被此默认值覆盖。要在 Cloud Shell\n     中使用不同的项目，您必须在 .env 文件中定义 GOOGLE_CLOUD_PROJECT。\n   * 示例: export GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_ID\"。\n * GOOGLE_APPLICATION_CREDENTIALS (字符串):\n   * 描述： 您的 Google 应用程序凭据 JSON 文件的路径。\n   * 示例： export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your/credentials.json\"\n * OTLP_GOOGLE_CLOUD_PROJECT:\n   * 您的 Google Cloud 项目 ID，用于 Google Cloud 中的遥测\n   * 示例: export OTLP_GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_ID\"。\n * GOOGLE_CLOUD_LOCATION:\n   * 您的 Google Cloud 项目位置（例如，us-central1）。\n   * 在非快速模式下使用 Vertex AI 所需。\n   * 示例: export GOOGLE_CLOUD_LOCATION=\"YOUR_PROJECT_LOCATION\"。\n * GEMINI_SANDBOX:\n   * settings.json 中 sandbox 设置的替代方法。\n   * 接受 true、false、docker、podman 或自定义命令字符串。\n * SEATBELT_PROFILE (macOS 特定):\n   * 在 macOS 上切换 Seatbelt (sandbox-exec) 配置文件。\n   * permissive-open: (默认) 限制写入项目文件夹（和一些其他文件夹，参见\n     packages/cli/src/utils/sandbox-macos-permissive-open.sb）但允许其他操作。\n   * strict: 使用严格的配置文件，默认拒绝操作。\n   * <profile_name>: 使用自定义配置文件。要定义自定义配置文件，请在项目的 .ve/ 目录中创建一个名为\n     sandbox-macos-<profile_name>.sb\n     的文件（例如，my-project/.ve/sandbox-macos-custom.sb）。\n * DEBUG 或 DEBUG_MODE (通常由底层库或 CLI 本身使用):\n   * 设置为 true 或 1 以启用详细的调试日志记录，这对故障排除很有帮助。\n   * 注意： 默认情况下，这些变量会自动从项目 .env 文件中排除，以防止干扰 gemini-cli 行为。如果需要专门为 gemini-cli\n     设置这些，请使用 .ve/.env 文件。\n * NO_COLOR:\n   * 设置为任何值以禁用 CLI 中的所有颜色输出。\n * CLI_TITLE:\n   * 设置为字符串以自定义 CLI 的标题。\n * CODE_ASSIST_ENDPOINT:\n   * 指定代码助手服务器的端点。\n   * 这对开发和测试很有用。\n\n\n命令行参数#\n\n运行 CLI 时直接传递的参数可以覆盖该特定会话的其他配置。\n\n * --model <model_name> (-m <model_name>):\n   * 指定此会话要使用的 Gemini 模型。\n   * 示例: npm start -- --model gemini-1.5-pro-latest\n * --prompt <your_prompt> (-p <your_prompt>):\n   * 用于直接将提示传递给命令。这会在非交互模式下调用 VeCLI。\n * --prompt-interactive <your_prompt> (-i <your_prompt>):\n   * 以提供的提示作为初始输入启动交互式会话。\n   * 提示在交互式会话中处理，而不是在会话之前处理。\n   * 在从 stdin 管道输入时不能使用。\n   * 示例: gemini -i \"explain this code\"\n * --sandbox (-s):\n   * 为此会话启用沙盒模式。\n * --sandbox-image:\n   * 设置沙盒镜像 URI。\n * --debug (-d):\n   * 为此会话启用调试模式，提供更详细的输出。\n * --all-files (-a):\n   * 如果设置，则递归地将当前目录中的所有文件作为提示的上下文包含在内。\n * --help (或 -h):\n   * 显示有关命令行参数的帮助信息。\n * --show-memory-usage:\n   * 显示当前内存使用情况。\n * --yolo:\n   * 启用 YOLO 模式，自动批准所有工具调用。\n * --approval-mode <mode>:\n   * 设置工具调用的批准模式。可用模式：\n     * default: 在每次工具调用时提示批准（默认行为）\n     * auto_edit: 自动批准编辑工具（replace, write_file），同时提示其他工具\n     * yolo: 自动批准所有工具调用（等同于 --yolo）\n   * 不能与 --yolo 一起使用。对于新的统一方法，请使用 --approval-mode=yolo 而不是 --yolo。\n   * 示例: gemini --approval-mode auto_edit\n * --allowed-tools <tool1,tool2,...>:\n   * 一个逗号分隔的工具名称列表，将绕过确认对话框。\n   * 示例: gemini --allowed-tools \"ShellTool(git status)\"\n * --telemetry:\n   * 启用 遥测。\n * --telemetry-target:\n   * 设置遥测目标。有关更多信息，请参阅 遥测。\n * --telemetry-otlp-endpoint:\n   * 设置遥测的 OTLP 端点。有关更多信息，请参阅 遥测。\n * --telemetry-otlp-protocol:\n   * 设置遥测的 OTLP 协议 (grpc 或 http)。默认为 grpc。有关更多信息，请参阅 遥测。\n * --telemetry-log-prompts:\n   * 启用遥测的日志提示。有关更多信息，请参阅 遥测。\n * --checkpointing:\n   * 启用 检查点。\n * --extensions <extension_name ...> (-e <extension_name ...>):\n   * 指定会话要使用的扩展列表。如果未提供，则使用所有可用扩展。\n   * 使用特殊术语 gemini -e none 禁用所有扩展。\n   * 示例: gemini -e my-extension -e my-other-extension\n * --list-extensions (-l):\n   * 列出所有可用扩展并退出。\n * --proxy:\n   * 设置 CLI 的代理。\n   * 示例: --proxy http://localhost:7890。\n * --include-directories <dir1,dir2,...>:\n   * 在工作区中包含其他目录以支持多目录。\n   * 可以多次指定或作为逗号分隔的值。\n   * 最多可以添加 5 个目录。\n   * 示例: --include-directories /path/to/project1,/path/to/project2 或\n     --include-directories /path/to/project1 --include-directories\n     /path/to/project2\n * --screen-reader:\n   * 启用屏幕阅读器模式以实现辅助功能。\n * --version:\n   * 显示 CLI 的版本。\n\n\n上下文文件（分层指令上下文）#\n\n虽然严格来说不是 CLI 行为 的配置，但上下文文件（默认为 GEMINI.md，但可通过 contextFileName 设置配置）对于配置提供给\nGemini 模型的 指令上下文（也称为“内存”）至关重要。这个强大的功能允许您为 AI\n提供项目特定的指令、编码风格指南或任何相关的背景信息，使其响应更加符合您的需求。CLI 包含 UI\n元素，例如页脚中显示已加载上下文文件数量的指示器，以让您了解活动上下文。\n\n * 目的： 这些 Markdown 文件包含您希望 Gemini 模型在与您交互时了解的指令、指南或上下文。该系统旨在分层管理此指令上下文。\n\n\n示例上下文文件内容（例如，GEMINI.md）#\n\n以下是在 TypeScript 项目根目录中的上下文文件可能包含的概念示例：\n\n\n\n此示例演示了如何提供通用项目上下文、特定编码约定，甚至关于特定文件或组件的注释。您的上下文文件越相关和精确，AI\n就能越好地协助您。强烈建议使用项目特定的上下文文件来建立约定和上下文。\n\n * 分层加载和优先级： CLI\n   通过从多个位置加载上下文文件（例如，GEMINI.md）来实现复杂的分层内存系统。此列表中较低位置的文件（更具体）的内容通常会覆盖或补充较高位置（更通用）\n   的文件内容。可以使用 /memory show 命令检查确切的连接顺序和最终上下文。典型的加载顺序是：\n   1. 全局上下文文件：\n      * 位置：~/.ve/<contextFileName>（例如，用户主目录中的 ~/.ve/GEMINI.md）。\n      * 范围：为所有项目提供默认指令。\n   2. 项目根目录和祖先上下文文件：\n      * 位置：CLI 在当前工作目录中搜索配置的上下文文件，然后在每个父目录中搜索，直到项目根目录（由 .git 文件夹标识）或您的主目录。\n      * 范围：提供与整个项目或其重要部分相关的上下文。\n   3. 子目录上下文文件（上下文/本地）：\n      * 位置：CLI 还会在当前工作目录 下方 的子目录中扫描配置的上下文文件（遵守常见的忽略模式，如 node_modules、.git\n        等）。此搜索的广度默认限制为 200 个目录，但可以通过 settings.json 文件中的 memoryDiscoveryMaxDirs\n        字段进行配置。\n      * 范围：允许为项目的特定组件、模块或子部分提供高度具体的指令。\n * 连接和 UI 指示： 所有找到的上下文文件的内容都会连接起来（用分隔符指示其来源和路径），并作为系统提示的一部分提供给 Gemini 模型。CLI\n   页脚显示已加载的上下文文件计数，为您提供关于活动指令上下文的快速视觉提示。\n * 导入内容： 您可以使用 @path/to/file.md 语法通过导入其他 Markdown 文件来模块化您的上下文文件。有关详细信息，请参阅\n   内存导入处理器文档。\n * 内存管理命令：\n   * 使用 /memory refresh 强制重新扫描和重新加载所有配置位置的所有上下文文件。这会更新 AI 的指令上下文。\n   * 使用 /memory show 显示当前加载的组合指令上下文，允许您验证 AI 使用的层次结构和内容。\n   * 有关 /memory 命令及其子命令（show 和 refresh）的完整详细信息，请参阅 命令文档。\n\n通过理解和利用这些配置层和上下文文件的分层性质，您可以有效地管理 AI 的内存，并根据您的特定需求和项目定制 VeCLI 的响应。\n\n\n沙盒#\n\nVeCLI 可以在沙盒环境中执行潜在的不安全操作（如 shell 命令和文件修改），以保护您的系统。\n\n沙盒默认是禁用的，但您可以通过以下几种方式启用它：\n\n * 使用 --sandbox 或 -s 标志。\n * 设置 GEMINI_SANDBOX 环境变量。\n * 默认情况下，使用 --yolo 或 --approval-mode=yolo 时会启用沙盒。\n\n默认情况下，它使用预构建的 gemini-cli-sandbox Docker 镜像。\n\n对于项目的特定沙盒需求，您可以在项目根目录的 .ve/sandbox.Dockerfile 中创建一个自定义 Dockerfile。此 Dockerfile\n可以基于基础沙盒镜像：\n\n\n\n当 .ve/sandbox.Dockerfile 存在时，您可以在运行 VeCLI 时使用 BUILD_SANDBOX 环境变量来自动构建自定义沙盒镜像：\n\n\n\n\n使用统计#\n\n为了帮助我们改进 VeCLI，我们会收集匿名化的使用统计信息。这些数据帮助我们了解 CLI 的使用方式，识别常见问题，并优先考虑新功能。\n\n我们收集的内容：\n\n * 工具调用： 我们记录被调用的工具名称、它们是成功还是失败，以及执行所需的时间。我们不收集传递给工具的参数或工具返回的任何数据。\n * API 请求： 我们记录每个请求使用的 Gemini 模型、请求的持续时间和是否成功。我们不收集提示或响应的内容。\n * 会话信息： 我们收集有关 CLI 配置的信息，例如启用的工具和批准模式。\n\n我们不收集的内容：\n\n * 个人身份信息 (PII)： 我们不收集任何个人信息，例如您的姓名、电子邮件地址或 API 密钥。\n * 提示和响应内容： 我们不记录您的提示或 Gemini 模型的响应内容。\n * 文件内容： 我们不记录 CLI 读取或写入的任何文件的内容。\n\n如何选择退出：\n\n您可以随时通过在 settings.json 文件中将 usageStatisticsEnabled 属性设置为 false 来选择退出使用统计信息收集：\n\n","routePath":"/zh/cli/configuration-v1","lang":"","toc":[{"text":"配置层","id":"配置层","depth":2,"charIndex":215},{"text":"设置文件","id":"设置文件","depth":2,"charIndex":451},{"text":"项目中的 `.ve` 目录","id":"项目中的-ve-目录","depth":3,"charIndex":-1},{"text":"`settings.json` 中的可用设置：","id":"settingsjson-中的可用设置","depth":3,"charIndex":-1},{"text":"排查文件搜索性能问题","id":"排查文件搜索性能问题","depth":3,"charIndex":2599},{"text":"示例 `settings.json`：","id":"示例-settingsjson","depth":3,"charIndex":-1},{"text":"Shell 历史","id":"shell-历史","depth":2,"charIndex":9419},{"text":"环境变量和 `.env` 文件","id":"环境变量和-env-文件","depth":2,"charIndex":-1},{"text":"命令行参数","id":"命令行参数","depth":2,"charIndex":12049},{"text":"上下文文件（分层指令上下文）","id":"上下文文件分层指令上下文","depth":2,"charIndex":14139},{"text":"示例上下文文件内容（例如，`GEMINI.md`）","id":"示例上下文文件内容例如geminimd","depth":3,"charIndex":-1},{"text":"沙盒","id":"沙盒","depth":2,"charIndex":15683},{"text":"使用统计","id":"使用统计","depth":2,"charIndex":16091}],"domain":"","frontmatter":{},"version":""},{"id":82,"title":"VeCLI 配置","content":"#\n\n关于新配置格式的说明\n\nsettings.json 文件的格式已更新为新的、更有组织的结构。\n\n * 新格式将在 [09/10/25] 开始的稳定版本中得到支持。\n * 从旧格式到新格式的自动迁移将从 [09/17/25] 开始。\n\n有关先前格式的详细信息，请参阅 v1 配置文档。\n\nVeCLI 提供了几种配置其行为的方法，包括环境变量、命令行参数和设置文件。本文档概述了不同的配置方法和可用设置。\n\n\n配置层#\n\n配置按以下优先级顺序应用（较低的数字会被较高的数字覆盖）：\n\n 1. 默认值： 应用程序内的硬编码默认值。\n 2. 系统默认文件： 系统范围的默认设置，可以被其他设置文件覆盖。\n 3. 用户设置文件： 当前用户的全局设置。\n 4. 项目设置文件： 项目特定的设置。\n 5. 系统设置文件： 系统范围的设置，覆盖所有其他设置文件。\n 6. 环境变量： 系统范围或会话特定的变量，可能从 .env 文件加载。\n 7. 命令行参数： 启动 CLI 时传递的值。\n\n\n设置文件#\n\nVeCLI 使用 JSON 设置文件进行持久配置。这些文件有四个位置：\n\n * 系统默认文件：\n   * 位置： /etc/vecli/system-defaults.json (Linux),\n     C:\\\\ProgramData\\\\vecli\\\\system-defaults.json (Windows) 或\n     /Library/Application Support/GeminiCli/system-defaults.json (macOS)。路径可以使用\n     VE_CLI_SYSTEM_DEFAULTS_PATH 环境变量覆盖。\n   * 范围： 提供系统范围默认设置的基础层。这些设置具有最低优先级，旨在被用户、项目或系统覆盖设置覆盖。\n * 用户设置文件：\n   * 位置： ~/.ve/settings.json (其中 ~ 是您的主目录)。\n   * 范围： 适用于当前用户的所有 VeCLI 会话。用户设置覆盖系统默认值。\n * 项目设置文件：\n   * 位置： 项目根目录中的 .ve/settings.json。\n   * 范围： 仅在从该特定项目运行 VeCLI 时应用。项目设置覆盖用户设置和系统默认值。\n * 系统设置文件：\n   * 位置： /etc/vecli/settings.json (Linux), C:\\\\ProgramData\\\\vecli\\\\settings.json\n     (Windows) 或 /Library/Application Support/VeCli/settings.json (macOS)。路径可以使用\n     VE_CLI_SYSTEM_SETTINGS_PATH 环境变量覆盖。\n   * 范围： 适用于系统上的所有 VeCLI\n     会话，适用于所有用户。系统设置作为覆盖，优先于所有其他设置文件。对于企业中的系统管理员来说，可能有用，可以控制用户的 VeCLI 设置。\n\n关于设置中的环境变量的说明： 您的 settings.json 文件中的字符串值可以使用 $VAR_NAME 或 ${VAR_NAME}\n语法引用环境变量。加载设置时，这些变量将自动解析。例如，如果您有一个环境变量 MY_API_TOKEN，您可以在 settings.json\n中这样使用它：\"apiKey\": \"$MY_API_TOKEN\"。\n\n> 企业用户须知： 有关在企业环境中部署和管理 VeCLI 的指导，请参阅 企业配置 文档。\n\n\n项目中的 .ve 目录#\n\n除了项目设置文件外，项目的 .ve 目录还可以包含与 VeCLI 操作相关的其他项目特定文件，例如：\n\n * 自定义沙盒配置文件 (例如，.ve/sandbox-macos-custom.sb, .ve/sandbox.Dockerfile)。\n\n\nsettings.json 中的可用设置#\n\n设置按类别组织。所有设置都应放在 settings.json 文件中相应的顶层类别对象中。\n\ngeneral#\n\n * general.preferredEditor (字符串):\n   \n   * 描述： 打开文件的首选编辑器。\n   * 默认值： undefined\n\n * general.vimMode (布尔值):\n   \n   * 描述： 启用 Vim 键绑定。\n   * 默认值： false\n\n * general.disableAutoUpdate (布尔值):\n   \n   * 描述： 禁用自动更新。\n   * 默认值： false\n\n * general.disableUpdateNag (布尔值):\n   \n   * 描述： 禁用更新通知提示。\n   * 默认值： false\n\n * general.checkpointing.enabled (布尔值):\n   \n   * 描述： 启用会话检查点以进行恢复。\n   * 默认值： false\n\nui#\n\n * ui.theme (字符串):\n   \n   * 描述： UI 的颜色主题。有关可用选项，请参阅 主题。\n   * 默认值： undefined\n\n * ui.customThemes (对象):\n   \n   * 描述： 自定义主题定义。\n   * 默认值： {}\n\n * ui.hideWindowTitle (布尔值):\n   \n   * 描述： 隐藏窗口标题栏。\n   * 默认值： false\n\n * ui.hideTips (布尔值):\n   \n   * 描述： 隐藏 UI 中的有用提示。\n   * 默认值： false\n\n * ui.hideBanner (布尔值):\n   \n   * 描述： 隐藏应用程序横幅。\n   * 默认值： false\n\n * ui.hideFooter (布尔值):\n   \n   * 描述： 从 UI 中隐藏页脚。\n   * 默认值： false\n\n * ui.showMemoryUsage (布尔值):\n   \n   * 描述： 在 UI 中显示内存使用信息。\n   * 默认值： false\n\n * ui.showLineNumbers (布尔值):\n   \n   * 描述： 在聊天中显示行号。\n   * 默认值： false\n\n * ui.showCitations (布尔值):\n   \n   * 描述： 在聊天中显示生成文本的引用。\n   * 默认值： false\n\n * ui.accessibility.disableLoadingPhrases (布尔值):\n   \n   * 描述： 为无障碍功能禁用加载短语。\n   * 默认值： false\n\nide#\n\n * ide.enabled (布尔值):\n   \n   * 描述： 启用 IDE 集成模式。\n   * 默认值： false\n\n * ide.hasSeenNudge (布尔值):\n   \n   * 描述： 用户是否已看到 IDE 集成提示。\n   * 默认值： false\n\nprivacy#\n\n * privacy.usageStatisticsEnabled (布尔值):\n   * 描述： 启用使用统计信息的收集。\n   * 默认值： true\n\nmodel#\n\n * model.name (字符串):\n   \n   * 描述： 用于对话的 模型。\n   * 默认值： undefined\n\n * model.maxSessionTurns (数字):\n   \n   * 描述： 会话中保留的用户/模型/工具轮次的最大数量。-1 表示无限制。\n   * 默认值： -1\n\n * model.summarizeToolOutput (对象):\n   \n   * 描述： 启用或禁用工具输出的摘要。您可以使用 tokenBudget 设置指定摘要的令牌预算。注意：目前仅支持 run_shell_command\n     工具。例如 {\"run_shell_command\": {\"tokenBudget\": 2000}}\n   * 默认值： undefined\n\n * model.chatCompression.contextPercentageThreshold (数字):\n   \n   * 描述： 将聊天历史压缩的阈值设置为模型总令牌限制的百分比。这是 0 到 1 之间的值，适用于自动压缩和手动 /compress 命令。例如，值 0.6\n     将在聊天历史超过令牌限制的 60% 时触发压缩。\n   * 默认值： 0.7\n\n * model.skipNextSpeakerCheck (布尔值):\n   \n   * 描述： 跳过下一个发言者检查。\n   * 默认值： false\n\ncontext#\n\n * context.fileName (字符串或字符串数组):\n   \n   * 描述： 上下文文件的名称。\n   * 默认值： undefined\n\n * context.importFormat (字符串):\n   \n   * 描述： 导入内存时使用的格式。\n   * 默认值： undefined\n\n * context.discoveryMaxDirs (数字):\n   \n   * 描述： 搜索内存的最大目录数。\n   * 默认值： 200\n\n * context.includeDirectories (数组):\n   \n   * 描述： 要包含在工作区上下文中的其他目录。缺少的目录将被跳过并发出警告。\n   * 默认值： []\n\n * context.loadFromIncludeDirectories (布尔值):\n   \n   * 描述： 控制 /memory refresh 命令的行为。如果设置为 true，VE.md 文件应从所有添加的目录加载。如果设置为\n     false，VE.md 应仅从当前目录加载。\n   * 默认值： false\n\n * context.fileFiltering.respectGitIgnore (布尔值):\n   \n   * 描述： 搜索时尊重 .gitignore 文件。\n   * 默认值： true\n\n * context.fileFiltering.respectGeminiIgnore (布尔值):\n   \n   * 描述： 搜索时尊重 .veignore 文件。\n   * 默认值： true\n\n * context.fileFiltering.enableRecursiveFileSearch (布尔值):\n   \n   * 描述： 在提示中完成 @ 前缀时，是否启用在当前树下递归搜索文件名。\n   * 默认值： true\n\ntools#\n\n * tools.sandbox (布尔值或字符串):\n   \n   * 描述： 沙盒执行环境（可以是布尔值或路径字符串）。\n   * 默认值： undefined\n\n * tools.usePty (布尔值):\n   \n   * 描述： 使用 node-pty 执行 shell 命令。仍然适用回退到 child_process。\n   * 默认值： false\n\n * tools.core (字符串数组):\n   \n   * 描述： 这可以用于限制内置工具集 使用允许列表。有关核心工具列表，请参阅 内置工具。匹配语义与 tools.allowed 相同。\n   * 默认值： undefined\n\n * tools.exclude (字符串数组):\n   \n   * 描述： 要从发现中排除的工具名称。\n   * 默认值： undefined\n\n * tools.allowed (字符串数组):\n   \n   * 描述： 将绕过确认对话框的工具名称列表。这对于您信任并经常使用的工具很有用。例如，[\"run_shell_command(git)\",\n     \"run_shell_command(npm test)\"] 将跳过运行任何 git 和 npm test\n     命令的确认对话框。有关前缀匹配、命令链接等的详细信息，请参阅 Shell 工具命令限制。\n   * 默认值： undefined\n\n * tools.discoveryCommand (字符串):\n   \n   * 描述： 用于工具发现的命令。\n   * 默认值： undefined\n\n * tools.callCommand (字符串):\n   \n   * 描述： 定义一个自定义 shell 命令，用于调用使用 tools.discoveryCommand 发现的特定工具。shell\n     命令必须满足以下条件：\n     * 它必须将函数 name（与 函数声明 中的完全相同）作为第一个命令行参数。\n     * 它必须在 stdin 上以 JSON 格式读取函数参数，类似于 functionCall.args。\n     * 它必须在 stdout 上以 JSON 格式返回函数输出，类似于 functionResponse.response.content。\n   * 默认值： undefined\n\nmcp#\n\n * mcp.serverCommand (字符串):\n   \n   * 描述： 启动 MCP 服务器的命令。\n   * 默认值： undefined\n\n * mcp.allowed (字符串数组):\n   \n   * 描述： 允许的 MCP 服务器的允许列表。\n   * 默认值： undefined\n\n * mcp.excluded (字符串数组):\n   \n   * 描述： 要排除的 MCP 服务器的拒绝列表。\n   * 默认值： undefined\n\nsecurity#\n\n * security.folderTrust.enabled (布尔值):\n   \n   * 描述： 跟踪文件夹信任是否启用的设置。\n   * 默认值： false\n\n * security.auth.selectedType (字符串):\n   \n   * 描述： 当前选择的身份验证类型。\n   * 默认值： undefined\n\n * security.auth.enforcedType (字符串):\n   \n   * 描述： 所需的身份验证类型（对企业有用）。\n   * 默认值： undefined\n\n * security.auth.useExternal (布尔值):\n   \n   * 描述： 是否使用外部身份验证流程。\n   * 默认值： undefined\n\nadvanced#\n\n * advanced.autoConfigureMemory (布尔值):\n   \n   * 描述： 自动配置 Node.js 内存限制。\n   * 默认值： false\n\n * advanced.dnsResolutionOrder (字符串):\n   \n   * 描述： DNS 解析顺序。\n   * 默认值： undefined\n\n * advanced.excludedEnvVars (字符串数组):\n   \n   * 描述： 要从项目上下文中排除的环境变量。\n   * 默认值： [\"DEBUG\",\"DEBUG_MODE\"]\n\n * advanced.bugCommand (对象):\n   \n   * 描述： 错误报告命令的配置。\n   * 默认值： undefined\n\nmcpServers#\n\n配置与一个或多个模型上下文协议 (MCP) 服务器的连接，用于发现和使用自定义工具。VeCLI 尝试连接到每个配置的 MCP 服务器以发现可用工具。如果多个\nMCP\n服务器暴露了同名工具，则工具名称将使用您在配置中定义的服务器别名作为前缀（例如，serverAlias__actualToolName）以避免冲突。请注意，系统\n可能会为了兼容性而从 MCP 工具定义中剥离某些模式属性。必须至少提供 command、url 或 httpUrl 之一。如果指定了多个，则优先级顺序为\nhttpUrl，然后是 url，然后是 command。\n\n * mcpServers.<SERVER_NAME> (对象): 命名服务器的服务器参数。\n   * command (字符串, 可选): 通过标准 I/O 启动 MCP 服务器的命令。\n   * args (字符串数组, 可选): 传递给命令的参数。\n   * env (对象, 可选): 为服务器进程设置的环境变量。\n   * cwd (字符串, 可选): 启动服务器的工作目录。\n   * url (字符串, 可选): 使用服务器发送事件 (SSE) 进行通信的 MCP 服务器的 URL。\n   * httpUrl (字符串, 可选): 使用可流式 HTTP 进行通信的 MCP 服务器的 URL。\n   * headers (对象, 可选): 发送到 url 或 httpUrl 的请求的 HTTP 标头映射。\n   * timeout (数字, 可选): 对此 MCP 服务器的请求超时时间（毫秒）。\n   * trust (布尔值, 可选): 信任此服务器并绕过所有工具调用确认。\n   * description (字符串, 可选): 服务器的简要描述，可能用于显示目的。\n   * includeTools (字符串数组, 可选): 要从此 MCP\n     服务器包含的工具名称列表。指定时，只有此处列出的工具将从此服务器可用（允许列表行为）。如果未指定，则默认启用服务器中的所有工具。\n   * excludeTools (字符串数组, 可选): 要从此 MCP 服务器排除的工具名称列表。此处列出的工具将不可用，即使服务器暴露了它们。注意：\n     excludeTools 优先于 includeTools - 如果一个工具在两个列表中，它将被排除。\n\ntelemetry#\n\n配置 VeCLI 的日志记录和指标收集。有关更多信息，请参阅 遥测。\n\n * 属性：\n   * enabled (布尔值): 是否启用遥测。\n   * target (字符串): 收集的遥测数据的目的地。支持的值为 local 和 gcp。\n   * otlpEndpoint (字符串): OTLP 导出器的端点。\n   * otlpProtocol (字符串): OTLP 导出器的协议 (grpc 或 http)。\n   * logPrompts (布尔值): 是否在日志中包含用户提示的内容。\n   * outfile (字符串): 当 target 为 local 时写入遥测数据的文件。\n\n\n示例 settings.json#\n\n以下是一个具有嵌套结构的 settings.json 文件示例，自 v0.3.0 起为新格式：\n\n\n\n\nShell 历史#\n\nCLI 会保留您运行的 shell 命令的历史记录。为了避免不同项目之间的冲突，此历史记录存储在用户主文件夹中的项目特定目录中。\n\n * 位置： ~/.ve/tmp/<project_hash>/shell_history\n   * <project_hash> 是从您的项目根路径生成的唯一标识符。\n   * 历史记录存储在名为 shell_history 的文件中。\n\n\n环境变量和 .env 文件#\n\n环境变量是配置应用程序的常见方式，特别是对于 API 密钥等敏感信息或在不同环境之间可能更改的设置。有关身份验证设置，请参阅\n身份验证文档，其中涵盖了所有可用的身份验证方法。\n\nCLI 会自动从 .env 文件加载环境变量。加载顺序为：\n\n 1. 当前工作目录中的 .env 文件。\n 2. 如果未找到，它会向上搜索父目录，直到找到 .env 文件或到达项目根目录（由 .git 文件夹标识）或主目录。\n 3. 如果仍未找到，它会查找 ~/.env（在用户的主目录中）。\n\n环境变量排除： 某些环境变量（如 DEBUG 和 DEBUG_MODE）会自动从项目 .env 文件中排除，以防止干扰 vecli 行为。.ve/.env\n文件中的变量永远不会被排除。您可以使用 settings.json 文件中的 advanced.excludedEnvVars 设置自定义此行为。\n\n * GEMINI_API_KEY:\n   * 您的火山引擎 API 密钥。\n   * 几种可用的 身份验证方法 之一。\n   * 在您的 shell 配置文件（例如 ~/.bashrc、~/.zshrc）或 .env 文件中设置。\n * GEMINI_MODEL:\n   * 指定要使用的默认火山引擎模型。\n   * 覆盖硬编码的默认值\n   * 示例: export VECLI_MODEL=\"deepseek-v3-1\"\n * GOOGLE_API_KEY:\n   * 您的火山引擎 API 密钥。\n   * 在快速模式下使用 Vertex AI 所需。\n   * 确保您拥有必要的权限。\n   * 示例: export ARK_API_KEY=\"YOUR_Ark_API_KEY\"。\n * GOOGLE_CLOUD_PROJECT:\n   * 您的火山引擎项目 ID。\n   * 使用代码助手或 Vertex AI 所需。\n   * 如果使用 Vertex AI，请确保您在此项目中拥有必要的权限。\n   * Cloud Shell 注意： 在 Cloud Shell 环境中运行时，此变量默认为分配给 Cloud Shell 用户的特殊项目。如果您在\n     Cloud Shell 的全局环境中设置了 VOLCANO_ENGINE_PROJECT，它将被此默认值覆盖。要在 Cloud Shell\n     中使用不同的项目，您必须在 .env 文件中定义 VOLCANO_ENGINE_PROJECT。\n   * 示例: export VOLCANO_ENGINE_PROJECT=\"YOUR_PROJECT_ID\"。\n * GOOGLE_APPLICATION_CREDENTIALS (字符串):\n   * 描述： 您的火山引擎应用程序凭据 JSON 文件的路径。\n   * 示例： export\n     VOLCANO_ENGINE_APPLICATION_CREDENTIALS=\"/path/to/your/credentials.json\"\n * OTLP_VOLCANO_ENGINE_CLOUD_PROJECT:\n   * 您的火山引擎遥测项目 ID\n   * 示例: export OTLP_VOLCANO_ENGINE_PROJECT=\"YOUR_PROJECT_ID\"。\n * VOLCANO_ENGINE_LOCATION:\n   * 您的火山引擎项目位置（例如，us-central1）。\n   * 在非快速模式下使用 Vertex AI 所需。\n   * 示例: export VOLCANO_ENGINE_LOCATION=\"YOUR_PROJECT_LOCATION\"。\n * GEMINI_SANDBOX:\n   * settings.json 中 sandbox 设置的替代方法。\n   * 接受 true、false、docker、podman 或自定义命令字符串。\n * SEATBELT_PROFILE (macOS 特定):\n   * 在 macOS 上切换 Seatbelt (sandbox-exec) 配置文件。\n   * permissive-open: (默认) 限制写入项目文件夹（和一些其他文件夹，参见\n     packages/cli/src/utils/sandbox-macos-permissive-open.sb）但允许其他操作。\n   * strict: 使用严格的配置文件，默认拒绝操作。\n   * <profile_name>: 使用自定义配置文件。要定义自定义配置文件，请在项目的 .ve/ 目录中创建一个名为\n     sandbox-macos-<profile_name>.sb\n     的文件（例如，my-project/.ve/sandbox-macos-custom.sb）。\n * DEBUG 或 DEBUG_MODE (通常由底层库或 CLI 本身使用):\n   * 设置为 true 或 1 以启用详细的调试日志记录，这对故障排除很有帮助。\n   * 注意： 默认情况下，这些变量会自动从项目 .env 文件中排除，以防止干扰 vecli 行为。如果需要专门为 vecli 设置这些，请使用\n     .ve/.env 文件。\n * NO_COLOR:\n   * 设置为任何值以禁用 CLI 中的所有颜色输出。\n * CLI_TITLE:\n   * 设置为字符串以自定义 CLI 的标题。\n * CODE_ASSIST_ENDPOINT:\n   * 指定代码助手服务器的端点。\n   * 这对开发和测试很有用。\n\n\n命令行参数#\n\n运行 CLI 时直接传递的参数可以覆盖该特定会话的其他配置。\n\n * --model <model_name> (-m <model_name>):\n   * 指定此会话要使用的火山引擎模型。\n   * 示例: npm start -- --model deepseek-v3-1\n * --prompt <your_prompt> (-p <your_prompt>):\n   * 用于直接将提示传递给命令。这会在非交互模式下调用 VeCLI。\n * --prompt-interactive <your_prompt> (-i <your_prompt>):\n   * 以提供的提示作为初始输入启动交互式会话。\n   * 提示在交互式会话中处理，而不是在会话之前处理。\n   * 在从 stdin 管道输入时不能使用。\n   * 示例: vecli -i \"explain this code\"\n * --sandbox (-s):\n   * 为此会话启用沙盒模式。\n * --sandbox-image:\n   * 设置沙盒镜像 URI。\n * --debug (-d):\n   * 为此会话启用调试模式，提供更详细的输出。\n * --all-files (-a):\n   * 如果设置，则递归地将当前目录中的所有文件作为提示的上下文包含在内。\n * --help (或 -h):\n   * 显示有关命令行参数的帮助信息。\n * --show-memory-usage:\n   * 显示当前内存使用情况。\n * --yolo:\n   * 启用 YOLO 模式，自动批准所有工具调用。\n * --approval-mode <mode>:\n   * 设置工具调用的批准模式。可用模式：\n     * default: 在每次工具调用时提示批准（默认行为）\n     * auto_edit: 自动批准编辑工具（replace, write_file），同时提示其他工具\n     * yolo: 自动批准所有工具调用（等同于 --yolo）\n   * 不能与 --yolo 一起使用。对于新的统一方法，请使用 --approval-mode=yolo 而不是 --yolo。\n   * 示例: vecli --approval-mode auto_edit\n * --allowed-tools <tool1,tool2,...>:\n   * 一个逗号分隔的工具名称列表，将绕过确认对话框。\n   * 示例: vecli --allowed-tools \"ShellTool(git status)\"\n * --telemetry:\n   * 启用 遥测。\n * --telemetry-target:\n   * 设置遥测目标。有关更多信息，请参阅 遥测。\n * --telemetry-otlp-endpoint:\n   * 设置遥测的 OTLP 端点。有关更多信息，请参阅 遥测。\n * --telemetry-otlp-protocol:\n   * 设置遥测的 OTLP 协议 (grpc 或 http)。默认为 grpc。有关更多信息，请参阅 遥测。\n * --telemetry-log-prompts:\n   * 启用遥测的日志提示。有关更多信息，请参阅 遥测。\n * --checkpointing:\n   * 启用 检查点。\n * --extensions <extension_name ...> (-e <extension_name ...>):\n   * 指定会话要使用的扩展列表。如果未提供，则使用所有可用扩展。\n   * 使用特殊术语 vecli -e none 禁用所有扩展。\n   * 示例: vecli -e my-extension -e my-other-extension\n * --list-extensions (-l):\n   * 列出所有可用扩展并退出。\n * --proxy:\n   * 设置 CLI 的代理。\n   * 示例: --proxy http://localhost:7890。\n * --include-directories <dir1,dir2,...>:\n   * 在工作区中包含其他目录以支持多目录。\n   * 可以多次指定或作为逗号分隔的值。\n   * 最多可以添加 5 个目录。\n   * 示例: --include-directories /path/to/project1,/path/to/project2 或\n     --include-directories /path/to/project1 --include-directories\n     /path/to/project2\n * --screen-reader:\n   * 启用屏幕阅读器模式，调整 TUI 以更好地兼容屏幕阅读器。\n * --version:\n   * 显示 CLI 的版本。\n\n\n上下文文件（分层指令上下文）#\n\n虽然严格来说不是 CLI 行为 的配置，但上下文文件（默认为 vecli.md，但可通过 context.fileName\n设置配置）对于配置提供给火山引擎模型的 指令上下文（也称为“内存”）至关重要。这个强大的功能允许您为 AI\n提供项目特定的指令、编码风格指南或任何相关的背景信息，使其响应更加符合您的需求。CLI 包含 UI\n元素，例如页脚中显示已加载上下文文件数量的指示器，以让您了解活动上下文。\n\n * 目的： 这些 Markdown 文件包含您希望火山引擎模型在与您交互时了解的指令、指南或上下文。该系统旨在分层管理此指令上下文。\n\n\n示例上下文文件内容（例如，VE.md）#\n\n以下是在 TypeScript 项目根目录中的上下文文件可能包含的概念示例：\n\n\n\n此示例演示了如何提供通用项目上下文、特定编码约定，甚至关于特定文件或组件的注释。您的上下文文件越相关和精确，AI\n就能越好地协助您。强烈建议使用项目特定的上下文文件来建立约定和上下文。\n\n * 分层加载和优先级： CLI\n   通过从多个位置加载上下文文件（例如，VE.md）来实现复杂的分层内存系统。此列表中较低位置的文件（更具体）的内容通常会覆盖或补充较高位置（更通用）的文件内\n   容。可以使用 /memory show 命令检查确切的连接顺序和最终上下文。典型的加载顺序是：\n   1. 全局上下文文件：\n      * 位置：~/.ve/<configured-context-filename>（例如，用户主目录中的 ~/.ve/VE.md）。\n      * 范围：为所有项目提供默认指令。\n   2. 项目根目录和祖先上下文文件：\n      * 位置：CLI 在当前工作目录中搜索配置的上下文文件，然后在每个父目录中搜索，直到项目根目录（由 .git 文件夹标识）或您的主目录。\n      * 范围：提供与整个项目或其重要部分相关的上下文。\n   3. 子目录上下文文件（上下文/本地）：\n      * 位置：CLI 还会在当前工作目录 下方 的子目录中扫描配置的上下文文件（尊重常见的忽略模式，如 node_modules、.git\n        等）。此搜索的广度默认限制为 200 个目录，但可以通过 settings.json 文件中的 context.discoveryMaxDirs\n        设置进行配置。\n      * 范围：允许为项目的特定组件、模块或子部分提供高度具体的指令。\n * 连接和 UI 指示： 所有找到的上下文文件的内容都会连接起来（用分隔符指示其来源和路径），并作为系统提示的一部分提供给火山引擎模型。CLI\n   页脚显示已加载的上下文文件计数，为您提供关于活动指令上下文的快速视觉提示。\n * 导入内容： 您可以使用 @path/to/file.md 语法通过导入其他 Markdown 文件来模块化您的上下文文件。有关详细信息，请参阅\n   内存导入处理器文档。\n * 内存管理命令：\n   * 使用 /memory refresh 强制重新扫描和重新加载所有配置位置的所有上下文文件。这会更新 AI 的指令上下文。\n   * 使用 /memory show 显示当前加载的组合指令上下文，允许您验证 AI 使用的层次结构和内容。\n   * 有关 /memory 命令及其子命令（show 和 refresh）的完整详细信息，请参阅 命令文档。\n\n通过理解和利用这些配置层和上下文文件的分层性质，您可以有效地管理 AI 的内存，并根据您的特定需求和项目定制 VeCLI 的响应。\n\n\n沙盒#\n\nVeCLI 可以在沙盒环境中执行潜在的不安全操作（如 shell 命令和文件修改），以保护您的系统。\n\n沙盒默认是禁用的，但您可以通过以下几种方式启用它：\n\n * 使用 --sandbox 或 -s 标志。\n * 设置 VECLI_SANDBOX 环境变量。\n * 默认情况下，使用 --yolo 或 --approval-mode=yolo 时会启用沙盒。\n\n默认情况下，它使用预构建的 vecli-sandbox Docker 镜像。\n\n对于项目的特定沙盒需求，您可以在项目根目录的 .ve/sandbox.Dockerfile 中创建一个自定义 Dockerfile。此 Dockerfile\n可以基于基础沙盒镜像：\n\n\n\n当 .ve/sandbox.Dockerfile 存在时，您可以在运行 VeCLI 时使用 BUILD_SANDBOX 环境变量来自动构建自定义沙盒镜像：\n\n\n\n\n使用统计#\n\n为了帮助我们改进 VeCLI，我们会收集匿名化的使用统计信息。这些数据帮助我们了解 CLI 的使用方式，识别常见问题，并优先考虑新功能。\n\n我们收集的内容：\n\n * 工具调用： 我们记录被调用的工具名称、它们是成功还是失败，以及执行所需的时间。我们不收集传递给工具的参数或工具返回的任何数据。\n * API 请求： 我们记录每个请求使用的火山引擎模型、请求的持续时间和是否成功。我们不收集提示或响应的内容。\n * 会话信息： 我们收集有关 CLI 配置的信息，例如启用的工具和批准模式。\n\n我们不收集的内容：\n\n * 个人身份信息 (PII)： 我们不收集任何个人信息，例如您的姓名、电子邮件地址或 API 密钥。\n * 提示和响应内容： 我们不记录您的提示或火山引擎模型的响应内容。\n * 文件内容： 我们不记录 CLI 读取或写入的任何文件的内容。\n\n如何选择退出：\n\n您可以随时通过在 settings.json 文件中的 privacy 类别下将 usageStatisticsEnabled 属性设置为 false\n来选择退出使用统计信息收集：\n\n","routePath":"/zh/cli/configuration","lang":"","toc":[{"text":"配置层","id":"配置层","depth":2,"charIndex":205},{"text":"设置文件","id":"设置文件","depth":2,"charIndex":441},{"text":"项目中的 `.ve` 目录","id":"项目中的-ve-目录","depth":3,"charIndex":-1},{"text":"`settings.json` 中的可用设置","id":"settingsjson-中的可用设置","depth":3,"charIndex":-1},{"text":"`general`","id":"general","depth":4,"charIndex":-1},{"text":"`ui`","id":"ui","depth":4,"charIndex":-1},{"text":"`ide`","id":"ide","depth":4,"charIndex":-1},{"text":"`privacy`","id":"privacy","depth":4,"charIndex":-1},{"text":"`model`","id":"model","depth":4,"charIndex":-1},{"text":"`context`","id":"context","depth":4,"charIndex":-1},{"text":"`tools`","id":"tools","depth":4,"charIndex":-1},{"text":"`mcp`","id":"mcp","depth":4,"charIndex":-1},{"text":"`security`","id":"security","depth":4,"charIndex":-1},{"text":"`advanced`","id":"advanced","depth":4,"charIndex":-1},{"text":"`mcpServers`","id":"mcpservers","depth":4,"charIndex":-1},{"text":"`telemetry`","id":"telemetry","depth":4,"charIndex":-1},{"text":"示例 `settings.json`","id":"示例-settingsjson","depth":3,"charIndex":-1},{"text":"Shell 历史","id":"shell-历史","depth":2,"charIndex":7878},{"text":"环境变量和 `.env` 文件","id":"环境变量和-env-文件","depth":2,"charIndex":-1},{"text":"命令行参数","id":"命令行参数","depth":2,"charIndex":10449},{"text":"上下文文件（分层指令上下文）","id":"上下文文件分层指令上下文","depth":2,"charIndex":12534},{"text":"示例上下文文件内容（例如，`VE.md`）","id":"示例上下文文件内容例如vemd","depth":3,"charIndex":-1},{"text":"沙盒","id":"沙盒","depth":2,"charIndex":14068},{"text":"使用统计","id":"使用统计","depth":2,"charIndex":14470}],"domain":"","frontmatter":{},"version":""},{"id":83,"title":"企业版 VeCLI","content":"#\n\n本文档概述了在企业环境中部署和管理 VeCLI 的配置模式和最佳实践。通过利用系统级设置，管理员可以强制执行安全策略、管理工具访问并确保所有用户的一致体验。\n\n> 安全说明： 本文档中描述的模式旨在帮助管理员为使用 VeCLI 创建更受控制和安全的环境。但是，它们不应被视为万无一失的安全边界。一个拥有足够本地机器权限的\n> determined 用户可能仍然能够绕过这些配置。这些措施旨在防止意外误用并在受管理的环境中强制执行公司策略，而不是防御拥有本地管理权限的恶意行为者。\n\n\n集中配置：系统设置文件#\n\n企业管理员最强大的工具是系统范围的设置文件。这些文件允许您定义基线配置 (system-defaults.json) 和一组适用于机器上所有用户的覆盖\n(settings.json)。有关配置选项的完整概述，请参阅 配置文档。\n\n设置从四个文件中合并。单值设置（如 theme）的优先级顺序为：\n\n 1. 系统默认值 (system-defaults.json)\n 2. 用户设置 (~/.ve/settings.json)\n 3. 工作区设置 (<project>/.ve/settings.json)\n 4. 系统覆盖 (settings.json)\n\n这意味着系统覆盖文件具有最终决定权。对于数组 (includeDirectories) 或对象 (mcpServers) 的设置，值会被合并。\n\n合并和优先级示例：\n\n以下是不同级别设置如何组合的示例。\n\n * 系统默认值 system-defaults.json：\n   \n   \n\n * 用户 settings.json (~/.ve/settings.json)：\n   \n   \n\n * 工作区 settings.json (<project>/.ve/settings.json)：\n   \n   \n\n * 系统覆盖 settings.json：\n   \n   \n\n这会导致以下合并配置：\n\n * 最终合并配置：\n   \n   \n\n原因：\n\n * theme：使用系统覆盖中的值 (system-enforced-theme)，因为它具有最高优先级。\n\n * mcpServers：对象被合并。系统覆盖中的 corp-server 定义优先于用户的定义。唯一的 user-tool 和 project-tool\n   被包含。\n\n * includeDirectories：数组按系统默认值、用户、工作区和系统覆盖的顺序连接。\n\n * 位置：\n   \n   * Linux：/etc/gemini-cli/settings.json\n   * Windows：C:\\ProgramData\\gemini-cli\\settings.json\n   * macOS：/Library/Application Support/GeminiCli/settings.json\n   * 可以使用 GEMINI_CLI_SYSTEM_SETTINGS_PATH 环境变量覆盖路径。\n\n * 控制：此文件应由系统管理员管理，并使用适当的文件权限进行保护，以防止用户未经授权修改。\n\n通过使用系统设置文件，您可以强制执行以下描述的安全和配置模式。\n\n\n限制工具访问#\n\n您可以通过控制 Gemini 模型可以使用哪些工具来显著增强安全性。这是通过 tools.core 和 tools.exclude\n设置实现的。有关可用工具的列表，请参阅 工具文档。\n\n\n使用 coreTools 进行允许列表#\n\n最安全的方法是将允许用户执行的工具和命令明确添加到允许列表中。这可以防止使用任何未在批准列表中的工具。\n\n示例： 仅允许安全的只读文件操作和列出文件。\n\n\n\n\n使用 excludeTools 进行阻止列表#\n\n或者，您可以将环境中被认为危险的特定工具添加到阻止列表中。\n\n示例： 防止使用 shell 工具删除文件。\n\n\n\n安全说明： 使用 excludeTools 进行阻止列表不如使用 coreTools\n进行允许列表安全，因为它依赖于阻止已知的坏命令，而聪明的用户可能会找到绕过基于字符串的简单阻止的方法。建议使用允许列表方法。\n\n\n管理自定义工具 (MCP 服务器)#\n\n如果您的组织通过 模型上下文协议 (MCP) 服务器 使用自定义工具，则必须了解如何管理服务器配置以有效应用安全策略。\n\n\nMCP 服务器配置如何合并#\n\nVeCLI 从三个级别加载 settings.json 文件：系统、工作区和用户。当涉及到 mcpServers 对象时，这些配置会被合并：\n\n 1. 合并： 来自所有三个级别的服务器列表被合并成一个单一列表。\n 2. 优先级： 如果在多个级别定义了具有相同名称的服务器（例如，在系统和用户设置中都存在名为 corp-api\n    的服务器），则使用最高优先级级别的定义。优先级顺序为：系统 > 工作区 > 用户。\n\n这意味着用户无法覆盖已在系统级设置中定义的服务器定义。但是，他们可以添加具有唯一名称的新服务器。\n\n\n强制执行工具目录#\n\nMCP 工具生态系统安全性取决于定义规范服务器并将它们的名称添加到允许列表的组合。\n\n\n限制 MCP 服务器内的工具#\n\n为了获得更高的安全性，尤其是在处理第三方 MCP 服务器时，您可以限制从服务器暴露给模型的特定工具。这是通过在服务器定义中使用 includeTools 和\nexcludeTools 属性来完成的。这允许您使用服务器工具的子集，而不允许潜在的危险工具。\n\n遵循最小权限原则，强烈建议使用 includeTools 创建一个仅包含必要工具的允许列表。\n\n示例： 仅允许来自第三方 MCP 服务器的 code-search 和 get-ticket-details 工具，即使该服务器提供其他工具如\ndelete-ticket。\n\n\n\n更安全的模式：在系统设置中定义并添加到允许列表#\n\n为了创建一个安全的、集中管理的工具目录，系统管理员必须在系统级 settings.json 文件中执行以下两项操作：\n\n 1. 定义完整配置：在 mcpServers 对象中为每个已批准的服务器定义完整配置。这确保即使用户定义了同名服务器，安全的系统级定义也会优先。\n 2. 添加名称：使用 mcp.allowed\n    设置将这些服务器的名称添加到允许列表中。这是一个关键的安全步骤，可以防止用户运行任何不在该列表中的服务器。如果省略此设置，CLI\n    将合并并允许用户定义的任何服务器。\n\n示例系统 settings.json：\n\n 1. 将所有已批准服务器的_名称_添加到允许列表中。 这将防止用户添加自己的服务器。\n\n 2. 为允许列表中的每个服务器提供规范的_定义_。\n\n\n\n这种模式更安全，因为它同时使用了定义和允许列表。用户定义的任何服务器要么被系统定义覆盖（如果它有相同的名称），要么因为它的名称不在 mcp.allowed\n列表中而被阻止。\n\n\n较不安全的模式：省略允许列表#\n\n如果管理员定义了 mcpServers 对象但未能同时指定 mcp.allowed 允许列表，则用户可能会添加自己的服务器。\n\n示例系统 settings.json：\n\n此配置定义了服务器但未强制执行允许列表。 管理员未包含 \"mcp.allowed\" 设置。\n\n\n\n在这种情况下，用户可以在他们的本地 settings.json 中添加自己的服务器。由于没有 mcp.allowed\n列表来过滤合并结果，用户的服务器将被添加到可用工具列表中并被允许运行。\n\n\n强制执行沙盒以确保安全#\n\n为了减轻潜在有害操作的风险，您可以强制所有工具执行都在沙盒中进行。沙盒在容器化环境中隔离工具执行。\n\n示例： 强制所有工具执行都在 Docker 沙盒中进行。\n\n\n\n您还可以使用 --sandbox-image 命令行参数指定自定义的强化 Docker 镜像，或者通过构建自定义的 sandbox.Dockerfile\n来实现，如 沙盒文档 中所述。\n\n\n通过代理控制网络访问#\n\n在具有严格网络策略的企业环境中，您可以配置 VeCLI 通过企业代理路由所有出站流量。这可以通过环境变量设置，但也可以通过 mcpServers\n配置为自定义工具强制执行。\n\n示例（针对 MCP 服务器）：\n\n\n\n\n遥测和审计#\n\n出于审计和监控目的，您可以配置 VeCLI 将遥测数据发送到中央位置。这使您能够跟踪工具使用情况和其他事件。有关更多信息，请参阅 遥测文档。\n\n示例： 启用遥测并将其发送到本地 OTLP 收集器。如果未指定 otlpEndpoint，它默认为 http://localhost:4317。\n\n\n\n注意： 在企业环境中，请确保将 logPrompts 设置为 false，以避免收集用户提示中的潜在敏感信息。\n\n\n身份验证#\n\n您可以通过在系统级 settings.json 文件中设置 enforcedAuthType\n来为所有用户强制执行特定的身份验证方法。这可以防止用户选择不同的身份验证方法。有关更多详细信息，请参阅 身份验证文档。\n\n示例： 强制所有用户使用 Google 登录。\n\n\n\n如果用户配置了不同的身份验证方法，系统将提示他们切换到强制的方法。在非交互模式下，如果配置的身份验证方法与强制的方法不匹配，CLI 将以错误退出。\n\n\n综合应用：示例系统 settings.json#\n\n以下是一个系统 settings.json 文件的示例，它结合了上述几种模式，为 VeCLI 创建了一个安全、受控的环境。\n\n\n\n此配置：\n\n * 强制所有工具执行都在 Docker 沙盒中进行。\n * 严格使用允许列表，仅包含少量安全的 shell 命令和文件工具。\n * 定义并允许一个企业 MCP 服务器用于自定义工具。\n * 启用遥测以进行审计，但不记录提示内容。\n * 将 /bug 命令重定向到内部工单系统。\n * 禁用常规使用统计信息收集。","routePath":"/zh/cli/enterprise","lang":"","toc":[{"text":"集中配置：系统设置文件","id":"集中配置系统设置文件","depth":2,"charIndex":244},{"text":"限制工具访问","id":"限制工具访问","depth":2,"charIndex":1361},{"text":"使用 `coreTools` 进行允许列表","id":"使用-coretools-进行允许列表","depth":3,"charIndex":-1},{"text":"使用 `excludeTools` 进行阻止列表","id":"使用-excludetools-进行阻止列表","depth":3,"charIndex":-1},{"text":"管理自定义工具 (MCP 服务器)","id":"管理自定义工具-mcp-服务器","depth":2,"charIndex":1756},{"text":"MCP 服务器配置如何合并","id":"mcp-服务器配置如何合并","depth":3,"charIndex":1838},{"text":"强制执行工具目录","id":"强制执行工具目录","depth":3,"charIndex":2115},{"text":"限制 MCP 服务器内的工具","id":"限制-mcp-服务器内的工具","depth":3,"charIndex":2170},{"text":"更安全的模式：在系统设置中定义并添加到允许列表","id":"更安全的模式在系统设置中定义并添加到允许列表","depth":4,"charIndex":2453},{"text":"较不安全的模式：省略允许列表","id":"较不安全的模式省略允许列表","depth":3,"charIndex":2914},{"text":"强制执行沙盒以确保安全","id":"强制执行沙盒以确保安全","depth":2,"charIndex":3162},{"text":"通过代理控制网络访问","id":"通过代理控制网络访问","depth":2,"charIndex":3354},{"text":"遥测和审计","id":"遥测和审计","depth":2,"charIndex":3475},{"text":"身份验证","id":"身份验证","depth":2,"charIndex":3689},{"text":"综合应用：示例系统 `settings.json`","id":"综合应用示例系统-settingsjson","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":84,"title":"VeCLI","content":"#\n\n在 VeCLI 中，packages/cli 是用户与 Gemini AI 模型及其相关工具发送和接收提示的前端。有关 VeCLI 的一般概述，请参阅\n主文档页面。\n\n\n导航此部分#\n\n * 身份验证： 设置与 Google AI 服务身份验证的指南。\n * 命令： VeCLI 命令的参考（例如，/help、/tools、/theme）。\n * 配置： 使用配置文件定制 VeCLI 行为的指南。\n * 企业版： 企业配置指南。\n * 令牌缓存： 通过令牌缓存优化 API 成本。\n * 主题：使用不同主题自定义 CLI 外观的指南。\n * 教程：展示如何使用 VeCLI 自动化开发任务的教程。\n\n\n非交互模式#\n\nVeCLI 可以在非交互模式下运行，这对于脚本和自动化非常有用。在此模式下，您可以将输入通过管道传输到 CLI，它会执行命令，然后退出。\n\n以下示例从您的终端将命令通过管道传输到 VeCLI：\n\n\n\nVeCLI 执行命令并将输出打印到您的终端。请注意，您可以通过使用 --prompt 或 -p 标志来实现相同的行为。例如：\n\n","routePath":"/zh/cli/","lang":"","toc":[{"text":"导航此部分","id":"导航此部分","depth":2,"charIndex":87},{"text":"非交互模式","id":"非交互模式","depth":2,"charIndex":304}],"domain":"","frontmatter":{},"version":""},{"id":85,"title":"主题","content":"#\n\nVeCLI 支持各种主题来自定义其配色方案和外观。您可以通过 /theme 命令或 \"theme\": 配置设置来更改主题以适应您的喜好。\n\n\n可用主题#\n\nVeCLI 预装了一些预定义主题，您可以在 VeCLI 中使用 /theme 命令列出这些主题：\n\n * 深色主题：\n   * ANSI\n   * Atom One\n   * Ayu\n   * Default\n   * Dracula\n   * GitHub\n * 浅色主题：\n   * ANSI Light\n   * Ayu Light\n   * Default Light\n   * GitHub Light\n   * Google Code\n   * Xcode\n\n\n更改主题#\n\n 1. 在 VeCLI 中输入 /theme。\n 2. 会出现一个对话框或选择提示，列出可用的主题。\n 3. 使用箭头键选择一个主题。某些界面在您选择时可能会提供实时预览或高亮显示。\n 4. 确认您的选择以应用主题。\n\n注意： 如果在您的 settings.json 文件中定义了主题（按名称或文件路径），则必须从文件中删除 \"theme\" 设置，然后才能使用 /theme\n命令更改主题。\n\n\n主题持久化#\n\n选定的主题会保存在 VeCLI 的 配置 中，以便在会话之间记住您的偏好。\n\n--------------------------------------------------------------------------------\n\n\n自定义颜色主题#\n\nVeCLI 允许您通过在 settings.json 文件中指定来自定义自己的颜色主题。这使您可以完全控制 CLI 中使用的调色板。\n\n\n如何定义自定义主题#\n\n将 customThemes 块添加到您的用户、项目或系统 settings.json 文件中。每个自定义主题都定义为一个具有唯一名称和一组颜色键的对象。例如：\n\n\n\n颜色键：\n\n * Background\n * Foreground\n * LightBlue\n * AccentBlue\n * AccentPurple\n * AccentCyan\n * AccentGreen\n * AccentYellow\n * AccentRed\n * Comment\n * Gray\n * DiffAdded (可选，用于 diff 中的新增行)\n * DiffRemoved (可选，用于 diff 中的删除行)\n * DiffModified (可选，用于 diff 中的修改行)\n\n必需属性：\n\n * name (必须与 customThemes 对象中的键匹配且为字符串)\n * type (必须是字符串 \"custom\")\n * Background\n * Foreground\n * LightBlue\n * AccentBlue\n * AccentPurple\n * AccentCyan\n * AccentGreen\n * AccentYellow\n * AccentRed\n * Comment\n * Gray\n\n您可以对任何颜色值使用十六进制代码（例如，#FF0000）或标准 CSS 颜色名称（例如，coral、teal、blue）。有关支持的名称的完整列表，请参见\nCSS 颜色名称。\n\n您可以通过向 customThemes 对象添加更多条目来定义多个自定义主题。\n\n\n从文件加载主题#\n\n除了在 settings.json 中定义自定义主题外，您还可以通过在 settings.json 中指定文件路径直接从 JSON\n文件加载主题。这对于共享主题或将它们与您的主要配置分开保存很有用。\n\n要从文件加载主题，请在 settings.json 中将 theme 属性设置为您的主题文件的路径：\n\n\n\n主题文件必须是有效的 JSON 文件，其结构与在 settings.json 中定义的自定义主题相同。\n\n示例 my-theme.json：\n\n\n\n安全提示： 为了您的安全，VeCLI\n只会加载位于您的主目录中的主题文件。如果您尝试从主目录之外加载主题，将显示警告并且不会加载该主题。这是为了防止从不受信任的来源加载潜在的恶意主题文件。\n\n\n示例自定义主题#\n\n\n使用您的自定义主题#\n\n * 在 VeCLI 中使用 /theme 命令选择您的自定义主题。您的自定义主题将出现在主题选择对话框中。\n * 或者，通过在 settings.json 的 ui 对象中添加 \"theme\": \"MyCustomTheme\" 将其设置为默认值。\n * 自定义主题可以在用户、项目或系统级别设置，并遵循与其他设置相同的 配置优先级。\n\n--------------------------------------------------------------------------------\n\n\n深色主题#\n\n\nANSI#\n\n\nAtom OneDark#\n\n\nAyu#\n\n\nDefault#\n\n\nDracula#\n\n\nGitHub#\n\n\n浅色主题#\n\n\nANSI Light#\n\n\nAyu Light#\n\n\nDefault Light#\n\n\nGitHub Light#\n\n\nGoogle Code#\n\n\nXcode#","routePath":"/zh/cli/themes","lang":"","toc":[{"text":"可用主题","id":"可用主题","depth":2,"charIndex":73},{"text":"更改主题","id":"更改主题","depth":3,"charIndex":317},{"text":"主题持久化","id":"主题持久化","depth":3,"charIndex":522},{"text":"自定义颜色主题","id":"自定义颜色主题","depth":2,"charIndex":652},{"text":"如何定义自定义主题","id":"如何定义自定义主题","depth":3,"charIndex":731},{"text":"从文件加载主题","id":"从文件加载主题","depth":3,"charIndex":1437},{"text":"示例自定义主题","id":"示例自定义主题","depth":3,"charIndex":1773},{"text":"使用您的自定义主题","id":"使用您的自定义主题","depth":3,"charIndex":1784},{"text":"深色主题","id":"深色主题","depth":2,"charIndex":2048},{"text":"ANSI","id":"ansi","depth":3,"charIndex":2056},{"text":"Atom OneDark","id":"atom-onedark","depth":3,"charIndex":2064},{"text":"Ayu","id":"ayu","depth":3,"charIndex":2080},{"text":"Default","id":"default","depth":3,"charIndex":2087},{"text":"Dracula","id":"dracula","depth":3,"charIndex":2098},{"text":"GitHub","id":"github","depth":3,"charIndex":2109},{"text":"浅色主题","id":"浅色主题","depth":2,"charIndex":2119},{"text":"ANSI Light","id":"ansi-light","depth":3,"charIndex":2127},{"text":"Ayu Light","id":"ayu-light","depth":3,"charIndex":2141},{"text":"Default Light","id":"default-light","depth":3,"charIndex":2154},{"text":"GitHub Light","id":"github-light","depth":3,"charIndex":2171},{"text":"Google Code","id":"google-code","depth":3,"charIndex":2187},{"text":"Xcode","id":"xcode","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":86,"title":"令牌缓存和成本优化","content":"#\n\nVeCLI 在使用 API 密钥身份验证（Gemini API 密钥或 Vertex AI）时，通过令牌缓存自动优化 API\n成本。此功能重用之前的系统指令和上下文，以减少后续请求中处理的令牌数量。\n\n令牌缓存适用于：\n\n * API 密钥用户（Gemini API 密钥）\n * Vertex AI 用户（需要项目和位置设置）\n\n令牌缓存不适用于：\n\n * OAuth 用户（Google 个人/企业账户）——代码助手 API 目前不支持缓存内容创建\n\n您可以使用 /stats 命令查看您的令牌使用情况和缓存令牌节省量。当缓存令牌可用时，它们将显示在统计输出中。","routePath":"/zh/cli/token-caching","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":87,"title":"教程","content":"#\n\n本页面包含与 VeCLI 交互的教程。\n\n\n设置模型上下文协议 (MCP) 服务器#\n\nCAUTION\n\n在使用第三方 MCP 服务器之前，请确保您信任其来源并了解其提供的工具。您使用第三方服务器的风险自负。\n\n本教程演示了如何设置 MCP 服务器，以 GitHub MCP 服务器 为例。GitHub MCP 服务器提供了与 GitHub\n仓库交互的工具，例如创建议题和对拉取请求进行评论。\n\n\n先决条件#\n\n在开始之前，请确保您已安装并配置了以下内容：\n\n * Docker： 安装并运行 Docker。\n * GitHub 个人访问令牌 (PAT)： 创建一个新的 [经典] 或 [细粒度] PAT，并具有必要的作用域。\n\n\n指南#\n\n在 settings.json 中配置 MCP 服务器#\n\n在您的项目根目录中，创建或打开 .ve/settings.json 文件。在文件中，添加 mcpServers 配置块，该配置块提供了如何启动 GitHub\nMCP 服务器的说明。\n\n\n\n设置您的 GitHub 令牌#\n\nCAUTION\n\n使用一个可以访问个人和私有仓库的广泛作用域的个人访问令牌，可能会导致私有仓库中的信息泄露到公共仓库中。我们建议使用一个不共享对公共和私有仓库访问权限的细粒度访问\n令牌。\n\n使用环境变量来存储您的 GitHub PAT：\n\n\n\nVeCLI 在 settings.json 文件中定义的 mcpServers 配置中使用此值。\n\n启动 VeCLI 并验证连接#\n\n当您启动 VeCLI 时，它会自动读取您的配置并在后台启动 GitHub MCP 服务器。然后您可以使用自然语言提示来要求 VeCLI 执行 GitHub\n操作。例如：\n\n","routePath":"/zh/cli/tutorials","lang":"","toc":[{"text":"设置模型上下文协议 (MCP) 服务器","id":"设置模型上下文协议-mcp-服务器","depth":2,"charIndex":24},{"text":"先决条件","id":"先决条件","depth":3,"charIndex":201},{"text":"指南","id":"指南","depth":3,"charIndex":318},{"text":"在 `settings.json` 中配置 MCP 服务器","id":"在-settingsjson-中配置-mcp-服务器","depth":4,"charIndex":-1},{"text":"设置您的 GitHub 令牌","id":"设置您的-github-令牌","depth":4,"charIndex":447},{"text":"启动 VeCLI 并验证连接","id":"启动-vecli-并验证连接","depth":4,"charIndex":636}],"domain":"","frontmatter":{},"version":""},{"id":88,"title":"VeCLI 核心","content":"#\n\nVeCLI 的核心包 (packages/core) 是 VeCLI 的后端部分，负责与火山引擎 API 通信、管理工具以及处理从 packages/cli\n发送的请求。有关 VeCLI 的一般概述，请参阅 主文档页面。\n\n\n导航此部分#\n\n * 核心工具 API: 有关核心如何定义、注册和使用工具的信息。\n * 内存导入处理器: 使用 @file.md 语法的模块化 VE.md 导入功能的文档。\n\n\n核心的作用#\n\n虽然 VeCLI 的 packages/cli 部分提供了用户界面，但 packages/core 负责：\n\n * 火山引擎 API 交互: 安全地与火山引擎 API 通信，发送用户提示并接收模型响应。\n * 提示工程: 为火山引擎模型构建有效的提示，可能包含对话历史、工具定义以及来自 VE.md 文件的指令上下文。\n * 工具管理与编排:\n   * 注册可用工具（例如，文件系统工具、shell 命令执行）。\n   * 解释来自火山引擎模型的工具使用请求。\n   * 使用提供的参数执行请求的工具。\n   * 将工具执行结果返回给火山引擎模型以进行进一步处理。\n * 会话和状态管理: 跟踪对话状态，包括历史记录和进行连贯交互所需的任何相关上下文。\n * 配置: 管理核心特定的配置，例如 API 密钥访问、模型选择和工具设置。\n\n\n安全注意事项#\n\n核心在安全方面发挥着至关重要的作用：\n\n * API 密钥管理: 它处理 VOLCENGINE_API_KEY 并确保在与火山引擎 API 通信时安全地使用它。\n * 工具执行:\n   当工具与本地系统交互时（例如，run_shell_command），核心（及其底层工具实现）必须以适当的谨慎进行操作，通常涉及沙盒机制以防止意外修改。\n\n\n聊天历史压缩#\n\n为了确保长对话不会超过火山引擎模型的令牌限制，核心包含了一个聊天历史压缩功能。\n\n当对话接近配置模型的令牌限制时，核心会在将对话历史发送到模型之前自动压缩它。这种压缩在传达的信息方面被设计为无损的，但它减少了使用的令牌总数。\n\n您可以在 火山引擎方舟平台 中找到每个模型的令牌限制。\n\n\n模型回退#\n\nVeCLI 包含一个模型回退机制，以确保即使默认的 \"pro\" 模型受到速率限制，您也可以继续使用 CLI。\n\n如果您使用的是默认的 \"pro\" 模型，并且 CLI 检测到您受到速率限制，它会自动在当前会话中切换到 \"flash\" 模型。这使您能够继续工作而不会中断。\n\n\n文件发现服务#\n\n文件发现服务负责在项目中查找与当前上下文相关的文件。它由 @ 命令和其他需要访问文件的工具使用。\n\n\n内存发现服务#\n\n内存发现服务负责查找和加载提供模型上下文的 VE.md\n文件。它以分层方式搜索这些文件，从当前工作目录开始，向上移动到项目根目录和用户的主目录。它还会在子目录中搜索。\n\n这允许您拥有全局、项目级别和组件级别的上下文文件，所有这些文件都组合在一起，为模型提供最相关的信息。\n\n您可以使用 /memory 命令 来 show、add 和 refresh 已加载的 VE.md 文件的内容。","routePath":"/zh/core/","lang":"","toc":[{"text":"导航此部分","id":"导航此部分","depth":2,"charIndex":115},{"text":"核心的作用","id":"核心的作用","depth":2,"charIndex":205},{"text":"安全注意事项","id":"安全注意事项","depth":2,"charIndex":582},{"text":"聊天历史压缩","id":"聊天历史压缩","depth":2,"charIndex":762},{"text":"模型回退","id":"模型回退","depth":2,"charIndex":915},{"text":"文件发现服务","id":"文件发现服务","depth":2,"charIndex":1059},{"text":"内存发现服务","id":"内存发现服务","depth":2,"charIndex":1119}],"domain":"","frontmatter":{},"version":""},{"id":89,"title":"内存导入处理器","content":"#\n\n内存导入处理器是一项功能，允许您通过使用 @file.md 语法从其他文件导入内容来模块化您的 VE.md 文件。\n\n\n概述#\n\n此功能使您能够将大型 VE.md\n文件分解为更小、更易于管理的组件，这些组件可以在不同的上下文中重复使用。导入处理器支持相对路径和绝对路径，并内置了安全功能以防止循环导入并确保文件访问安全。\n\n\n语法#\n\n使用 @ 符号后跟要导入的文件路径：\n\n\n\n\n支持的路径格式#\n\n\n相对路径#\n\n * @./file.md - 从同一目录导入\n * @../file.md - 从父目录导入\n * @./components/file.md - 从子目录导入\n\n\n绝对路径#\n\n * @/absolute/path/to/file.md - 使用绝对路径导入\n\n\n示例#\n\n\n基本导入#\n\n\n\n\n嵌套导入#\n\n导入的文件本身可以包含导入，从而创建嵌套结构：\n\n\n\n\n\n\n安全功能#\n\n\n循环导入检测#\n\n处理器会自动检测并防止循环导入：\n\n\n\n\n文件访问安全#\n\nvalidateImportPath 函数确保仅允许从指定目录导入，防止访问允许范围之外的敏感文件。\n\n\n最大导入深度#\n\n为了防止无限递归，有一个可配置的最大导入深度（默认值：5 级）。\n\n\n错误处理#\n\n\n缺少文件#\n\n如果引用的文件不存在，导入将优雅地失败，并在输出中显示错误注释。\n\n\n文件访问错误#\n\n权限问题或其他文件系统错误会通过适当的错误消息优雅地处理。\n\n\n代码区域检测#\n\n导入处理器使用 marked 库来检测代码块和内联代码片段，确保这些区域内的 @ 导入被正确忽略。这提供了对嵌套代码块和复杂 Markdown 结构的稳健处理。\n\n\n导入树结构#\n\n处理器返回一个导入树，显示导入文件的层次结构，类似于 Claude 的 /memory 功能。这有助于用户通过显示读取了哪些文件及其导入关系来调试他们的\nVE.md 文件。\n\n示例树结构：\n\n\n\n该树保留了文件被导入的顺序，并显示了完整的导入链以用于调试目的。\n\n\n与 Claude Code 的 /memory (claude.md) 方法的比较#\n\nClaude Code 的 /memory 功能（如 claude.md\n中所示）通过连接所有包含的文件生成一个扁平的线性文档，始终用清晰的注释和路径名称标记文件边界。它不显式呈现导入层次结构，但 LLM\n会接收到所有文件内容和路径，如果需要，这足以重建层次结构。\n\n注意：导入树主要用于开发过程中的清晰性，对 LLM 的消费影响有限。\n\n\nAPI 参考#\n\n\nprocessImports(content, basePath, debugMode?, importState?)#\n\n处理 VE.md 内容中的导入语句。\n\n参数:\n\n * content (string): 要处理导入的内容\n * basePath (string): 当前文件所在的目录路径\n * debugMode (boolean, optional): 是否启用调试日志（默认值：false）\n * importState (ImportState, optional): 用于防止循环导入的状态跟踪\n\n返回: Promise - 包含处理后的内容和导入树的对象\n\n\nProcessImportsResult#\n\n\n\n\nMemoryFile#\n\n\n\n\nvalidateImportPath(importPath, basePath, allowedDirectories)#\n\n验证导入路径以确保它们是安全的并且在允许的目录内。\n\n参数:\n\n * importPath (string): 要验证的导入路径\n * basePath (string): 解析相对路径的基本目录\n * allowedDirectories (string[]): 允许的目录路径数组\n\n返回: boolean - 导入路径是否有效\n\n\nfindProjectRoot(startDir)#\n\n通过从给定的起始目录向上搜索 .git 目录来查找项目根目录。实现为一个 async 函数，使用非阻塞文件系统 API 以避免阻塞 Node.js 事件循环。\n\n参数:\n\n * startDir (string): 开始搜索的目录\n\n返回: Promise - 项目根目录（如果未找到 .git，则为起始目录）\n\n\n最佳实践#\n\n 1. 使用描述性的文件名 为导入的组件命名\n 2. 保持导入浅层 - 避免深度嵌套的导入链\n 3. 记录您的结构 - 维护导入文件的清晰层次结构\n 4. 测试您的导入 - 确保所有引用的文件都存在且可访问\n 5. 尽可能使用相对路径 以获得更好的可移植性\n\n\n故障排除#\n\n\n常见问题#\n\n 1. 导入不工作: 检查文件是否存在以及路径是否正确\n 2. 循环导入警告: 检查您的导入结构是否存在循环引用\n 3. 权限错误: 确保文件可读且在允许的目录内\n 4. 路径解析问题: 如果相对路径无法正确解析，请使用绝对路径\n\n\n调试模式#\n\n启用调试模式以查看导入过程的详细日志：\n\n","routePath":"/zh/core/memport","lang":"","toc":[{"text":"概述","id":"概述","depth":2,"charIndex":62},{"text":"语法","id":"语法","depth":2,"charIndex":165},{"text":"支持的路径格式","id":"支持的路径格式","depth":2,"charIndex":193},{"text":"相对路径","id":"相对路径","depth":3,"charIndex":204},{"text":"绝对路径","id":"绝对路径","depth":3,"charIndex":295},{"text":"示例","id":"示例","depth":2,"charIndex":345},{"text":"基本导入","id":"基本导入","depth":3,"charIndex":351},{"text":"嵌套导入","id":"嵌套导入","depth":3,"charIndex":361},{"text":"安全功能","id":"安全功能","depth":2,"charIndex":398},{"text":"循环导入检测","id":"循环导入检测","depth":3,"charIndex":406},{"text":"文件访问安全","id":"文件访问安全","depth":3,"charIndex":436},{"text":"最大导入深度","id":"最大导入深度","depth":3,"charIndex":498},{"text":"错误处理","id":"错误处理","depth":2,"charIndex":542},{"text":"缺少文件","id":"缺少文件","depth":3,"charIndex":550},{"text":"文件访问错误","id":"文件访问错误","depth":3,"charIndex":592},{"text":"代码区域检测","id":"代码区域检测","depth":2,"charIndex":633},{"text":"导入树结构","id":"导入树结构","depth":2,"charIndex":725},{"text":"与 Claude Code 的 `/memory` (`claude.md`) 方法的比较","id":"与-claude-code-的-memory-claudemd-方法的比较","depth":2,"charIndex":-1},{"text":"API 参考","id":"api-参考","depth":2,"charIndex":1081},{"text":"`processImports(content, basePath, debugMode?, importState?)`","id":"processimportscontent-basepath-debugmode-importstate","depth":3,"charIndex":-1},{"text":"`ProcessImportsResult`","id":"processimportsresult","depth":3,"charIndex":-1},{"text":"`MemoryFile`","id":"memoryfile","depth":3,"charIndex":-1},{"text":"`validateImportPath(importPath, basePath, allowedDirectories)`","id":"validateimportpathimportpath-basepath-alloweddirectories","depth":3,"charIndex":-1},{"text":"`findProjectRoot(startDir)`","id":"findprojectrootstartdir","depth":3,"charIndex":-1},{"text":"最佳实践","id":"最佳实践","depth":2,"charIndex":1843},{"text":"故障排除","id":"故障排除","depth":2,"charIndex":1981},{"text":"常见问题","id":"常见问题","depth":3,"charIndex":1989},{"text":"调试模式","id":"调试模式","depth":3,"charIndex":2113}],"domain":"","frontmatter":{},"version":""},{"id":90,"title":"VeCLI 核心：工具 API","content":"#\n\nVeCLI 核心 (packages/core)\n具有一个强大的系统，用于定义、注册和执行工具。这些工具扩展了火山引擎模型的功能，使其能够与本地环境交互、获取网络内容并执行各种超越简单文本生成的操作。\n\n\n核心概念#\n\n * 工具 (tools.ts): 一个接口和基类 (BaseTool)，它定义了所有工具的契约。每个工具必须具有：\n   \n   * name: 一个唯一的内部名称（在调用火山引擎 API 时使用）。\n   * displayName: 一个用户友好的名称。\n   * description: 对工具功能的清晰解释，该解释将提供给火山引擎模型。\n   * parameterSchema: 定义工具接受的参数的 JSON 模式。这对于火山引擎模型理解如何正确调用工具至关重要。\n   * validateToolParams(): 一个用于验证传入参数的方法。\n   * getDescription(): 一个在执行前提供工具将使用特定参数执行什么操作的用户可读描述的方法。\n   * shouldConfirmExecute(): 一个用于确定执行前是否需要用户确认的方法（例如，对于潜在的破坏性操作）。\n   * execute(): 执行工具操作并返回 ToolResult 的核心方法。\n\n * ToolResult (tools.ts): 定义工具执行结果结构的接口：\n   \n   * llmContent: 要包含在发送回 LLM 以供上下文使用的历史记录中的事实内容。这可以是一个简单的字符串或 PartListUnion（一个\n     Part 对象和字符串的数组）以支持富内容。\n   * returnDisplay: 一个用户友好的字符串（通常是 Markdown）或特殊对象（如 FileDiff），用于在 CLI 中显示。\n\n * 返回富内容: 工具不仅限于返回简单文本。llmContent 可以是一个 PartListUnion，它是一个可以包含 Part\n   对象（用于图像、音频等）和 string 的混合数组。这允许单个工具执行返回多个富内容片段。\n\n * 工具注册表 (tool-registry.ts): 一个类 (ToolRegistry)，负责：\n   \n   * 注册工具: 持有所有可用内置工具的集合（例如，ReadFileTool、ShellTool）。\n   * 发现工具: 它还可以动态发现工具：\n     * 基于命令的发现: 如果在设置中配置了 tools.discoveryCommand，则会执行此命令。它应该输出描述自定义工具的\n       JSON，然后将这些工具注册为 DiscoveredTool 实例。\n     * 基于 MCP 的发现: 如果配置了 mcp.serverCommand，注册表可以连接到模型上下文协议 (MCP) 服务器以列出和注册工具\n       (DiscoveredMCPTool)。\n   * 提供模式: 将所有已注册工具的 FunctionDeclaration 模式暴露给火山引擎模型，以便它知道哪些工具可用以及如何使用它们。\n   * 检索工具: 允许核心通过名称获取特定工具以供执行。\n\n\n内置工具#\n\n核心包含一套预定义的工具，通常位于 packages/core/src/tools/ 中。这些工具包括：\n\n * 文件系统工具:\n   * LSTool (ls.ts): 列出目录内容。\n   * ReadFileTool (read-file.ts): 读取单个文件的内容。它接受一个 absolute_path 参数，该参数必须是绝对路径。\n   * WriteFileTool (write-file.ts): 将内容写入文件。\n   * GrepTool (grep.ts): 在文件中搜索模式。\n   * GlobTool (glob.ts): 查找与 glob 模式匹配的文件。\n   * EditTool (edit.ts): 对文件执行就地修改（通常需要确认）。\n   * ReadManyFilesTool (read-many-files.ts): 从多个文件或 glob 模式读取并连接内容（在 CLI 中由 @\n     命令使用）。\n * 执行工具:\n   * ShellTool (shell.ts): 执行任意 shell 命令（需要仔细的沙盒和用户确认）。\n * 网络工具:\n   * WebFetchTool (web-fetch.ts): 从 URL 获取内容。\n   * WebSearchTool (web-search.ts): 执行网络搜索。\n * 内存工具:\n   * MemoryTool (memoryTool.ts): 与 AI 的内存交互。\n\n这些工具都扩展了 BaseTool 并实现了其特定功能所需的方法。\n\n\n工具执行流程#\n\n 1. 模型请求: 火山引擎模型根据用户的提示和提供的工具模式，决定使用一个工具，并在其响应中返回一个 FunctionCall 部分，指定工具名称和参数。\n 2. 核心接收请求: 核心解析此 FunctionCall。\n 3. 工具检索: 它在 ToolRegistry 中查找请求的工具。\n 4. 参数验证: 调用工具的 validateToolParams() 方法。\n 5. 确认 (如果需要):\n    * 调用工具的 shouldConfirmExecute() 方法。\n    * 如果它返回确认详情，核心会将其传达回 CLI，CLI 会提示用户。\n    * 用户的决定（例如，继续、取消）会被发送回核心。\n 6. 执行: 如果已验证并确认（或者如果不需要确认），核心会使用提供的参数和 AbortSignal（用于潜在取消）调用工具的 execute() 方法。\n 7. 结果处理: 从 execute() 接收到的 ToolResult 会被核心接收。\n 8. 对模型的响应: ToolResult 中的 llmContent 会被打包为 FunctionResponse\n    并发送回火山引擎模型，以便它能够继续生成面向用户的响应。\n 9. 向用户显示: ToolResult 中的 returnDisplay 会被发送到 CLI 以向用户显示工具执行了什么操作。\n\n\n使用自定义工具扩展#\n\n虽然在提供的文件中没有明确详细说明普通最终用户可以直接以编程方式注册新工具的主要工作流程，但架构通过以下方式支持扩展：\n\n * 基于命令的发现: 高级用户或项目管理员可以在 settings.json 中定义 tools.discoveryCommand。当 VeCLI\n   核心运行此命令时，该命令应输出一个 FunctionDeclaration 对象的 JSON 数组。然后核心会将这些作为 DiscoveredTool\n   实例提供。相应的 tools.callCommand 将负责实际执行这些自定义工具。\n * MCP 服务器: 对于更复杂的场景，可以设置一个或多个 MCP 服务器，并通过 settings.json 中的 mcpServers\n   设置进行配置。VeCLI 核心然后可以发现并使用这些服务器暴露的工具。如前所述，如果您有多个 MCP\n   服务器，工具名称将使用您配置中的服务器名称作为前缀（例如，serverAlias__actualToolName）。\n\n此工具系统提供了一种灵活而强大的方式来增强火山引擎模型的功能，使 VeCLI 成为处理各种任务的多功能助手。","routePath":"/zh/core/tools-api","lang":"","toc":[{"text":"核心概念","id":"核心概念","depth":2,"charIndex":105},{"text":"内置工具","id":"内置工具","depth":2,"charIndex":1368},{"text":"工具执行流程","id":"工具执行流程","depth":2,"charIndex":2060},{"text":"使用自定义工具扩展","id":"使用自定义工具扩展","depth":2,"charIndex":2667}],"domain":"","frontmatter":{},"version":""},{"id":91,"title":"VeCLI 执行和部署","content":"#\n\n本文档描述了如何运行 VeCLI 并解释 VeCLI 使用的部署架构。\n\n\n运行 VeCLI#\n\n有几种方法可以运行 VeCLI。您选择的选项取决于您打算如何使用 VeCLI。\n\n--------------------------------------------------------------------------------\n\n\n1. 标准安装（推荐给典型用户）#\n\n这是推荐给最终用户安装 VeCLI 的方式。它涉及从 NPM 注册表下载 VeCLI 包。\n\n * 全局安装：\n   \n   \n   \n   然后，从任何地方运行 CLI：\n   \n   \n\n * NPX 执行：\n   \n   \n\n--------------------------------------------------------------------------------\n\n\n2. 在沙盒中运行（Docker/Podman）#\n\n为了安全和隔离，VeCLI 可以在容器内运行。这是 CLI 执行可能有副作用的工具的默认方式。\n\n * 直接从注册表运行： 您可以直接运行已发布的沙盒镜像。这对于只有 Docker 并且想要运行 CLI 的环境很有用。\n   \n   \n\n * 使用 --sandbox 标志： 如果您在本地安装了 VeCLI（使用上述标准安装），您可以指示它在沙盒容器内运行。\n   \n   \n\n--------------------------------------------------------------------------------\n\n\n3. 从源代码运行（推荐给 VeCLI 贡献者）#\n\n项目的贡献者将希望直接从源代码运行 CLI。\n\n * 开发模式： 此方法提供热重载，对积极开发很有用。\n   \n   \n\n * 类似生产的模式（链接包）： 此方法通过链接您的本地包来模拟全局安装。这对于在生产工作流中测试本地构建很有用。\n   \n   \n\n--------------------------------------------------------------------------------\n\n\n4. 从 GitHub 运行最新的 VeCLI 提交#\n\n您可以直接从 GitHub 存储库运行最近提交的 VeCLI 版本。这对于测试仍在开发中的功能很有用。\n\n\n\n\n部署架构#\n\n上述执行方法由以下架构组件和流程实现：\n\nNPM 包\n\nVeCLI 项目是一个 monorepo，将两个核心包发布到 NPM 注册表：\n\n * @vecli/vecli-core：后端，处理逻辑和工具执行。\n * @vecli/vecli：面向用户的前端。\n\n在执行标准安装和从源代码运行 VeCLI 时会使用这些包。\n\n构建和打包流程\n\n根据分发渠道，使用两种不同的构建流程：\n\n * NPM 发布： 对于发布到 NPM 注册表，@vecli/vecli-core 和 @vecli/vecli 中的 TypeScript 源代码使用\n   TypeScript 编译器 (tsc) 转译为标准 JavaScript。生成的 dist/ 目录是 NPM 包中发布的内容。这是 TypeScript\n   库的标准方法。\n\n * GitHub npx 执行： 当直接从 GitHub 运行最新版本的 VeCLI 时，package.json 中的 prepare\n   脚本会触发不同的流程。该脚本使用 esbuild 将整个应用程序及其依赖项捆绑到一个单一的、自包含的 JavaScript\n   文件中。此捆绑包在用户的机器上即时创建，不会签入存储库。\n\nDocker 沙盒镜像\n\n基于 Docker 的执行方法由 vecli-sandbox 容器镜像支持。此镜像发布到容器注册表，包含预安装的全局 VeCLI 版本。\n\n\n发布流程#\n\n发布流程通过 GitHub Actions 自动化。发布工作流执行以下操作：\n\n 1. 使用 tsc 构建 NPM 包。\n 2. 将 NPM 包发布到工件注册表。\n 3. 创建包含捆绑资产的 GitHub 发布。","routePath":"/zh/deployment","lang":"","toc":[{"text":"运行 VeCLI","id":"运行-vecli","depth":2,"charIndex":40},{"text":"1. 标准安装（推荐给典型用户）","id":"1-标准安装推荐给典型用户","depth":3,"charIndex":175},{"text":"2. 在沙盒中运行（Docker/Podman）","id":"2-在沙盒中运行dockerpodman","depth":3,"charIndex":393},{"text":"3. 从源代码运行（推荐给 VeCLI 贡献者）","id":"3-从源代码运行推荐给-vecli-贡献者","depth":3,"charIndex":693},{"text":"4. 从 GitHub 运行最新的 VeCLI 提交","id":"4-从-github-运行最新的-vecli-提交","depth":3,"charIndex":931},{"text":"部署架构","id":"部署架构","depth":2,"charIndex":1016},{"text":"发布流程","id":"发布流程","depth":2,"charIndex":1637}],"domain":"","frontmatter":{},"version":""},{"id":92,"title":"示例代理脚本","content":"#\n\n以下是可与 GEMINI_SANDBOX_PROXY_COMMAND 环境变量一起使用的代理脚本示例。此脚本仅允许到 example.com:443 的\nHTTPS 连接，并拒绝所有其他请求。\n\n","routePath":"/zh/examples/proxy-script","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":93,"title":"VeCLI 扩展\\n\\nVeCLI 支持可以用于配置和扩展其功能的扩展。\\n\\n## 工作原理\\n\\n在启动时，VeCLI 会在两个位置查找扩展：\\n\\n1.  `<workspace>/.vecli/extensions`\\n2.  `<home>/.vecli/extensions`\\n\\nVeCLI 从这两个位置加载所有扩展。如果两个位置都存在同名扩展，则工作区目录中的扩展优先。\\n\\n在每个位置内，单个扩展作为一个包含 `vecli-extension.json` 文件的目录存在。例如：\\n\\n`<workspace>/.vecli/extensions/my-extension/vecli-extension.json`\\n\\n### `vecli-extension.json`\\n\\n`vecli-extension.json` 文件包含扩展的配置。该文件具有以下结构：\\n\\n`json\\n{\\n  \\\"name\\\": \\\"my-extension\\\",\\n  \\\"version\\\": \\\"1.0.0\\\",\\n  \\\"mcpServers\\\": {\\n    \\\"my-server\\\": {\\n      \\\"command\\\": \\\"node my-server.js\\\"\\n    }\\n  },\\n  \\\"contextFileName\\\": \\\"VE.md\\\",\\n  \\\"excludeTools\\\": [\\\"run_shell_command\\\"]\\n}\\n`\\n\\n- `name`: 扩展的名称。这用于唯一标识扩展，并在扩展命令与用户或项目命令同名时用于冲突解决。\\n- `version`: 扩展的版本。\\n- `mcpServers`: 要配置的 MCP 服务器映射。键是服务器的名称，值是服务器配置。这些服务器将在启动时加载，就像在  中配置的 MCP 服务器一样。如果扩展和 `settings.json` 文件都配置了同名的 MCP 服务器，则 `settings.json` 文件中定义的服务器优先。\\n- `contextFileName`: 包含扩展上下文的文件名。这将用于从工作区加载上下文。如果未使用此属性，但扩展目录中存在 `VE.md` 文件，则将加载该文件。\\n- `excludeTools`: 要从模型中排除的工具名称数组。您还可以为支持它的工具指定命令特定的限制，例如 `run_shell_command` 工具。例如，`\\\"excludeTools\\\": [\\\"run_shell_command(rm -rf)\\\"]` 将阻止 `rm -rf` 命令。\\n\\n当 VeCLI 启动时，它会加载所有扩展并合并其配置。如果存在任何冲突，工作区配置优先。\\n\\n## 扩展命令\\n\\n扩展可以通过在扩展目录内的 `commands/` 子目录中放置 TOML 文件来提供 自定义命令。这些命令遵循与用户和项目自定义命令相同的格式，并使用标准命名约定。\\n\\n### 示例\\n\\n一个名为 `gcp` 的扩展具有以下结构：\\n\\n`\\n.vecli/extensions/gcp/\\n├── vecli-extension.json\\n└── commands/\\n    ├── deploy.toml\\n    └── gcs/\\n        └── sync.toml\\n`\\n\\n将提供以下命令：\\n\\n- `/deploy` - 在帮助中显示为 `[gcp] Custom command from deploy.toml`\\n- `/gcs:sync` - 在帮助中显示为 `[gcp] Custom command from sync.toml`\\n\\n### 冲突解决\\n\\n扩展命令具有最低优先级。当与用户或项目命令发生冲突时：\\n\\n1. **无冲突**: 扩展命令使用其自然名称（例如，`/deploy`）\\n2. **有冲突**: 扩展命令使用扩展前缀重命名（例如，`/gcp.deploy`）\\n\\n例如，如果用户和 `gcp` 扩展都定义了一个 `deploy` 命令：\\n\\n- `/deploy` - 执行用户的 deploy 命令\\n- `/gcp.deploy` - 执行扩展的 deploy 命令（标记为 `[gcp]`）\\n\\n# 变量\\n\\nVeCLI 扩展允许在 `vecli-extension.json` 中进行变量替换。如果例如您需要当前目录来使用 `\\\"cwd\\\": \\\"${extensionPath}${/}run.ts\\\"` 运行 MCP 服务器，这将非常有用。\\n\\n**支持的变量:**\\n\\n| 变量                   | 描述                                                                                                                                                     |\\n| -------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| `${extensionPath}`         | 用户文件系统中扩展的完全限定路径，例如 '/Users/username/.vecli/extensions/example-extension'。这不会展开符号链接。 |\\n| `${/} or ${pathSeparator}` | 路径分隔符（因操作系统而异）。                                                                                                                            |","content":"VeCLI 扩展\\n\\nVeCLI 支持可以用于配置和扩展其功能的扩展。\\n\\n## 工作原理\\n\\n在启动时，VeCLI 会在两个位置查找扩展：\\n\\n1.\n<workspace>/.vecli/extensions\\n2. <home>/.vecli/extensions\\n\\nVeCLI\n从这两个位置加载所有扩展。如果两个位置都存在同名扩展，则工作区目录中的扩展优先。\\n\\n在每个位置内，单个扩展作为一个包含\nvecli-extension.json\n文件的目录存在。例如：\\n\\n<workspace>/.vecli/extensions/my-extension/vecli-extension.json\\n\n\\n### vecli-extension.json\\n\\nvecli-extension.json\n文件包含扩展的配置。该文件具有以下结构：\\n\\njson\\n{\\n \\\"name\\\": \\\"my-extension\\\",\\n \\\"version\\\":\n\\\"1.0.0\\\",\\n \\\"mcpServers\\\": {\\n \\\"my-server\\\": {\\n \\\"command\\\": \\\"node\nmy-server.js\\\"\\n }\\n },\\n \\\"contextFileName\\\": \\\"VE.md\\\",\\n \\\"excludeTools\\\":\n[\\\"run_shell_command\\\"]\\n}\\n\\n\\n- name:\n扩展的名称。这用于唯一标识扩展，并在扩展命令与用户或项目命令同名时用于冲突解决。\\n- version: 扩展的版本。\\n- mcpServers: 要配置的\nMCP 服务器映射。键是服务器的名称，值是服务器配置。这些服务器将在启动时加载，就像在 settings.json 文件 中配置的 MCP\n服务器一样。如果扩展和 settings.json 文件都配置了同名的 MCP 服务器，则 settings.json 文件中定义的服务器优先。\\n-\ncontextFileName: 包含扩展上下文的文件名。这将用于从工作区加载上下文。如果未使用此属性，但扩展目录中存在 VE.md\n文件，则将加载该文件。\\n- excludeTools: 要从模型中排除的工具名称数组。您还可以为支持它的工具指定命令特定的限制，例如\nrun_shell_command 工具。例如，\\\"excludeTools\\\": [\\\"run_shell_command(rm -rf)\\\"] 将阻止 rm\n-rf 命令。\\n\\n当 VeCLI 启动时，它会加载所有扩展并合并其配置。如果存在任何冲突，工作区配置优先。\\n\\n##\n扩展命令\\n\\n扩展可以通过在扩展目录内的 commands/ 子目录中放置 TOML 文件来提供\n自定义命令。这些命令遵循与用户和项目自定义命令相同的格式，并使用标准命名约定。\\n\\n### 示例\\n\\n一个名为 gcp\n的扩展具有以下结构：\\n\\n\\n.vecli/extensions/gcp/\\n├── vecli-extension.json\\n└──\ncommands/\\n ├── deploy.toml\\n └── gcs/\\n └── sync.toml\\n\\n\\n将提供以下命令：\\n\\n-\n/deploy - 在帮助中显示为 [gcp] Custom command from deploy.toml\\n- /gcs:sync - 在帮助中显示为\n[gcp] Custom command from sync.toml\\n\\n###\n冲突解决\\n\\n扩展命令具有最低优先级。当与用户或项目命令发生冲突时：\\n\\n1. 无冲突: 扩展命令使用其自然名称（例如，/deploy）\\n2. 有冲突:\n扩展命令使用扩展前缀重命名（例如，/gcp.deploy）\\n\\n例如，如果用户和 gcp 扩展都定义了一个 deploy 命令：\\n\\n- /deploy -\n执行用户的 deploy 命令\\n- /gcp.deploy - 执行扩展的 deploy 命令（标记为 [gcp]）\\n\\n# 变量\\n\\nVeCLI\n扩展允许在 vecli-extension.json 中进行变量替换。如果例如您需要当前目录来使用 \\\"cwd\\\":\n\\\"${extensionPath}${/}run.ts\\\" 运行 MCP 服务器，这将非常有用。\\n\\n支持的变量:\\n\\n| 变量 | 描述 |\\n|\n-------------------------- |\n--------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\n|\\n| ${extensionPath} | 用户文件系统中扩展的完全限定路径，例如\n'/Users/username/.vecli/extensions/example-extension'。这不会展开符号链接。 |\\n| ${/} or\n${pathSeparator} | 路径分隔符（因操作系统而异）。 |#","routePath":"/zh/extension","lang":"","toc":[],"domain":"","frontmatter":{},"version":""},{"id":94,"title":"IDE 集成","content":"#\n\nVeCLI 可以与您的 IDE 集成，以提供更无缝和上下文感知的体验。这种集成使 CLI 能够更好地理解您的工作区，并启用强大的功能，如编辑器内原生差异比较。\n\n目前，唯一支持的 IDE 是 Visual Studio Code 和其他支持 VS Code 扩展的编辑器。\n\n\n功能#\n\n * 工作区上下文: CLI 会自动获取对您工作区的感知，以提供更相关和准确的响应。此上下文包括：\n   \n   * 您工作区中最近访问的 10 个文件。\n   * 您的活动光标位置。\n   * 您选择的任何文本（最多 16KB；更长的选择将被截断）。\n\n * 原生差异比较: 当火山引擎建议代码修改时，您可以直接在 IDE 的原生差异查看器中查看更改。这允许您无缝地审查、编辑并接受或拒绝建议的更改。\n\n * VS Code 命令: 您可以直接从 VS Code 命令面板 (Cmd+Shift+P 或 Ctrl+Shift+P) 访问 VeCLI 功能：\n   \n   * VeCLI: Run: 在集成终端中启动新的 VeCLI 会话。\n   * VeCLI: Accept Diff: 接受活动差异编辑器中的更改。\n   * VeCLI: Close Diff Editor: 拒绝更改并关闭活动差异编辑器。\n   * VeCLI: View Third-Party Notices: 显示扩展的第三方通知。\n\n\n安装和设置#\n\n有三种方法可以设置 IDE 集成：\n\n\n1. 自动提示 (推荐)#\n\n当您在受支持的编辑器内运行 VeCLI 时，它会自动检测您的环境并提示您连接。回答“是”将自动运行必要的设置，包括安装配套扩展并启用连接。\n\n\n2. 从 CLI 手动安装#\n\n如果您之前忽略了提示或想手动安装扩展，可以在 VeCLI 内运行以下命令：\n\n\n\n这将找到适合您 IDE 的正确扩展并进行安装。\n\n\n3. 从市场手动安装#\n\n您也可以直接从市场安装扩展。\n\n * 对于 Visual Studio Code: 从 VS Code 市场 安装。\n * 对于 VS Code 分支: 为了支持 VS Code 的分支，该扩展也发布在 Open VSX Registry\n   上。请按照您的编辑器说明从该注册表安装扩展。\n\n> 注意： “VeCLI Companion”扩展可能出现在搜索结果的底部。如果您没有立即看到它，请尝试向下滚动或按“新发布”排序。\n> \n> 手动安装扩展后，您必须在 CLI 中运行 /ide enable 以激活集成。\n\n\n使用#\n\n\n启用和禁用#\n\n您可以从 CLI 内控制 IDE 集成：\n\n * 要启用与 IDE 的连接，请运行：\n   \n   \n\n * 要禁用连接，请运行：\n   \n   \n\n启用后，VeCLI 将自动尝试连接到 IDE 配套扩展。\n\n\n检查状态#\n\n要检查连接状态并查看 CLI 从 IDE 接收到的上下文，请运行：\n\n\n\n如果已连接，此命令将显示它连接到的 IDE 以及它所知道的最近打开的文件列表。\n\n(注意：文件列表仅限于您工作区中最近访问的 10 个文件，并且仅包括磁盘上的本地文件。)\n\n\n使用差异比较#\n\n当您要求火山引擎修改文件时，它可以直接在您的编辑器中打开一个差异视图。\n\n要接受差异，您可以执行以下任何操作：\n\n * 单击差异编辑器标题栏中的对勾图标。\n * 保存文件（例如，使用 Cmd+S 或 Ctrl+S）。\n * 打开命令面板并运行 VeCLI: Accept Diff。\n * 在 CLI 中提示时回复 yes。\n\n要拒绝差异，您可以：\n\n * 单击差异编辑器标题栏中的**'x' 图标**。\n * 关闭差异编辑器选项卡。\n * 打开命令面板并运行 VeCLI: Close Diff Editor。\n * 在 CLI 中提示时回复 no。\n\n您还可以在差异视图中直接修改建议的更改，然后再接受它们。\n\n如果您在 CLI 中选择‘Yes, allow always’，更改将不再显示在 IDE 中，因为它们将被自动接受。\n\n\n与沙盒一起使用#\n\n如果您在沙盒内使用 VeCLI，请注意以下事项：\n\n * 在 macOS 上: IDE 集成需要网络访问以与 IDE 配套扩展通信。您必须使用允许网络访问的 Seatbelt 配置文件。\n * 在 Docker 容器中: 如果您在 Docker (或 Podman) 容器内运行 VeCLI，IDE 集成仍然可以连接到在您的主机上运行的 VS\n   Code 扩展。CLI 配置为自动在 host.docker.internal 上查找 IDE 服务器。通常不需要特殊配置，但您可能需要确保您的\n   Docker 网络设置允许从容器到主机的连接。\n\n\n故障排除#\n\n如果您在 IDE 集成方面遇到问题，以下是一些常见错误消息及其解决方法。\n\n\n连接错误#\n\n * 消息: 🔴 Disconnected: Failed to connect to IDE companion extension in [IDE\n   Name]. Please ensure the extension is running. To install the extension, run\n   /ide install.\n   \n   * 原因: VeCLI 无法找到必要的环境变量 (VE_CLI_IDE_WORKSPACE_PATH 或 VE_CLI_IDE_SERVER_PORT)\n     来连接到 IDE。这通常意味着 IDE 配套扩展未运行或未正确初始化。\n   * 解决方案:\n     1. 确保您已在 IDE 中安装了 VeCLI Companion 扩展并且已启用。\n     2. 在您的 IDE 中打开一个新终端窗口以确保它获取了正确的环境。\n\n * 消息: 🔴 Disconnected: IDE connection error. The connection was lost\n   unexpectedly. Please try reconnecting by running /ide enable\n   \n   * 原因: 与 IDE 配套的连接丢失。\n   * 解决方案: 运行 /ide enable 以尝试重新连接。如果问题持续存在，请打开一个新终端窗口或重新启动您的 IDE。\n\n\n配置错误#\n\n * 消息: 🔴 Disconnected: Directory mismatch. VeCLI is running in a different\n   location than the open workspace in [IDE Name]. Please run the CLI from one\n   of the following directories: [List of directories]\n   \n   * 原因: CLI 的当前工作目录在您 IDE 中打开的工作区之外。\n   * 解决方案: cd 到您 IDE 中打开的同一目录并重新启动 CLI。\n\n * 消息: 🔴 Disconnected: To use this feature, please open a workspace folder in\n   [IDE Name] and try again.\n   \n   * 原因: 您的 IDE 中没有打开工作区。\n   * 解决方案: 在您的 IDE 中打开一个工作区并重新启动 CLI。\n\n\n一般错误#\n\n * 消息: IDE integration is not supported in your current environment. To use this\n   feature, run VeCLI in one of these supported IDEs: [List of IDEs]\n   \n   * 原因: 您在不支持的 IDE 的终端或环境中运行 VeCLI。\n   * 解决方案: 从受支持的 IDE（如 VS Code）的集成终端运行 VeCLI。\n\n * 消息: No installer is available for IDE. Please install the VeCLI Companion\n   extension manually from the marketplace.\n   \n   * 原因: 您运行了 /ide install，但 CLI 没有针对您特定 IDE 的自动安装程序。\n   * 解决方案: 打开您的 IDE 的扩展市场，搜索 \"VeCLI Companion\"，然后 手动安装它。","routePath":"/zh/ide-integration","lang":"","toc":[{"text":"功能","id":"功能","depth":2,"charIndex":141},{"text":"安装和设置","id":"安装和设置","depth":2,"charIndex":607},{"text":"1. 自动提示 (推荐)","id":"1-自动提示-推荐","depth":3,"charIndex":635},{"text":"2. 从 CLI 手动安装","id":"2-从-cli-手动安装","depth":3,"charIndex":722},{"text":"3. 从市场手动安装","id":"3-从市场手动安装","depth":3,"charIndex":805},{"text":"使用","id":"使用","depth":2,"charIndex":1079},{"text":"启用和禁用","id":"启用和禁用","depth":3,"charIndex":1085},{"text":"检查状态","id":"检查状态","depth":3,"charIndex":1199},{"text":"使用差异比较","id":"使用差异比较","depth":3,"charIndex":1331},{"text":"与沙盒一起使用","id":"与沙盒一起使用","depth":2,"charIndex":1709},{"text":"故障排除","id":"故障排除","depth":2,"charIndex":1996},{"text":"连接错误","id":"连接错误","depth":3,"charIndex":2042},{"text":"配置错误","id":"配置错误","depth":3,"charIndex":2678},{"text":"一般错误","id":"一般错误","depth":3,"charIndex":3156}],"domain":"","frontmatter":{},"version":""},{"id":95,"title":"欢迎使用 VeCLI 文档","content":"#\n\n本文档提供了安装、使用和开发 VeCLI 的综合指南。此工具允许您通过命令行界面与火山引擎模型进行交互。\n\n\n概述#\n\nVeCLI 将火山引擎模型的功能带到您的终端，提供一个交互式的读取-求值-打印循环 (REPL) 环境。VeCLI 由一个客户端应用程序\n(packages/cli) 组成，它与一个本地服务器 (packages/core) 通信，后者负责管理对火山引擎 API 及其 AI 模型的请求。VeCLI\n还包含多种工具，用于执行文件系统操作、运行 shell 和网络获取等任务，这些工具由 packages/core 管理。\n\n\n导航文档#\n\n本文档组织如下：\n\n * 执行与部署: 运行 VeCLI 的信息。\n * 架构概述: 了解 VeCLI 的高层设计，包括其组件及交互方式。\n * CLI 使用: packages/cli 的文档。\n   * CLI 介绍: 命令行界面的概述。\n   * 命令: 可用 CLI 命令的描述。\n   * 配置: 配置 CLI 的信息。\n   * 检查点: 检查点功能的文档。\n   * 扩展: 如何使用新功能扩展 CLI。\n   * IDE 集成: 将 CLI 连接到您的编辑器。\n   * 遥测: CLI 中遥测功能的概述。\n * 核心详情: packages/core 的文档。\n   * 核心介绍: 核心组件的概述。\n   * 工具 API: 有关核心如何管理和暴露工具的信息。\n * 工具:\n   * 工具概述: 可用工具的概述。\n   * 文件系统工具: read_file 和 write_file 工具的文档。\n   * 多文件读取工具: read_many_files 工具的文档。\n   * Shell 工具: run_shell_command 工具的文档。\n   * 网络获取工具: web_fetch 工具的文档。\n   * 内存工具: save_memory 工具的文档。\n * 贡献与开发指南: 面向贡献者和开发者的资讯，包括设置、构建、测试和编码规范。\n * NPM: 有关项目包结构的详细信息。\n * 故障排除指南: 解决常见问题和 FAQ。\n * 服务条款与隐私声明: 有关适用于您使用 VeCLI 的服务条款和隐私声明的信息。\n * 版本发布: 有关项目版本发布和部署节奏的信息。\n\n我们希望本文档能帮助您充分利用 VeCLI！","routePath":"/zh/","lang":"","toc":[{"text":"概述","id":"概述","depth":2,"charIndex":57},{"text":"导航文档","id":"导航文档","depth":2,"charIndex":275}],"domain":"","frontmatter":{},"version":""},{"id":96,"title":"集成测试","content":"#\n\n本文档提供了有关本项目中使用的集成测试框架的信息。\n\n\n概述#\n\n集成测试旨在验证 VeCLI 的端到端功能。它们在受控环境中执行构建的二进制文件，并验证其在与文件系统交互时的行为是否符合预期。\n\n这些测试位于 integration-tests 目录中，并使用自定义测试运行器运行。\n\n\n运行测试#\n\n集成测试不作为默认 npm run test 命令的一部分运行。它们必须使用 npm run test:integration:all 脚本显式运行。\n\n集成测试也可以使用以下快捷方式运行：\n\n\n\n\n运行特定的测试集#\n\n要运行测试文件的子集，您可以使用 npm run <integration test command> <file_name1> ....，其中 是\ntest:e2e 或 test:integration*，<file_name> 是 integration-tests/ 目录中的任何 .test.js\n文件。例如，以下命令运行 list_directory.test.js 和 write_file.test.js：\n\n\n\n\n按名称运行单个测试#\n\n要按名称运行单个测试，请使用 --test-name-pattern 标志：\n\n\n\n\n运行所有测试#\n\n要运行整个集成测试套件，请使用以下命令：\n\n\n\n\n沙盒矩阵#\n\nall 命令将运行 no sandboxing、docker 和 podman 的测试。 可以使用以下命令运行每种单独的类型：\n\n\n\n\n\n\n\n\n诊断#\n\n集成测试运行器提供了几种诊断选项，以帮助追踪测试失败。\n\n\n保留测试输出#\n\n您可以保留测试运行期间创建的临时文件以供检查。这对于调试文件系统操作问题很有用。\n\n要保留测试输出，请将 KEEP_OUTPUT 环境变量设置为 true。\n\n\n\n当保留输出时，测试运行器将打印测试运行的唯一目录的路径。\n\n\n详细输出#\n\n对于更详细的调试，请将 VERBOSE 环境变量设置为 true。\n\n\n\n当在同一命令中使用 VERBOSE=true 和 KEEP_OUTPUT=true 时，输出会流式传输到控制台，并保存到测试临时目录中的日志文件中。\n\n详细输出的格式清晰地标识了日志的来源：\n\n\n\n\n代码检查和格式化#\n\n为了确保代码质量和一致性，集成测试文件作为主要构建过程的一部分进行代码检查。您也可以手动运行代码检查器和自动修复器。\n\n\n运行代码检查器#\n\n要检查代码检查错误，请运行以下命令：\n\n\n\n您可以在命令中包含 :fix 标志以自动修复任何可修复的代码检查错误：\n\n\n\n\n目录结构#\n\n集成测试在 .integration-tests\n目录内为每次测试运行创建一个唯一目录。在此目录中，为每个测试文件创建一个子目录，而在该子目录中，为每个单独的测试用例创建一个子目录。\n\n这种结构使得定位特定测试运行、文件或用例的工件变得容易。\n\n\n\n\n持续集成#\n\n为了确保始终运行集成测试，在 .github/workflows/e2e.yml 中定义了一个 GitHub Actions 工作流。该工作流会自动针对\nmain 分支的拉取请求运行集成测试，或在拉取请求添加到合并队列时运行。\n\n该工作流在不同的沙盒环境中运行测试，以确保 VeCLI 在每个环境中都经过测试：\n\n * sandbox:none：在没有任何沙盒的情况下运行测试。\n * sandbox:docker：在 Docker 容器中运行测试。\n * sandbox:podman：在 Podman 容器中运行测试。","routePath":"/zh/integration-tests","lang":"","toc":[{"text":"概述","id":"概述","depth":2,"charIndex":30},{"text":"运行测试","id":"运行测试","depth":2,"charIndex":147},{"text":"运行特定的测试集","id":"运行特定的测试集","depth":2,"charIndex":254},{"text":"按名称运行单个测试","id":"按名称运行单个测试","depth":3,"charIndex":480},{"text":"运行所有测试","id":"运行所有测试","depth":3,"charIndex":535},{"text":"沙盒矩阵","id":"沙盒矩阵","depth":3,"charIndex":569},{"text":"诊断","id":"诊断","depth":2,"charIndex":648},{"text":"保留测试输出","id":"保留测试输出","depth":3,"charIndex":683},{"text":"详细输出","id":"详细输出","depth":3,"charIndex":805},{"text":"代码检查和格式化","id":"代码检查和格式化","depth":2,"charIndex":949},{"text":"运行代码检查器","id":"运行代码检查器","depth":3,"charIndex":1021},{"text":"目录结构","id":"目录结构","depth":2,"charIndex":1093},{"text":"持续集成","id":"持续集成","depth":2,"charIndex":1226}],"domain":"","frontmatter":{},"version":""},{"id":97,"title":"自动化和分类流程","content":"#\n\n本文档详细概述了我们用于管理和分类问题和拉取请求的自动化流程。我们的目标是提供及时的反馈，并确保贡献得到高效审查和整合。了解这些自动化将帮助您作为贡献者知道可以\n期待什么，以及如何最好地与我们的存储库机器人互动。\n\n\n指导原则：问题和拉取请求#\n\n首先，几乎每个拉取请求 (PR) 都应该链接到相应的问题。问题描述了“什么”和“为什么”（错误或功能），而 PR\n是“如何”（实现）。这种分离有助于我们跟踪工作、确定功能优先级并保持清晰的历史背景。我们的自动化就是围绕这一原则构建的。\n\n--------------------------------------------------------------------------------\n\n\n详细的自动化工作流程#\n\n以下是我们存储库中运行的特定自动化工作流程的细分。\n\n\n1. 当您打开一个问题时：自动化问题分类#\n\n这是您创建问题时将互动的第一个机器人。它的工作是执行初步分析并应用正确的标签。\n\n * 工作流程文件: .github/workflows/gemini-automated-issue-triage.yml\n * 运行时间: 在问题创建或重新打开后立即运行。\n * 执行内容:\n   * 它使用 Gemini 模型根据详细指南分析问题的标题和正文。\n   * 应用一个 area/* 标签: 将问题分类到项目的功能领域（例如，area/ux、area/models、area/platform）。\n   * 应用一个 kind/* 标签: 识别问题类型（例如，kind/bug、kind/enhancement、kind/question）。\n   * 应用一个 priority/* 标签: 根据描述的影响分配从 P0（关键）到 P3（低）的优先级。\n   * 可能应用 status/need-information: 如果问题缺少关键细节（如日志或重现步骤），它将被标记以获取更多信息。\n   * 可能应用 status/need-retesting: 如果问题引用的 CLI 版本超过六个版本，则会被标记以在当前版本上重新测试。\n * 您应该做什么:\n   * 尽可能完整地填写问题模板。您提供的细节越多，分类就越准确。\n   * 如果添加了 status/need-information 标签，请在评论中提供所请求的详细信息。\n\n\n2. 当您打开一个拉取请求时：持续集成 (CI)#\n\n此工作流程确保所有更改在合并之前都符合我们的质量标准。\n\n * 工作流程文件: .github/workflows/ci.yml\n * 运行时间: 在每次推送到拉取请求时。\n * 执行内容:\n   * Lint: 检查您的代码是否符合我们项目的格式和样式规则。\n   * Test: 在 macOS、Windows 和 Linux 上以及多个 Node.js 版本上运行我们的全套自动化测试。这是 CI\n     流程中最耗时的部分。\n   * 发布覆盖率评论: 在所有测试成功通过后，机器人将在您的 PR 上发布评论。此评论提供了您的更改受测试覆盖情况的摘要。\n * 您应该做什么:\n   * 确保所有 CI 检查都通过。当一切成功时，您的提交旁边会出现绿色勾号 ✅。\n   * 如果检查失败（红色“X”❌），请点击失败检查旁边的“详细信息”链接查看日志，找出问题并推送修复。\n\n\n3. 拉取请求的持续分类：PR 审核和标签同步#\n\n此工作流程定期运行，以确保所有开放的 PR 都正确链接到问题并具有统一的标签。\n\n * 工作流程文件: .github/workflows/gemini-scheduled-pr-triage.yml\n * 运行时间: 每 15 分钟在所有开放的拉取请求上运行。\n * 执行内容:\n   * 检查链接的问题: 机器人扫描您的 PR 描述以查找链接到问题的关键字（例如，Fixes #123、Closes #456）。\n   * 添加 status/need-issue: 如果未找到链接的问题，机器人将向您的 PR 添加 status/need-issue\n     标签。这是一个明确的信号，表明需要创建并链接一个问题。\n   * 同步标签: 如果链接了问题，机器人会确保 PR 的标签与问题的标签完全匹配。它将添加任何缺失的标签并移除任何不属于的标签，并且如果存在，将移除\n     status/need-issue 标签。\n * 您应该做什么:\n   * 始终将您的 PR 链接到一个问题。 这是最重要的步骤。在您的 PR 描述中添加一行类似于 Resolves #<issue-number> 的内容。\n   * 这将确保您的 PR 得到正确分类，并顺利通过审查流程。\n\n\n4. 问题的持续分类：计划问题分类#\n\n这是一个后备工作流程，以确保没有问题被分类流程遗漏。\n\n * 工作流程文件: .github/workflows/gemini-scheduled-issue-triage.yml\n * 运行时间: 每小时在所有开放问题上运行。\n * 执行内容:\n   * 它积极寻找完全没有标签或仍有 status/need-triage 标签的问题。\n   * 然后它会触发与初始分类机器人相同的强大基于 Gemini 的分析，以应用正确的标签。\n * 您应该做什么:\n   * 您通常不需要做任何事情。此工作流程是一个安全网，以确保每个问题最终都被分类，即使初始分类失败。\n\n\n5. 发布自动化#\n\n此工作流程处理打包和发布 VeCLI 新版本的过程。\n\n * 工作流程文件: .github/workflows/release.yml\n * 运行时间: 每日计划用于“夜间”版本，并手动用于官方补丁/次要版本。\n * 执行内容:\n   * 自动构建项目，提升版本号，并将包发布到 npm。\n   * 在 GitHub 上创建相应的发布，并生成发布说明。\n * 您应该做什么:\n   * 作为贡献者，您无需为此流程做任何事情。您可以确信，一旦您的 PR 合并到 main 分支，您的更改将包含在下一个夜间版本中。\n\n我们希望这个详细的概述对您有所帮助。如果您对我们的自动化或流程有任何疑问，请随时提问！","routePath":"/zh/issue-and-pr-automation","lang":"","toc":[{"text":"指导原则：问题和拉取请求","id":"指导原则问题和拉取请求","depth":2,"charIndex":111},{"text":"详细的自动化工作流程","id":"详细的自动化工作流程","depth":2,"charIndex":328},{"text":"1. 当您打开一个问题时：`自动化问题分类`","id":"1-当您打开一个问题时自动化问题分类","depth":3,"charIndex":-1},{"text":"2. 当您打开一个拉取请求时：`持续集成 (CI)`","id":"2-当您打开一个拉取请求时持续集成-ci","depth":3,"charIndex":-1},{"text":"3. 拉取请求的持续分类：`PR 审核和标签同步`","id":"3-拉取请求的持续分类pr-审核和标签同步","depth":3,"charIndex":-1},{"text":"4. 问题的持续分类：`计划问题分类`","id":"4-问题的持续分类计划问题分类","depth":3,"charIndex":-1},{"text":"5. 发布自动化","id":"5-发布自动化","depth":3,"charIndex":2305}],"domain":"","frontmatter":{},"version":""},{"id":98,"title":"VeCLI 键盘快捷键","content":"#\n\n本文档列出了 VeCLI 中可用的键盘快捷键。\n\n\n常规#\n\n快捷键      描述\nEsc      关闭对话框和建议。\nCtrl+C   取消正在进行的请求并清除输入。按两次退出应用程序。\nCtrl+D   如果输入为空，则退出应用程序。按两次确认。\nCtrl+L   清除屏幕。\nCtrl+O   切换调试控制台的显示。\nCtrl+S   允许长响应完全打印，禁用截断。使用终端的回滚功能查看整个输出。\nCtrl+T   切换工具描述的显示。\nCtrl+Y   切换所有工具调用的自动批准（YOLO 模式）。\n\n\n输入提示#\n\n快捷键                                            描述\n!                                              当输入为空时，切换 shell 模式。\n\\ (行尾) + Enter                                 插入一个换行符。\nDown Arrow                                     在输入历史记录中向下导航。\nEnter                                          提交当前提示。\nMeta+Delete / Ctrl+Delete                      删除光标右侧的单词。\nTab                                            如果存在，则自动完成当前建议。\nUp Arrow                                       在输入历史记录中向上导航。\nCtrl+A / Home                                  将光标移动到行首。\nCtrl+B / Left Arrow                            将光标向左移动一个字符。\nCtrl+C                                         清除输入提示\nEsc (双击)                                       清除输入提示。\nCtrl+D / Delete                                删除光标右侧的字符。\nCtrl+E / End                                   将光标移动到行尾。\nCtrl+F / Right Arrow                           将光标向右移动一个字符。\nCtrl+H / Backspace                             删除光标左侧的字符。\nCtrl+K                                         从光标删除到行尾。\nCtrl+Left Arrow / Meta+Left Arrow / Meta+B     将光标向左移动一个单词。\nCtrl+N                                         在输入历史记录中向下导航。\nCtrl+P                                         在输入历史记录中向上导航。\nCtrl+Right Arrow / Meta+Right Arrow / Meta+F   将光标向右移动一个单词。\nCtrl+U                                         从光标删除到行首。\nCtrl+V                                         粘贴剪贴板内容。如果剪贴板包含图像，它将被保存，并在提示中插入对它的引用。\nCtrl+W / Meta+Backspace / Ctrl+Backspace       删除光标左侧的单词。\nCtrl+X / Meta+Enter                            在外部编辑器中打开当前输入。\n\n\n建议#\n\n快捷键           描述\nDown Arrow    在建议中向下导航。\nTab / Enter   接受所选建议。\nUp Arrow      在建议中向上导航。\n\n\n单选按钮选择#\n\n快捷键              描述\nDown Arrow / j   向下移动选择。\nEnter            确认选择。\nUp Arrow / k     向上移动选择。\n1-9              通过编号选择项目。\n(多位数)            对于编号大于 9 的项目，请快速连续按数字以选择相应的项目。\n\n\nIDE 集成#\n\n快捷键      描述\nCtrl+G   查看 CLI 从 IDE 接收的上下文","routePath":"/zh/keyboard-shortcuts","lang":"","toc":[{"text":"常规","id":"常规","depth":2,"charIndex":28},{"text":"输入提示","id":"输入提示","depth":2,"charIndex":262},{"text":"建议","id":"建议","depth":2,"charIndex":1762},{"text":"单选按钮选择","id":"单选按钮选择","depth":2,"charIndex":1856},{"text":"IDE 集成","id":"ide-集成","depth":2,"charIndex":2035}],"domain":"","frontmatter":{},"version":""},{"id":99,"title":"包概述","content":"#\n\n此 monorepo 包含两个主要包：@vecli/vecli 和 @vecli/vecli-core。\n\n\n@vecli/vecli#\n\n这是 VeCLI 的主包。它负责用户界面、命令解析以及所有其他面向用户的功能。\n\n发布此包时，它会被捆绑成一个可执行文件。此捆绑包包括该包的所有依赖项，包括 @vecli/vecli-core。这意味着无论用户是使用 npm install\n-g @vecli/vecli 安装该包，还是直接使用 npx @vecli/vecli 运行它，他们都在使用这个单一的、自包含的可执行文件。\n\n\n@vecli/vecli-core#\n\n此包包含与火山引擎 API 交互的核心逻辑。它负责发出 API 请求、处理身份验证和管理本地缓存。\n\n此包未被捆绑。发布时，它作为一个标准的 Node.js 包发布，具有自己的依赖项。这允许在需要时将其作为独立包在其他项目中使用。dist 文件夹中的所有转译后的\njs 代码都包含在包中。\n\n\nNPM 工作区#\n\n此项目使用 NPM 工作区 来管理此 monorepo 中的包。这简化了开发，因为它允许我们从项目根目录管理依赖项并在多个包中运行脚本。\n\n\n工作原理#\n\n根 package.json 文件定义了此项目的工作区：\n\n\n\n这告诉 NPM，packages 目录中的任何文件夹都是一个单独的包，应作为工作区的一部分进行管理。\n\n\n工作区的优势#\n\n * 简化的依赖管理：从项目根目录运行 npm install 将安装工作区中所有包的依赖项并将它们链接在一起。这意味着您无需在每个包的目录中运行 npm\n   install。\n * 自动链接：工作区内的包可以相互依赖。运行 npm install 时，NPM\n   将自动在包之间创建符号链接。这意味着当您对一个包进行更改时，这些更改会立即对其依赖的其他包可用。\n * 简化的脚本执行：您可以使用 --workspace 标志从项目根目录运行任何包中的脚本。例如，要在 cli 包中运行 build 脚本，您可以运行 npm\n   run build --workspace @vecli/vecli。","routePath":"/zh/npm","lang":"","toc":[{"text":"`@vecli/vecli`","id":"veclivecli","depth":2,"charIndex":-1},{"text":"`@vecli/vecli-core`","id":"veclivecli-core","depth":2,"charIndex":-1},{"text":"NPM 工作区","id":"npm-工作区","depth":2,"charIndex":434},{"text":"工作原理","id":"工作原理","depth":3,"charIndex":515},{"text":"工作区的优势","id":"工作区的优势","depth":3,"charIndex":607}],"domain":"","frontmatter":{},"version":""},{"id":100,"title":"VeCLI：配额和定价","content":"#\n\nVeCLI 提供了慷慨的免费层，涵盖了众多个人开发者的使用场景。对于企业/专业用途，或者如果您需要更高的限制，根据您用于身份验证的帐户类型，有多种可能的途径。\n\n有关隐私政策和服务条款的详细信息，请参阅 隐私和条款。\n\n注意：公布的价格是标价；可能会有额外的协商商业折扣。\n\n本文概述了使用不同身份验证方法时适用于 VeCLI 的特定配额和定价。\n\n通常，有三个类别可供选择：\n\n * 免费使用：适合实验和轻度使用。\n * 付费层（固定价格）：适合需要更慷慨的每日配额和可预测成本的个人开发者或企业。\n * 按需付费：专业使用、长时间运行的任务或需要完全控制使用量时最灵活的选择。\n\n\n免费使用#\n\n您的旅程从慷慨的免费层开始，非常适合实验和轻度使用。\n\n您的免费使用限制取决于您的授权类型。\n\n\n使用 Google 登录（个人版 Ve Code Assist）#\n\n对于通过使用 Google 帐户访问个人版 Ve Code Assist 进行身份验证的用户。这包括：\n\n * 每个用户每天 1000 次模型请求\n * 每个用户每分钟 60 次模型请求\n * 模型请求将由 VeCLI 确定，在 Gemini 模型系列中进行。\n\n在 个人版 Ve Code Assist 限制 了解更多信息。\n\n\n使用 Gemini API 密钥登录（未付费）#\n\n如果您使用 Gemini API 密钥，您也可以享受免费层。这包括：\n\n * 每个用户每天 250 次模型请求\n * 每个用户每分钟 10 次模型请求\n * 仅针对 Flash 模型的模型请求。\n\n在 Gemini API 速率限制 了解更多信息。\n\n\n使用 Vertex AI 登录（快速模式）#\n\nVertex AI 提供了一种无需启用结算的快速模式。这包括：\n\n * 90 天后您需要启用结算。\n * 配额和模型是可变的，并且特定于您的帐户。\n\n在 Vertex AI 快速模式限制 了解更多信息。\n\n\n付费层：固定成本的更高限制#\n\n如果您用完了初始请求数量，您可以通过使用 Ve Code Assist 的标准版或企业版 升级您的计划来继续从 VeCLI 中受益，方法是在 此处\n注册。配额和定价基于固定价格订阅，分配许可证席位。为了可预测的成本，您可以使用 Google 登录。这包括：\n\n * 标准版：\n   * 每个用户每天 1500 次模型请求\n   * 每个用户每分钟 120 次模型请求\n * 企业版：\n   * 每个用户每天 2000 次模型请求\n   * 每个用户每分钟 120 次模型请求\n * 模型请求将由 VeCLI 确定，在 Gemini 模型系列中进行。\n\n在 Ve Code Assist 标准版和企业版许可证限制 了解更多信息。\n\n\n按需付费#\n\n如果您达到了每日请求限制，或者即使在升级后也耗尽了您的 Gemini Pro\n配额，最灵活的解决方案是切换到按需付费模式，在这种模式下，您只需为使用的特定处理量付费。这是不间断访问的推荐路径。\n\n为此，请使用 Gemini API 密钥或 Vertex AI 登录。\n\n * Vertex AI（常规模式）：\n   * 配额：由动态共享配额系统或预购的预配置吞吐量管理。\n   * 成本：基于模型和令牌使用量。\n\n在 Vertex AI 动态共享配额 和 Vertex AI 定价 了解更多信息。\n\n * Gemini API 密钥：\n   * 配额：因定价层而异。\n   * 成本：因定价层和模型/令牌使用量而异。\n\n在 Gemini API 速率限制 和 Gemini API 定价 了解更多信息。\n\n重要的是要强调，当使用 API 密钥时，您需要按令牌/调用付费。对于许多令牌很少的小调用来说，这可能更昂贵，但这是确保您的工作流不会因配额限制而中断的唯一方法。\n\n\nGoogle One 和 Ultra 计划，Gemini for Workspace 计划#\n\n这些计划目前仅适用于 Google 提供的基于 Web 的 Gemini 产品（例如，Gemini Web 应用程序或 Flow\n视频编辑器）。这些计划不适用于为 VeCLI 提供动力的 API 使用。支持这些计划正在积极考虑未来的支持。\n\n\n避免高成本的提示#\n\n在使用按需付费的 API 密钥时，请注意您的使用情况，以避免意外成本。\n\n * 不要盲目接受每个建议，特别是对于重构大型代码库等计算密集型任务。\n * 有意识地使用您的提示和命令。您是按调用付费的，所以要考虑完成工作的最有效方式。\n\n\nGemini API 与 Vertex#\n\n * Gemini API（gemini 开发者 api）：这是直接使用 Gemini 模型的最快方式。\n * Vertex AI：这是用于构建、部署和管理具有特定安全和控制要求的 Gemini 模型的企业级平台。\n\n\n了解您的使用情况#\n\n模型使用情况的摘要可通过 /stats 命令获得，并在会话结束时退出时呈现。","routePath":"/zh/quota-and-pricing","lang":"","toc":[{"text":"免费使用","id":"免费使用","depth":2,"charIndex":295},{"text":"使用 Google 登录（个人版 Ve Code Assist）","id":"使用-google-登录个人版-ve-code-assist","depth":3,"charIndex":351},{"text":"使用 Gemini API 密钥登录（未付费）","id":"使用-gemini-api-密钥登录未付费","depth":3,"charIndex":552},{"text":"使用 Vertex AI 登录（快速模式）","id":"使用-vertex-ai-登录快速模式","depth":3,"charIndex":705},{"text":"付费层：固定成本的更高限制","id":"付费层固定成本的更高限制","depth":2,"charIndex":833},{"text":"按需付费","id":"按需付费","depth":2,"charIndex":1165},{"text":"Google One 和 Ultra 计划，Gemini for Workspace 计划","id":"google-one-和-ultra-计划gemini-for-workspace-计划","depth":2,"charIndex":1608},{"text":"避免高成本的提示","id":"避免高成本的提示","depth":2,"charIndex":1778},{"text":"Gemini API 与 Vertex","id":"gemini-api-与-vertex","depth":2,"charIndex":1907},{"text":"了解您的使用情况","id":"了解您的使用情况","depth":2,"charIndex":2039}],"domain":"","frontmatter":{},"version":""},{"id":101,"title":"VeCLI 版本发布","content":"#\n\n\n发布节奏和标签#\n\n已发布新版本。\n\n\nNightly#\n\n","routePath":"/zh/releases","lang":"","toc":[{"text":"发布节奏和标签","id":"发布节奏和标签","depth":2,"charIndex":3},{"text":"Nightly","id":"nightly","depth":3,"charIndex":23}],"domain":"","frontmatter":{},"version":""},{"id":102,"title":"VeCLI 中的沙盒","content":"#\n\n本文档提供了 VeCLI 中沙盒的指南，包括先决条件、快速入门和配置。\n\n\n先决条件#\n\n在使用沙盒之前，您需要安装并设置 VeCLI：\n\n\n\n验证安装\n\n\n\n\n沙盒概述#\n\n沙盒将潜在的危险操作（例如 shell 命令或文件修改）与您的主机系统隔离开来，在 AI 操作和您的环境之间提供安全屏障。\n\n沙盒的好处包括：\n\n * 安全性：防止意外的系统损坏或数据丢失。\n * 隔离性：将文件系统访问限制在项目目录内。\n * 一致性：确保在不同系统上的环境可重现。\n * 安全性：在处理不受信任的代码或实验性命令时降低风险。\n\n\n沙盒方法#\n\n您理想的沙盒方法可能因平台和您首选的容器解决方案而异。\n\n\n1. macOS Seatbelt（仅限 macOS）#\n\n使用 sandbox-exec 的轻量级内置沙盒。\n\n默认配置文件：permissive-open - 限制项目目录外的写入，但允许大多数其他操作。\n\n\n2. 基于容器（Docker/Podman）#\n\n具有完整进程隔离的跨平台沙盒。\n\n注意：需要在本地构建沙盒镜像或使用组织注册表中发布的镜像。\n\n\n快速入门#\n\n\n\n\n配置#\n\n\n启用沙盒（按优先级顺序）#\n\n 1. 命令标志：-s 或 --sandbox\n 2. 环境变量：GEMINI_SANDBOX=true|docker|podman|sandbox-exec\n 3. 设置文件：在 settings.json 文件的 tools 对象中设置 \"sandbox\": true（例如，{\"tools\":\n    {\"sandbox\": true}}）。\n\n\nmacOS Seatbelt 配置文件#\n\n内置配置文件（通过 SEATBELT_PROFILE 环境变量设置）：\n\n * permissive-open（默认）：写入限制，允许网络\n * permissive-closed：写入限制，无网络\n * permissive-proxied：写入限制，通过代理网络\n * restrictive-open：严格限制，允许网络\n * restrictive-closed：最大限制\n\n\n自定义沙盒标志#\n\n对于基于容器的沙盒，您可以使用 SANDBOX_FLAGS 环境变量将自定义标志注入到 docker 或 podman\n命令中。这对于高级配置很有用，例如为特定用例禁用安全功能。\n\n示例（Podman）：\n\n要为卷挂载禁用 SELinux 标记，您可以设置以下内容：\n\n\n\n可以提供多个标志作为空格分隔的字符串：\n\n\n\n\nLinux UID/GID 处理#\n\n沙盒会自动处理 Linux 上的用户权限。使用以下命令覆盖这些权限：\n\n\n\n\n故障排除#\n\n\n常见问题#\n\n“Operation not permitted”\n\n * 操作需要沙盒外的访问权限。\n * 尝试使用更宽松的配置文件或添加挂载点。\n\n缺少命令\n\n * 添加到自定义 Dockerfile 中。\n * 通过 sandbox.bashrc 安装。\n\n网络问题\n\n * 检查沙盒配置文件是否允许网络。\n * 验证代理配置。\n\n\n调试模式#\n\n\n\n注意： 如果您的项目 .env 文件中有 DEBUG=true，由于自动排除，它不会影响 gemini-cli。请使用 .ve/.env 文件进行\ngemini-cli 特定的调试设置。\n\n\n检查沙盒#\n\n\n\n\n安全说明#\n\n * 沙盒可以减少但不能消除所有风险。\n * 使用允许您工作的最严格的配置文件。\n * 首次构建后，容器开销很小。\n * GUI 应用程序可能在沙盒中无法工作。\n\n\n相关文档#\n\n * 配置：完整配置选项。\n * 命令：可用命令。\n * 故障排除：一般故障排除。","routePath":"/zh/sandbox","lang":"","toc":[{"text":"先决条件","id":"先决条件","depth":2,"charIndex":40},{"text":"沙盒概述","id":"沙盒概述","depth":2,"charIndex":83},{"text":"沙盒方法","id":"沙盒方法","depth":2,"charIndex":265},{"text":"1. macOS Seatbelt（仅限 macOS）","id":"1-macos-seatbelt仅限-macos","depth":3,"charIndex":302},{"text":"2. 基于容器（Docker/Podman）","id":"2-基于容器dockerpodman","depth":3,"charIndex":409},{"text":"快速入门","id":"快速入门","depth":2,"charIndex":483},{"text":"配置","id":"配置","depth":2,"charIndex":493},{"text":"启用沙盒（按优先级顺序）","id":"启用沙盒按优先级顺序","depth":3,"charIndex":499},{"text":"macOS Seatbelt 配置文件","id":"macos-seatbelt-配置文件","depth":3,"charIndex":691},{"text":"自定义沙盒标志","id":"自定义沙盒标志","depth":3,"charIndex":906},{"text":"Linux UID/GID 处理","id":"linux-uidgid-处理","depth":2,"charIndex":1077},{"text":"故障排除","id":"故障排除","depth":2,"charIndex":1135},{"text":"常见问题","id":"常见问题","depth":3,"charIndex":1143},{"text":"调试模式","id":"调试模式","depth":3,"charIndex":1312},{"text":"检查沙盒","id":"检查沙盒","depth":3,"charIndex":1417},{"text":"安全说明","id":"安全说明","depth":2,"charIndex":1427},{"text":"相关文档","id":"相关文档","depth":2,"charIndex":1517}],"domain":"","frontmatter":{},"version":""},{"id":103,"title":"VeCLI 可观测性指南","content":"#\n\n遥测技术提供有关 VeCLI 性能、健康状况和使用情况的数据。通过启用它，您可以通过跟踪、指标和结构化日志来监控操作、调试问题和优化工具使用。\n\nVeCLI 的遥测系统基于 OpenTelemetry (OTEL) 标准构建，允许您将数据发送到任何兼容的后端。\n\n\n启用遥测#\n\n您可以通过多种方式启用遥测。配置主要通过 .ve/settings.json 文件 和环境变量进行管理，但 CLI 标志可以覆盖这些设置以用于特定会话。\n\n\n优先级顺序#\n\n以下列出了应用遥测设置的优先级，列表中较高的项目具有更高的优先级：\n\n 1. CLI 标志 (用于 vecli 命令):\n    \n    * --telemetry / --no-telemetry: 覆盖 telemetry.enabled。\n    * --telemetry-target <local|ve>: 覆盖 telemetry.target。\n    * --telemetry-otlp-endpoint <URL>: 覆盖 telemetry.otlpEndpoint。\n    * --telemetry-log-prompts / --no-telemetry-log-prompts: 覆盖\n      telemetry.logPrompts。\n    * --telemetry-outfile <path>: 将遥测输出重定向到文件。请参阅 导出到文件。\n\n 2. 环境变量:\n    \n    * OTEL_EXPORTER_OTLP_ENDPOINT: 覆盖 telemetry.otlpEndpoint。\n\n 3. 工作区设置文件 (.ve/settings.json): 来自此项目特定文件中 telemetry 对象的值。\n\n 4. 用户设置文件 (~/.ve/settings.json): 来自此全局用户文件中 telemetry 对象的值。\n\n 5. 默认值: 如果未通过上述任何方式设置，则应用默认值。\n    \n    * telemetry.enabled: false\n    * telemetry.target: local\n    * telemetry.otlpEndpoint: http://localhost:4317\n    * telemetry.logPrompts: true\n\n对于 npm run telemetry -- --target=<ve|local> 脚本: 此脚本的 --target 参数 仅\n在该脚本的持续时间和目的内覆盖 telemetry.target（即，选择要启动的收集器）。它不会永久更改您的 settings.json。该脚本将首先查看\nsettings.json 以获取 telemetry.target 作为其默认值。\n\n\n示例设置#\n\n以下代码可以添加到您的工作区 (.ve/settings.json) 或用户 (~/.ve/settings.json)\n设置中，以启用遥测并将输出发送到火山引擎：\n\n\n\n\n导出到文件#\n\n您可以将所有遥测数据导出到文件以供本地检查。\n\n要启用文件导出，请使用 --telemetry-outfile 标志并指定所需输出文件的路径。这必须使用 --telemetry-target=local\n运行。\n\n\n\n\n运行 OTEL 收集器#\n\nOTEL 收集器是一个接收、处理和导出遥测数据的服务。 CLI 可以使用 OTLP/gRPC 或 OTLP/HTTP 协议发送数据。 您可以通过\n--telemetry-otlp-protocol 标志或 settings.json 文件中的 telemetry.otlpProtocol\n设置指定要使用的协议。有关更多详细信息，请参阅 配置文档。\n\n在 documentation 中了解更多关于 OTEL 导出器标准配置的信息。\n\n\n本地#\n\n使用 npm run telemetry -- --target=local 命令来自动化设置本地遥测管道的过程，包括在 .ve/settings.json\n文件中配置必要的设置。底层脚本安装 otelcol-contrib（OpenTelemetry 收集器）和 jaeger（用于查看跟踪的 Jaeger\nUI）。要使用它：\n\n 1. 运行命令: 从存储库的根目录执行命令：\n    \n    \n    \n    该脚本将：\n    \n    * 如果需要，下载 Jaeger 和 OTEL。\n    * 启动一个本地 Jaeger 实例。\n    * 启动一个配置为从 VeCLI 接收数据的 OTEL 收集器。\n    * 自动在您的工作区设置中启用遥测。\n    * 退出时，禁用遥测。\n\n 2. 查看跟踪: 打开您的网络浏览器并导航到 http://localhost:16686 以访问 Jaeger UI。在这里您可以检查 VeCLI\n    操作的详细跟踪。\n\n 3. 检查日志和指标: 该脚本将 OTEL 收集器输出（包括日志和指标）重定向到\n    ~/.ve/tmp/<projectHash>/otel/collector.log。该脚本将提供链接以查看并在本地跟踪您的遥测数据（跟踪、指标、日志）\n    。\n\n 4. 停止服务: 在运行脚本的终端中按 Ctrl+C 以停止 OTEL 收集器和 Jaeger 服务。\n\n\n火山引擎#\n\n使用 npm run telemetry -- --target=volc 命令来自动化设置一个本地 OpenTelemetry\n收集器，该收集器将数据转发到您的火山引擎项目，包括在 .ve/settings.json 文件中配置必要的设置。底层脚本安装\notelcol-contrib。要使用它：\n\n 1. 运行命令: 从存储库的根目录执行命令：\n    \n    \n    \n    该脚本将：\n    \n    * 如果需要，下载 otelcol-contrib 二进制文件。\n    * 启动一个配置为从 VeCLI 接收数据并将其导出到您指定的火山引擎项目的 OTEL 收集器。\n    * 自动在您的工作区设置 (.ve/settings.json) 中启用遥测并禁用沙盒模式。\n    * 提供直接链接以在您的火山引擎控制台中查看跟踪、指标和日志。\n    * 退出时 (Ctrl+C)，它将尝试恢复您原来的遥测和沙盒设置。\n\n 2. 运行 VeCLI: 在一个单独的终端中，运行您的 VeCLI 命令。这会生成遥测数据，收集器会捕获这些数据。\n\n 3. 在火山引擎中查看遥测数据: 使用脚本提供的链接导航到火山引擎控制台并查看您的跟踪、指标和日志。\n\n 4. 检查本地收集器日志: 该脚本将本地 OTEL 收集器输出重定向到\n    ~/.ve/tmp/<projectHash>/otel/collector-ve.log。该脚本提供链接以查看并在本地跟踪您的收集器日志。\n\n 5. 停止服务: 在运行脚本的终端中按 Ctrl+C 以停止 OTEL 收集器。\n\n\n日志和指标参考#\n\n以下部分描述了为 VeCLI 生成的日志和指标的结构。\n\n * 所有日志和指标上都包含一个 sessionId 作为公共属性。\n\n\n日志#\n\n日志是特定事件的时间戳记录。为 VeCLI 记录以下事件：\n\n * vecli.config: 此事件在启动时发生一次，并带有 CLI 的配置。\n   \n   * 属性:\n     * model (字符串)\n     * embedding_model (字符串)\n     * sandbox_enabled (布尔值)\n     * core_tools_enabled (字符串)\n     * approval_mode (字符串)\n     * api_key_enabled (布尔值)\n     * vertex_ai_enabled (布尔值)\n     * code_assist_enabled (布尔值)\n     * log_prompts_enabled (布尔值)\n     * file_filtering_respect_git_ignore (布尔值)\n     * debug_mode (布尔值)\n     * mcp_servers (字符串)\n\n * vecli.user_prompt: 当用户提交提示时，此事件发生。\n   \n   * 属性:\n     * prompt_length (整数)\n     * prompt_id (字符串)\n     * prompt (字符串, 如果 log_prompts_enabled 配置为 false，则此属性被排除)\n     * auth_type (字符串)\n\n * vecli.tool_call: 每次函数调用时都会发生此事件。\n   \n   * 属性:\n     * function_name\n     * function_args\n     * duration_ms\n     * success (布尔值)\n     * decision (字符串: \"accept\", \"reject\", \"auto_accept\", 或 \"modify\", 如果适用)\n     * error (如果适用)\n     * error_type (如果适用)\n     * metadata (如果适用, 字符串 -> 任何的字典)\n\n * vecli.file_operation: 每次文件操作时都会发生此事件。\n   \n   * 属性:\n     * tool_name (字符串)\n     * operation (字符串: \"create\", \"read\", \"update\")\n     * lines (整数, 如果适用)\n     * mimetype (字符串, 如果适用)\n     * extension (字符串, 如果适用)\n     * programming_language (字符串, 如果适用)\n     * diff_stat (json 字符串, 如果适用): 一个包含以下成员的 JSON 字符串:\n       * ai_added_lines (整数)\n       * ai_removed_lines (整数)\n       * user_added_lines (整数)\n       * user_removed_lines (整数)\n\n * vecli.api_request: 向火山引擎 API 发出请求时，此事件发生。\n   \n   * 属性:\n     * model\n     * request_text (如果适用)\n\n * vecli.api_error: 如果 API 请求失败，此事件发生。\n   \n   * 属性:\n     * model\n     * error\n     * error_type\n     * status_code\n     * duration_ms\n     * auth_type\n\n * vecli.api_response: 接收到 vecli API 的响应时，此事件发生。\n   \n   * 属性:\n     * model\n     * status_code\n     * duration_ms\n     * error (可选)\n     * input_token_count\n     * output_token_count\n     * cached_content_token_count\n     * thoughts_token_count\n     * tool_token_count\n     * response_text (如果适用)\n     * auth_type\n\n * vecli.malformed_json_response: 当 vecli API 的 generateJson 响应无法解析为 json\n   时，此事件发生。\n   \n   * 属性:\n     * model\n\n * vecli.flash_fallback: 当 VeCLI 切换到 flash 作为后备时，此事件发生。\n   \n   * 属性:\n     * auth_type\n\n * vecli.slash_command: 当用户执行斜杠命令时，此事件发生。\n   \n   * 属性:\n     * command (字符串)\n     * subcommand (字符串, 如果适用)\n\n\n指标#\n\n指标是随时间推移的行为的数值测量。为 VeCLI 收集以下指标：\n\n * vecli.session.count (计数器, 整数): 每次 CLI 启动时递增一次。\n\n * vecli.tool.call.count (计数器, 整数): 计算工具调用次数。\n   \n   * 属性:\n     * function_name\n     * success (布尔值)\n     * decision (字符串: \"accept\", \"reject\", 或 \"modify\", 如果适用)\n     * tool_type (字符串: \"mcp\", 或 \"native\", 如果适用)\n\n * vecli.tool.call.latency (直方图, 毫秒): 测量工具调用延迟。\n   \n   * 属性:\n     * function_name\n     * decision (字符串: \"accept\", \"reject\", 或 \"modify\", 如果适用)\n\n * vecli.api.request.count (计数器, 整数): 计算所有 API 请求次数。\n   \n   * 属性:\n     * model\n     * status_code\n     * error_type (如果适用)\n\n * vecli.api.request.latency (直方图, 毫秒): 测量 API 请求延迟。\n   \n   * 属性:\n     * model\n\n * gemini_cli.token.usage (计数器, 整数): 计算使用的令牌数量。\n   \n   * 属性:\n     * model\n     * type (字符串: \"input\", \"output\", \"thought\", \"cache\", 或 \"tool\")\n\n * gemini_cli.file.operation.count (计数器, 整数): 计算文件操作次数。\n   \n   * 属性:\n     * operation (字符串: \"create\", \"read\", \"update\"): 文件操作的类型。\n     * lines (整数, 如果适用): 文件中的行数。\n     * mimetype (字符串, 如果适用): 文件的 mimetype。\n     * extension (字符串, 如果适用): 文件的扩展名。\n     * model_added_lines (整数, 如果适用): 模型添加/更改的行数。\n     * model_removed_lines (整数, 如果适用): 模型删除/更改的行数。\n     * user_added_lines (整数, 如果适用): 用户在 AI 提出的更改中添加/更改的行数。\n     * user_removed_lines (整数, 如果适用): 用户在 AI 提出的更改中删除/更改的行数。\n     * programming_language (字符串, 如果适用): 文件的编程语言。\n\n * gemini_cli.chat_compression (计数器, 整数): 计算聊天压缩操作次数\n   \n   * 属性:\n     * tokens_before: (整数): 压缩前上下文中的令牌数\n     * tokens_after: (整数): 压缩后上下文中的令牌数","routePath":"/zh/telemetry","lang":"","toc":[{"text":"启用遥测","id":"启用遥测","depth":2,"charIndex":135},{"text":"优先级顺序","id":"优先级顺序","depth":3,"charIndex":221},{"text":"示例设置","id":"示例设置","depth":3,"charIndex":1201},{"text":"导出到文件","id":"导出到文件","depth":3,"charIndex":1295},{"text":"运行 OTEL 收集器","id":"运行-otel-收集器","depth":2,"charIndex":1413},{"text":"本地","id":"本地","depth":3,"charIndex":1646},{"text":"火山引擎","id":"火山引擎","depth":3,"charIndex":2276},{"text":"日志和指标参考","id":"日志和指标参考","depth":2,"charIndex":2974},{"text":"日志","id":"日志","depth":3,"charIndex":3050},{"text":"指标","id":"指标","depth":3,"charIndex":5277}],"domain":"","frontmatter":{},"version":""},{"id":104,"title":"VeCLI 文件系统工具","content":"#\n\nVeCLI\n提供了一套全面的工具来与本地文件系统进行交互。这些工具允许火山引擎模型在您的控制下读取、写入、列出、搜索和修改文件及目录，并且对于敏感操作通常需要您的确认。\n\n注意: 出于安全考虑，所有文件系统工具都在一个 rootDirectory（通常是您启动 CLI\n的当前工作目录）内运行。您提供给这些工具的路径通常应为绝对路径，或者相对于此根目录进行解析。\n\n\n1. list_directory (ReadFolder)#\n\nlist_directory 列出指定目录路径内直接包含的文件和子目录的名称。它可以选择性地忽略与提供的 glob 模式匹配的条目。\n\n * 工具名称: list_directory\n * 显示名称: ReadFolder\n * 文件: ls.ts\n * 参数:\n   * path (字符串, 必需): 要列出的目录的绝对路径。\n   * ignore (字符串数组, 可选): 要从列表中排除的 glob 模式列表 (例如, [\"*.log\", \".git\"])。\n   * respect_git_ignore (布尔值, 可选): 列出文件时是否遵守 .gitignore 模式。默认为 true。\n * 行为:\n   * 返回文件和目录名称的列表。\n   * 指示每个条目是否为目录。\n   * 将条目排序，目录在前，然后按字母顺序排列。\n * 输出 (llmContent): 类似于以下的字符串: Directory listing for\n   /path/to/your/folder:\\n[DIR] subfolder1\\nfile1.txt\\nfile2.png\n * 确认: 否。\n\n\n2. read_file (ReadFile)#\n\nread_file 读取并返回指定文件的内容。此工具处理文本、图像（PNG、JPG、GIF、WEBP、SVG、BMP）和 PDF\n文件。对于文本文件，它可以读取特定的行范围。其他二进制文件类型通常会被跳过。\n\n * 工具名称: read_file\n * 显示名称: ReadFile\n * 文件: read-file.ts\n * 参数:\n   * path (字符串, 必需): 要读取的文件的绝对路径。\n   * offset (数字, 可选): 对于文本文件，从 0 开始的行号，表示从哪一行开始读取。需要设置 limit。\n   * limit (数字, 可选): 对于文本文件，要读取的最大行数。如果省略，则读取默认最大值（例如，2000 行）或在可行的情况下读取整个文件。\n * 行为:\n   * 对于文本文件: 返回内容。如果使用了 offset 和 limit，则仅返回该行范围的内容。如果由于行限制或行长限制而截断了内容，则会进行指示。\n   * 对于图像和 PDF 文件: 将文件内容作为 base64 编码的数据结构返回，适用于模型处理。\n   * 对于其他二进制文件: 尝试识别并跳过它们，返回一条消息，表明这是一个通用的二进制文件。\n * 输出: (llmContent):\n   * 对于文本文件: 文件内容，可能会带有截断消息前缀（例如，[File content truncated: showing lines 1-100 of\n     500 total lines...]\\nActual file content...）。\n   * 对于图像/PDF 文件: 一个包含 inlineData 的对象，其中包含 mimeType 和 base64 data（例如，{\n     inlineData: { mimeType: 'image/png', data: 'base64encodedstring' } }）。\n   * 对于其他二进制文件: 类似于 Cannot display content of binary file: /path/to/data.bin\n     的消息。\n * 确认: 否。\n\n\n3. write_file (WriteFile)#\n\nwrite_file 将内容写入指定的文件。如果文件存在，它将被覆盖。如果文件不存在，则会创建该文件（以及任何必要的父目录）。\n\n * 工具名称: write_file\n * 显示名称: WriteFile\n * 文件: write-file.ts\n * 参数:\n   * file_path (字符串, 必需): 要写入的文件的绝对路径。\n   * content (字符串, 必需): 要写入文件的内容。\n * 行为:\n   * 将提供的 content 写入 file_path。\n   * 如果父目录不存在，则创建它们。\n * 输出 (llmContent): 成功消息，例如，Successfully overwrote file: /path/to/your/file.txt\n   或 Successfully created and wrote to new file: /path/to/new/file.txt。\n * 确认: 是。在写入之前，会显示更改的差异并请求用户批准。\n\n\n4. glob (FindFiles)#\n\nglob 查找与特定 glob 模式匹配的文件（例如，src/**/*.ts、*.md），返回按修改时间排序的绝对路径（最新的在前）。\n\n * 工具名称: glob\n * 显示名称: FindFiles\n * 文件: glob.ts\n * 参数:\n   * pattern (字符串, 必需): 要匹配的 glob 模式（例如，\"*.py\"、\"src/**/*.js\"）。\n   * path (字符串, 可选): 要在其中搜索的目录的绝对路径。如果省略，则在工具的根目录中搜索。\n   * case_sensitive (布尔值, 可选): 搜索是否区分大小写。默认为 false。\n   * respect_git_ignore (布尔值, 可选): 查找文件时是否遵守 .gitignore 模式。默认为 true。\n * 行为:\n   * 在指定目录中搜索与 glob 模式匹配的文件。\n   * 返回绝对路径列表，按最近修改的文件优先排序。\n   * 默认情况下忽略常见的干扰目录，如 node_modules 和 .git。\n * 输出 (llmContent): 类似于以下的消息: Found 5 file(s) matching \"*.ts\" within src, sorted\n   by modification time (newest first):\\nsrc/file1.ts\\nsrc/subdir/file2.ts...\n * 确认: 否。\n\n\n5. search_file_content (SearchText)#\n\nsearch_file_content 在指定目录中的文件内容中搜索正则表达式模式。可以按 glob\n模式过滤文件。返回包含匹配项的行，以及它们的文件路径和行号。\n\n * 工具名称: search_file_content\n * 显示名称: SearchText\n * 文件: grep.ts\n * 参数:\n   * pattern (字符串, 必需): 要搜索的正则表达式 (regex)（例如，\"function\\\\s+myFunction\"）。\n   * path (字符串, 可选): 要在其中搜索的目录的绝对路径。默认为当前工作目录。\n   * include (字符串, 可选): 用于过滤要搜索的文件的 glob\n     模式（例如，\"*.js\"、\"src/**/*.{ts,tsx}\"）。如果省略，则搜索大多数文件（遵守常见的忽略规则）。\n * 行为:\n   * 如果在 Git 仓库中可用，则使用 git grep 以提高速度；否则，回退到系统 grep 或基于 JavaScript 的搜索。\n   * 返回匹配行的列表，每行都带有其文件路径（相对于搜索目录）和行号前缀。\n * 输出 (llmContent): 格式化的匹配项字符串，例如:\n   \n   \n\n * 确认: 否。\n\n\n6. replace (Edit)#\n\nreplace 替换文件中的文本。默认情况下，替换单个匹配项，但如果指定了\nexpected_replacements，则可以替换多个匹配项。此工具旨在进行精确、有针对性的更改，并且需要 old_string\n周围有大量上下文，以确保修改正确的位置。\n\n * 工具名称: replace\n\n * 显示名称: Edit\n\n * 文件: edit.ts\n\n * 参数:\n   \n   * file_path (字符串, 必需): 要修改的文件的绝对路径。\n   \n   * old_string (字符串, 必需): 要替换的确切文字文本。\n     \n     关键: 此字符串必须唯一标识要更改的单个实例。它应包含目标文本 之前 和 之后 至少 3 行的上下文，并且必须精确匹配空格和缩进。如果\n     old_string 为空，则该工具会尝试在 file_path 创建一个新文件，并将 new_string 作为内容。\n   \n   * new_string (字符串, 必需): 要替换 old_string 的确切文字文本。\n   \n   * expected_replacements (数字, 可选): 要替换的匹配项数量。默认为 1。\n\n * 行为:\n   \n   * 如果 old_string 为空且 file_path 不存在，则创建一个新文件，并将 new_string 作为内容。\n   * 如果提供了 old_string，则读取 file_path 并尝试找到 old_string 的唯一匹配项。\n   * 如果找到一个匹配项，则将其替换为 new_string。\n   * 增强可靠性（多阶段编辑校正）: 为了显著提高编辑的成功率，尤其是在模型提供的 old_string\n     可能不完全精确的情况下，该工具结合了多阶段编辑校正机制。\n     * 如果初始的 old_string 未找到或匹配多个位置，则该工具可以利用火山引擎模型迭代地改进 old_string（以及可能的\n       new_string）。\n     * 这种自我校正过程会尝试识别模型打算修改的唯一段落，即使初始上下文稍有不完美，也能使 replace 操作更加稳健。\n\n * 失败条件: 尽管有校正机制，但如果出现以下情况，该工具仍会失败：\n   \n   * file_path 不是绝对路径或在根目录之外。\n   * old_string 不为空，但 file_path 不存在。\n   * old_string 为空，但 file_path 已存在。\n   * 尝试校正后，在文件中仍未找到 old_string。\n   * old_string 找到多个匹配项，且自我校正机制无法将其解析为单个、明确的匹配项。\n\n * 输出 (llmContent):\n   \n   * 成功时: Successfully modified file: /path/to/file.txt (1 replacements). 或\n     Created new file: /path/to/new_file.txt with provided content.\n   * 失败时: 解释原因的错误消息（例如，Failed to edit, 0 occurrences found...、Failed to edit,\n     expected 1 occurrences but found 2...）。\n\n * 确认: 是。在写入文件之前，会显示建议的更改差异并请求用户批准。\n\n这些文件系统工具为 VeCLI 提供了理解和与您的本地项目上下文交互的基础。","routePath":"/zh/tools/file-system","lang":"","toc":[{"text":"1. `list_directory` (ReadFolder)","id":"1-list_directory-readfolder","depth":2,"charIndex":-1},{"text":"2. `read_file` (ReadFile)","id":"2-read_file-readfile","depth":2,"charIndex":-1},{"text":"3. `write_file` (WriteFile)","id":"3-write_file-writefile","depth":2,"charIndex":-1},{"text":"4. `glob` (FindFiles)","id":"4-glob-findfiles","depth":2,"charIndex":-1},{"text":"5. `search_file_content` (SearchText)","id":"5-search_file_content-searchtext","depth":2,"charIndex":-1},{"text":"6. `replace` (Edit)","id":"6-replace-edit","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":105,"title":"VeCLI 工具","content":"#\n\nVeCLI 包含内置工具，火山引擎模型使用这些工具与您的本地环境交互、访问信息并执行操作。这些工具增强了 CLI\n的功能，使其能够超越文本生成，协助您完成各种任务。\n\n\nVeCLI 工具概述#\n\n在 VeCLI 的上下文中，工具是火山引擎模型可以请求执行的特定功能或模块。例如，如果您要求火山引擎“总结 my_document.txt\n的内容”，模型很可能会识别出需要读取该文件，并会请求执行 read_file 工具。\n\n核心组件 (packages/core)\n管理这些工具，将其定义（模式）呈现给火山引擎模型，在请求时执行它们，并将结果返回给模型，以便进一步处理成面向用户的响应。\n\n这些工具提供了以下功能：\n\n * 访问本地信息： 工具允许火山引擎访问您的本地文件系统、读取文件内容、列出目录等。\n * 执行命令： 通过 run_shell_command 等工具，火山引擎可以运行 shell 命令（采取适当的安全措施并获得用户确认）。\n * 与网络交互： 工具可以从 URL 获取内容。\n * 执行操作： 工具可以修改文件、写入新文件或在您的系统上执行其他操作（同样，通常会采取安全措施）。\n * 使响应更可靠： 通过使用工具获取实时或特定的本地数据，火山引擎的响应可以更准确、相关，并基于您的实际上下文。\n\n\n如何使用 VeCLI 工具#\n\n要使用 VeCLI 工具，请向 VeCLI 提供一个提示。该过程如下：\n\n 1. 您向 VeCLI 提供一个提示。\n 2. CLI 将提示发送到核心。\n 3. 核心连同您的提示和对话历史记录，将可用工具列表及其描述/模式发送给火山引擎 API。\n 4. 火山引擎模型分析您的请求。如果它确定需要一个工具，其响应将包括一个请求，以使用特定参数执行某个工具。\n 5. 核心接收此工具请求，验证它，并（通常在用户对敏感操作进行确认后）执行该工具。\n 6. 工具的输出被发送回火山引擎模型。\n 7. 火山引擎模型使用工具的输出来制定其最终答案，然后通过核心发送回 CLI 并显示给您。\n\n您通常会在 CLI 中看到消息，指示何时调用了工具以及它是否成功或失败。\n\n\n安全和确认#\n\n许多工具，特别是那些可以修改您的文件系统或执行命令的工具（write_file、edit、run_shell_command），在设计时都考虑了安全性。VeCL\nI 通常会：\n\n * 要求确认： 在执行潜在的敏感操作之前提示您，并向您显示即将采取的操作。\n * 利用沙盒： 所有工具都受到沙盒强制执行的限制（请参阅 VeCLI 中的沙盒）。这意味着在沙盒中运行时，您希望使用的任何工具（包括 MCP\n   服务器）都必须在沙盒环境中可用。例如，要通过 npx 运行 MCP 服务器，npx 可执行文件必须安装在沙盒的 Docker 镜像中或在\n   sandbox-exec 环境中可用。\n\n在允许工具继续执行之前，仔细查看确认提示非常重要。\n\n\n了解更多关于 VeCLI 的工具#\n\nVeCLI 的内置工具可以大致分为以下几类：\n\n * 文件系统工具： 用于与文件和目录交互（读取、写入、列出、搜索等）。\n * Shell 工具 (run_shell_command)： 用于执行 shell 命令。\n * 网络获取工具 (web_fetch)： 用于从 URL 检索内容。\n * 网络搜索工具 (web_search)： 用于搜索网络。\n * 多文件读取工具 (read_many_files)： 一种专门用于从多个文件或目录读取内容的工具，通常由 @ 命令使用。\n * 内存工具 (save_memory)： 用于跨会话保存和回忆信息。\n\n此外，这些工具还结合了：\n\n * MCP 服务器: MCP 服务器充当火山引擎模型与您的本地环境或其他服务（如 API）之间的桥梁。\n * 沙盒: 沙盒将模型及其更改与您的环境隔离开来，以降低潜在风险。","routePath":"/zh/tools/","lang":"","toc":[{"text":"VeCLI 工具概述","id":"vecli-工具概述","depth":2,"charIndex":87},{"text":"如何使用 VeCLI 工具","id":"如何使用-vecli-工具","depth":2,"charIndex":564},{"text":"安全和确认","id":"安全和确认","depth":2,"charIndex":909},{"text":"了解更多关于 VeCLI 的工具","id":"了解更多关于-vecli-的工具","depth":2,"charIndex":1241}],"domain":"","frontmatter":{},"version":""},{"id":106,"title":"使用 VeCLI 的 MCP 服务器","content":"#\n\n本文档提供了在 VeCLI 中配置和使用模型上下文协议 (MCP) 服务器的指南。\n\n\n什么是 MCP 服务器？#\n\nMCP 服务器是一个通过模型上下文协议向 VeCLI 公开工具和资源的应用程序，使其能够与外部系统和数据源交互。MCP 服务器充当 Gemini\n模型与您的本地环境或其他服务（如 API）之间的桥梁。\n\nMCP 服务器使 VeCLI 能够：\n\n * 发现工具： 通过标准化的模式定义列出可用的工具、其描述和参数。\n * 执行工具： 使用定义的参数调用特定工具并接收结构化响应。\n * 访问资源： 从特定资源读取数据（尽管 VeCLI 主要专注于工具执行）。\n\n通过 MCP 服务器，您可以扩展 VeCLI 的功能，使其能够执行超出其内置功能的操作，例如与数据库、API、自定义脚本或专用工作流交互。\n\n\n核心集成架构#\n\nVeCLI 通过内置在核心包 (packages/core/src/tools/) 中的复杂发现和执行系统与 MCP 服务器集成：\n\n\n发现阶段 (mcp-client.ts)#\n\n发现过程由 discoverMcpTools() 编排，该过程：\n\n 1. 遍历配置的服务器 从您的 settings.json mcpServers 配置中\n 2. 建立连接 使用适当的传输机制（Stdio、SSE 或可流式 HTTP）\n 3. 获取工具定义 使用 MCP 协议从每个服务器获取\n 4. 清理和验证 工具模式以确保与 Gemini API 兼容\n 5. 注册工具 在全局工具注册表中，并解决冲突\n\n\n执行层 (mcp-tool.ts)#\n\n每个发现的 MCP 工具都包装在 DiscoveredMCPTool 实例中，该实例：\n\n * 处理确认逻辑 基于服务器信任设置和用户偏好\n * 管理工具执行 通过使用正确的参数调用 MCP 服务器\n * 处理响应 用于 LLM 上下文和用户显示\n * 维护连接状态 并处理超时\n\n\n传输机制#\n\nVeCLI 支持三种 MCP 传输类型：\n\n * Stdio 传输： 生成子进程并通过 stdin/stdout 通信\n * SSE 传输： 连接到服务器发送事件端点\n * 可流式 HTTP 传输： 使用 HTTP 流进行通信\n\n\n如何设置您的 MCP 服务器#\n\nVeCLI 使用 settings.json 文件中的 mcpServers 配置来定位和连接到 MCP 服务器。此配置支持具有不同传输机制的多个服务器。\n\n\n在 settings.json 中配置 MCP 服务器#\n\n您可以在 settings.json 文件中以两种主要方式配置 MCP 服务器：通过顶层 mcpServers 对象进行特定服务器定义，以及通过 mcp\n对象进行控制服务器发现和执行的全局设置。\n\n全局 MCP 设置 (mcp)#\n\nsettings.json 中的 mcp 对象允许您为所有 MCP 服务器定义全局规则。\n\n * mcp.serverCommand (字符串): 启动 MCP 服务器的全局命令。\n * mcp.allowed (字符串数组): 要允许的 MCP 服务器名称列表。如果设置了此项，则只会连接到此列表中的服务器（与 mcpServers\n   对象中的键匹配）。\n * mcp.excluded (字符串数组): 要排除的 MCP 服务器名称列表。此列表中的服务器将不会被连接。\n\n示例：\n\n\n\n服务器特定配置 (mcpServers)#\n\nmcpServers 对象是您定义希望 CLI 连接的每个单独 MCP 服务器的地方。\n\n\n配置结构#\n\n将 mcpServers 对象添加到您的 settings.json 文件中：\n\n\n\n\n配置属性#\n\n每个服务器配置支持以下属性：\n\n必需（以下之一）#\n\n * command (字符串): Stdio 传输的可执行文件路径\n * url (字符串): SSE 端点 URL（例如，\"http://localhost:8080/sse\"）\n * httpUrl (字符串): HTTP 流端点 URL\n\n可选#\n\n * args (字符串[]): Stdio 传输的命令行参数\n * headers (对象): 使用 url 或 httpUrl 时的自定义 HTTP 标头\n * env (对象): 服务器进程的环境变量。值可以使用 $VAR_NAME 或 ${VAR_NAME} 语法引用环境变量\n * cwd (字符串): Stdio 传输的工作目录\n * timeout (数字): 请求超时时间（毫秒）（默认值：600,000ms = 10 分钟）\n * trust (布尔值): 当为 true 时，绕过此服务器的所有工具调用确认（默认值：false）\n * includeTools (字符串[]): 要从此 MCP\n   服务器包含的工具名称列表。指定时，仅此处列出的工具将从此服务器可用（允许列表行为）。如果未指定，则默认启用服务器中的所有工具。\n * excludeTools (字符串[]): 要从此 MCP 服务器排除的工具名称列表。此处列出的工具将不可用于模型，即使服务器公开了它们。注意：\n   excludeTools 优先于 includeTools - 如果一个工具在两个列表中，则会被排除。\n\n\n远程 MCP 服务器的 OAuth 支持#\n\nVeCLI 支持使用 SSE 或 HTTP 传输对需要身份验证的远程 MCP 服务器进行 OAuth 2.0 身份验证。这可以安全地访问需要身份验证的 MCP\n服务器。\n\n自动 OAuth 发现#\n\n对于支持 OAuth 发现的服务器，您可以省略 OAuth 配置，让 CLI 自动发现它：\n\n\n\nCLI 将自动：\n\n * 检测服务器何时需要 OAuth 身份验证（401 响应）\n * 从服务器元数据发现 OAuth 端点\n * 如果支持，执行动态客户端注册\n * 处理 OAuth 流和令牌管理\n\n身份验证流#\n\n连接到启用了 OAuth 的服务器时：\n\n 1. 初始连接尝试 失败，返回 401 未授权\n 2. OAuth 发现 找到授权和令牌端点\n 3. 浏览器打开 用于用户身份验证（需要本地浏览器访问）\n 4. 授权码 兑换为访问令牌\n 5. 令牌安全存储 以供将来使用\n 6. 连接重试 使用有效令牌成功\n\n浏览器重定向要求#\n\n重要： OAuth 身份验证要求您的本地机器能够：\n\n * 打开 Web 浏览器进行身份验证\n * 在 http://localhost:7777/oauth/callback 上接收重定向\n\n此功能在以下环境中无法工作：\n\n * 没有浏览器访问权限的无头环境\n * 没有 X11 转发的远程 SSH 会话\n * 没有浏览器支持的容器化环境\n\n管理 OAuth 身份验证#\n\n使用 /mcp auth 命令管理 OAuth 身份验证：\n\n\n\nOAuth 配置属性#\n\n * enabled (布尔值): 为此服务器启用 OAuth\n * clientId (字符串): OAuth 客户端标识符（可选，使用动态注册时）\n * clientSecret (字符串): OAuth 客户端密钥（可选，用于公共客户端）\n * authorizationUrl (字符串): OAuth 授权端点（如果省略则自动发现）\n * tokenUrl (字符串): OAuth 令牌端点（如果省略则自动发现）\n * scopes (字符串[]): 所需的 OAuth 范围\n * redirectUri (字符串): 自定义重定向 URI（默认为 http://localhost:7777/oauth/callback）\n * tokenParamName (字符串): SSE URL 中令牌的查询参数名称\n * audiences (字符串[]): 令牌有效的受众\n\n令牌管理#\n\nOAuth 令牌会自动：\n\n * 安全存储 在 ~/.ve/mcp-oauth-tokens.json 中\n * 刷新 当过期时（如果有刷新令牌可用）\n * 验证 在每次连接尝试之前\n * 清理 当无效或过期时\n\n身份验证提供者类型#\n\n您可以使用 authProviderType 属性指定身份验证提供者类型：\n\n * authProviderType (字符串): 指定身份验证提供者。可以是以下之一：\n   * dynamic_discovery (默认): CLI 将自动从服务器发现 OAuth 配置。\n   * google_credentials: CLI 将使用 Google 应用默认凭据 (ADC)\n     对服务器进行身份验证。使用此提供者时，您必须指定所需的范围。\n\n\n\n\n示例配置#\n\nPython MCP 服务器 (Stdio)#\n\n\n\nNode.js MCP 服务器 (Stdio)#\n\n\n\n基于 Docker 的 MCP 服务器#\n\n\n\n基于 HTTP 的 MCP 服务器#\n\n\n\n带有自定义标头的基于 HTTP 的 MCP 服务器#\n\n\n\n带有工具过滤的 MCP 服务器#\n\n\n\n\n发现阶段深入#\n\n当 VeCLI 启动时，它通过以下详细过程执行 MCP 服务器发现：\n\n\n1. 服务器迭代和连接#\n\n对于 mcpServers 中配置的每个服务器：\n\n 1. 状态跟踪开始： 服务器状态设置为 CONNECTING\n 2. 传输选择： 基于配置属性：\n    * httpUrl → StreamableHTTPClientTransport\n    * url → SSEClientTransport\n    * command → StdioClientTransport\n 3. 连接建立： MCP 客户端尝试使用配置的超时连接\n 4. 错误处理： 连接失败会被记录，服务器状态设置为 DISCONNECTED\n\n\n2. 工具发现#\n\n连接成功后：\n\n 1. 工具列表： 客户端调用 MCP 服务器的工具列表端点\n 2. 模式验证： 验证每个工具的函数声明\n 3. 工具过滤： 根据 includeTools 和 excludeTools 配置过滤工具\n 4. 名称清理： 工具名称被清理以满足 Gemini API 要求：\n    * 无效字符（非字母数字、下划线、点、连字符）被替换为下划线\n    * 超过 63 个字符的名称会被截断，并用中间替换 (___)\n\n\n3. 冲突解决#\n\n当多个服务器公开同名工具时：\n\n 1. 首次注册获胜： 第一个注册工具名称的服务器获得未加前缀的名称\n 2. 自动加前缀： 后续服务器获得加前缀的名称：serverName__toolName\n 3. 注册表跟踪： 工具注册表维护服务器名称与其工具之间的映射\n\n\n4. 模式处理#\n\n工具参数模式经过清理以确保与 Gemini API 兼容：\n\n * $schema 属性 被移除\n * additionalProperties 被剥离\n * 带有 default 的 anyOf 其默认值被移除（Vertex AI 兼容性）\n * 递归处理 应用于嵌套模式\n\n\n5. 连接管理#\n\n发现后：\n\n * 持久连接： 成功注册工具的服务器保持其连接\n * 清理： 不提供可用工具的服务器的连接会被关闭\n * 状态更新： 最终服务器状态设置为 CONNECTED 或 DISCONNECTED\n\n\n工具执行流程#\n\n当 Gemini 模型决定使用 MCP 工具时，会发生以下执行流程：\n\n\n1. 工具调用#\n\n模型生成一个 FunctionCall，其中包含：\n\n * 工具名称： 注册的名称（可能加了前缀）\n * 参数： 与工具参数模式匹配的 JSON 对象\n\n\n2. 确认过程#\n\n每个 DiscoveredMCPTool 实现了复杂的确认逻辑：\n\n基于信任的绕过#\n\n\n\n动态允许列表#\n\n系统为以下内容维护内部允许列表：\n\n * 服务器级别： serverName → 来自此服务器的所有工具都被信任\n * 工具级别： serverName.toolName → 此特定工具被信任\n\n用户选择处理#\n\n需要确认时，用户可以选择：\n\n * 仅此一次： 仅执行这一次\n * 始终允许此工具： 添加到工具级别允许列表\n * 始终允许此服务器： 添加到服务器级别允许列表\n * 取消： 中止执行\n\n\n3. 执行#\n\n确认后（或信任绕过）：\n\n 1. 参数准备： 参数根据工具的模式进行验证\n\n 2. MCP 调用： 底层 CallableTool 使用以下内容调用服务器：\n    \n    \n\n 3. 响应处理： 结果被格式化为 LLM 上下文和用户显示\n\n\n4. 响应处理#\n\n执行结果包含：\n\n * llmContent： 用于语言模型上下文的原始响应部分\n * returnDisplay： 用于用户显示的格式化输出（通常是 JSON 格式的代码块）\n\n\n如何与您的 MCP 服务器交互#\n\n\n使用 /mcp 命令#\n\n/mcp 命令提供有关 MCP 服务器设置的全面信息：\n\n\n\n这将显示：\n\n * 服务器列表： 所有配置的 MCP 服务器\n * 连接状态： CONNECTED、CONNECTING 或 DISCONNECTED\n * 服务器详细信息： 配置摘要（不包括敏感数据）\n * 可用工具： 每个服务器的工具列表及其描述\n * 发现状态： 整体发现过程状态\n\n\n示例 /mcp 输出#\n\n\n\n\n工具使用#\n\n发现后，MCP 工具对 Gemini 模型来说就像内置工具一样可用。模型将自动：\n\n 1. 选择合适的工具 基于您的请求\n 2. 显示确认对话框 （除非服务器受信任）\n 3. 执行工具 使用正确的参数\n 4. 以用户友好的格式显示结果\n\n\n状态监控和故障排除#\n\n\n连接状态#\n\nMCP 集成跟踪几种状态：\n\n服务器状态 (MCPServerStatus)#\n\n * DISCONNECTED: 服务器未连接或有错误\n * CONNECTING: 正在进行连接尝试\n * CONNECTED: 服务器已连接并准备就绪\n\n发现状态 (MCPDiscoveryState)#\n\n * NOT_STARTED: 发现尚未开始\n * IN_PROGRESS: 当前正在发现服务器\n * COMPLETED: 发现已完成（有或没有错误）\n\n\n常见问题和解决方案#\n\n服务器无法连接#\n\n症状： 服务器显示 DISCONNECTED 状态\n\n故障排除：\n\n 1. 检查配置： 验证 command、args 和 cwd 是否正确\n 2. 手动测试： 直接运行服务器命令以确保其正常工作\n 3. 检查依赖项： 确保所有必需的包都已安装\n 4. 查看日志： 在 CLI 输出中查找错误消息\n 5. 验证权限： 确保 CLI 可以执行服务器命令\n\n未发现工具#\n\n症状： 服务器连接但没有可用的工具\n\n故障排除：\n\n 1. 验证工具注册： 确保您的服务器实际注册了工具\n 2. 检查 MCP 协议： 确认您的服务器正确实现了 MCP 工具列表\n 3. 查看服务器日志： 检查 stderr 输出以查找服务器端错误\n 4. 测试工具列表： 手动测试服务器的工具发现端点\n\n工具无法执行#\n\n症状： 工具被发现但在执行期间失败\n\n故障排除：\n\n 1. 参数验证： 确保您的工具接受预期的参数\n 2. 模式兼容性： 验证您的输入模式是有效的 JSON 模式\n 3. 错误处理： 检查您的工具是否抛出未处理的异常\n 4. 超时问题： 考虑增加 timeout 设置\n\n沙盒兼容性#\n\n症状： 启用沙盒时 MCP 服务器失败\n\n解决方案：\n\n 1. 基于 Docker 的服务器： 使用包含所有依赖项的 Docker 容器\n 2. 路径可访问性： 确保沙盒环境中可以访问服务器可执行文件\n 3. 网络访问： 配置沙盒以允许必要的网络连接\n 4. 环境变量： 验证所需的环境变量是否已传递\n\n\n调试技巧#\n\n 1. 启用调试模式： 使用 --debug 运行 CLI 以获取详细输出\n 2. 检查 stderr： MCP 服务器 stderr 被捕获并记录（INFO 消息被过滤）\n 3. 测试隔离： 在集成之前独立测试您的 MCP 服务器\n 4. 增量设置： 在添加复杂功能之前从简单工具开始\n 5. 频繁使用 /mcp： 在开发过程中监控服务器状态\n\n\n重要说明#\n\n\n安全注意事项#\n\n * 信任设置： trust 选项绕过所有确认对话框。请谨慎使用，仅用于您完全控制的服务器\n * 访问令牌： 配置包含 API 密钥或令牌的环境变量时要注意安全\n * 沙盒兼容性： 使用沙盒时，确保 MCP 服务器在沙盒环境中可用\n * 私人数据： 使用范围广泛的个人访问令牌可能导致存储库之间的信息泄露\n\n\n性能和资源管理#\n\n * 连接持久性： CLI 保持与成功注册工具的服务器的持久连接\n * 自动清理： 到不提供工具的服务器的连接会自动关闭\n * 超时管理： 根据服务器的响应特性配置适当的超时\n * 资源监控： MCP 服务器作为单独的进程运行并消耗系统资源\n\n\n模式兼容性#\n\n * 属性剥离： 系统会自动移除某些模式属性（$schema、additionalProperties）以确保与 Gemini API 兼容\n * 名称清理： 工具名称会自动清理以满足 API 要求\n * 冲突解决： 服务器之间的工具名称冲突通过自动加前缀解决\n\n这种全面的集成使 MCP 服务器成为扩展 VeCLI 功能的强大方式，同时保持安全性、可靠性和易用性。\n\n\n从工具返回富内容#\n\nMCP\n工具不仅限于返回简单文本。您可以返回富多部分内容，包括文本、图像、音频和其他二进制数据，作为单个工具响应。这使您能够构建强大的工具，可以在单个回合中向模型提供多\n样化的信息。\n\n所有从工具返回的数据都会被处理并作为上下文发送给模型，以供其下一次生成使用，使其能够推理或总结提供的信息。\n\n\n工作原理#\n\n要返回富内容，您的工具响应必须遵循 MCP 规范中的 CallToolResult。结果的 content 字段应该是一个 ContentBlock\n对象数组。VeCLI 将正确处理此数组，将文本与二进制数据分离并打包给模型。\n\n您可以在 content 数组中混合和匹配不同的内容块类型。支持的块类型包括：\n\n * text\n * image\n * audio\n * resource (嵌入内容)\n * resource_link\n\n\n示例：返回文本和图像#\n\n以下是一个有效的 MCP 工具 JSON 响应示例，该响应同时返回文本描述和图像：\n\n\n\n当 VeCLI 收到此响应时，它将：\n\n 1. 提取所有文本并将其组合成一个 functionResponse 部分供模型使用。\n 2. 将图像数据作为单独的 inlineData 部分呈现。\n 3. 在 CLI 中提供一个干净、用户友好的摘要，表明已收到文本和图像。\n\n这使您能够构建复杂的工具，向 Gemini 模型提供丰富的多模态上下文。\n\n\n作为斜杠命令的 MCP 提示#\n\n除了工具，MCP 服务器还可以公开预定义的提示，这些提示可以在 VeCLI 中作为斜杠命令执行。这使您能够为常见或复杂的查询创建快捷方式，可以通过名称轻松调用。\n\n\n在服务器上定义提示#\n\n以下是一个定义提示的小型 stdio MCP 服务器示例：\n\n\n\n这可以在 settings.json 中的 mcpServers 下包含：\n\n\n\n\n调用提示#\n\n发现提示后，您可以使用其名称作为斜杠命令来调用它。CLI 将自动处理参数解析。\n\n\n\n或者，使用位置参数：\n\n\n\n运行此命令时，VeCLI 在 MCP 服务器上执行 prompts/get 方法并提供参数。服务器负责将参数代入提示模板并返回最终提示文本。CLI\n然后将此提示发送给模型执行。这提供了一种方便的方式来自动化和共享常见工作流。\n\n\n使用 gemini mcp 管理 MCP 服务器#\n\n虽然您总是可以通过手动编辑 settings.json 文件来配置 MCP 服务器，但 VeCLI\n提供了一组方便的命令来以编程方式管理您的服务器配置。这些命令简化了添加、列出和删除 MCP 服务器的过程，而无需直接编辑 JSON 文件。\n\n\n添加服务器 (gemini mcp add)#\n\nadd 命令在您的 settings.json 中配置一个新的 MCP 服务器。根据范围 (-s, --scope)，它将被添加到用户配置\n~/.ve/settings.json 或项目配置 .ve/settings.json 文件中。\n\n命令：\n\n\n\n * <name>: 服务器的唯一名称。\n * <commandOrUrl>: 要执行的命令（用于 stdio）或 URL（用于 http/sse）。\n * [args...]: stdio 命令的可选参数。\n\n选项（标志）：\n\n * -s, --scope: 配置范围（用户或项目）。[默认: \"project\"]\n * -t, --transport: 传输类型（stdio, sse, http）。[默认: \"stdio\"]\n * -e, --env: 设置环境变量（例如 -e KEY=value）。\n * -H, --header: 为 SSE 和 HTTP 传输设置 HTTP 标头（例如 -H \"X-Api-Key: abc123\" -H\n   \"Authorization: Bearer abc123\"）。\n * --timeout: 设置连接超时时间（毫秒）。\n * --trust: 信任服务器（绕过所有工具调用确认提示）。\n * --description: 设置服务器的描述。\n * --include-tools: 以逗号分隔的要包含的工具列表。\n * --exclude-tools: 以逗号分隔的要排除的工具列表。\n\n添加一个 stdio 服务器#\n\n这是运行本地服务器的默认传输。\n\n\n\n添加一个 HTTP 服务器#\n\n此传输用于使用可流式 HTTP 传输的服务器。\n\n\n\n添加一个 SSE 服务器#\n\n此传输用于使用服务器发送事件 (SSE) 的服务器。\n\n\n\n\n列出服务器 (gemini mcp list)#\n\n要查看当前配置的所有 MCP 服务器，请使用 list 命令。它显示每个服务器的名称、配置详细信息和连接状态。\n\n命令：\n\n\n\n示例输出：\n\n\n\n\n删除服务器 (gemini mcp remove)#\n\n要从配置中删除服务器，请使用 remove 命令和服务器名称。\n\n命令：\n\n\n\n示例：\n\n\n\n这将根据范围 (-s, --scope) 在相应的 settings.json 文件中的 mcpServers 对象中查找并删除 \"my-server\" 条目。","routePath":"/zh/tools/mcp-server","lang":"","toc":[{"text":"什么是 MCP 服务器？","id":"什么是-mcp-服务器","depth":2,"charIndex":46},{"text":"核心集成架构","id":"核心集成架构","depth":2,"charIndex":363},{"text":"发现阶段 (`mcp-client.ts`)","id":"发现阶段-mcp-clientts","depth":3,"charIndex":-1},{"text":"执行层 (`mcp-tool.ts`)","id":"执行层-mcp-toolts","depth":3,"charIndex":-1},{"text":"传输机制","id":"传输机制","depth":3,"charIndex":834},{"text":"如何设置您的 MCP 服务器","id":"如何设置您的-mcp-服务器","depth":2,"charIndex":957},{"text":"在 settings.json 中配置 MCP 服务器","id":"在-settingsjson-中配置-mcp-服务器","depth":3,"charIndex":1054},{"text":"全局 MCP 设置 (`mcp`)","id":"全局-mcp-设置-mcp","depth":4,"charIndex":-1},{"text":"服务器特定配置 (`mcpServers`)","id":"服务器特定配置-mcpservers","depth":4,"charIndex":-1},{"text":"配置结构","id":"配置结构","depth":3,"charIndex":1517},{"text":"配置属性","id":"配置属性","depth":3,"charIndex":1568},{"text":"必需（以下之一）","id":"必需以下之一","depth":4,"charIndex":1591},{"text":"可选","id":"可选","depth":4,"charIndex":1726},{"text":"远程 MCP 服务器的 OAuth 支持","id":"远程-mcp-服务器的-oauth-支持","depth":3,"charIndex":2239},{"text":"自动 OAuth 发现","id":"自动-oauth-发现","depth":4,"charIndex":2348},{"text":"身份验证流","id":"身份验证流","depth":4,"charIndex":2513},{"text":"浏览器重定向要求","id":"浏览器重定向要求","depth":4,"charIndex":2673},{"text":"管理 OAuth 身份验证","id":"管理-oauth-身份验证","depth":4,"charIndex":2856},{"text":"OAuth 配置属性","id":"oauth-配置属性","depth":4,"charIndex":2905},{"text":"令牌管理","id":"令牌管理","depth":4,"charIndex":3313},{"text":"身份验证提供者类型","id":"身份验证提供者类型","depth":4,"charIndex":3427},{"text":"示例配置","id":"示例配置","depth":3,"charIndex":3670},{"text":"Python MCP 服务器 (Stdio)","id":"python-mcp-服务器-stdio","depth":4,"charIndex":3677},{"text":"Node.js MCP 服务器 (Stdio)","id":"nodejs-mcp-服务器-stdio","depth":4,"charIndex":3704},{"text":"基于 Docker 的 MCP 服务器","id":"基于-docker-的-mcp-服务器","depth":4,"charIndex":3732},{"text":"基于 HTTP 的 MCP 服务器","id":"基于-http-的-mcp-服务器","depth":4,"charIndex":3756},{"text":"带有自定义标头的基于 HTTP 的 MCP 服务器","id":"带有自定义标头的基于-http-的-mcp-服务器","depth":4,"charIndex":3778},{"text":"带有工具过滤的 MCP 服务器","id":"带有工具过滤的-mcp-服务器","depth":4,"charIndex":3808},{"text":"发现阶段深入","id":"发现阶段深入","depth":2,"charIndex":3829},{"text":"1. 服务器迭代和连接","id":"1-服务器迭代和连接","depth":3,"charIndex":3875},{"text":"2. 工具发现","id":"2-工具发现","depth":3,"charIndex":4151},{"text":"3. 冲突解决","id":"3-冲突解决","depth":3,"charIndex":4379},{"text":"4. 模式处理","id":"4-模式处理","depth":3,"charIndex":4521},{"text":"5. 连接管理","id":"5-连接管理","depth":3,"charIndex":4671},{"text":"工具执行流程","id":"工具执行流程","depth":2,"charIndex":4785},{"text":"1. 工具调用","id":"1-工具调用","depth":3,"charIndex":4831},{"text":"2. 确认过程","id":"2-确认过程","depth":3,"charIndex":4919},{"text":"基于信任的绕过","id":"基于信任的绕过","depth":4,"charIndex":4963},{"text":"动态允许列表","id":"动态允许列表","depth":4,"charIndex":4975},{"text":"用户选择处理","id":"用户选择处理","depth":4,"charIndex":5082},{"text":"3. 执行","id":"3-执行","depth":3,"charIndex":5186},{"text":"4. 响应处理","id":"4-响应处理","depth":3,"charIndex":5317},{"text":"如何与您的 MCP 服务器交互","id":"如何与您的-mcp-服务器交互","depth":2,"charIndex":5418},{"text":"使用 `/mcp` 命令","id":"使用-mcp-命令","depth":3,"charIndex":-1},{"text":"示例 `/mcp` 输出","id":"示例-mcp-输出","depth":3,"charIndex":-1},{"text":"工具使用","id":"工具使用","depth":3,"charIndex":5643},{"text":"状态监控和故障排除","id":"状态监控和故障排除","depth":2,"charIndex":5770},{"text":"连接状态","id":"连接状态","depth":3,"charIndex":5783},{"text":"服务器状态 (`MCPServerStatus`)","id":"服务器状态-mcpserverstatus","depth":4,"charIndex":-1},{"text":"发现状态 (`MCPDiscoveryState`)","id":"发现状态-mcpdiscoverystate","depth":4,"charIndex":-1},{"text":"常见问题和解决方案","id":"常见问题和解决方案","depth":3,"charIndex":6016},{"text":"服务器无法连接","id":"服务器无法连接","depth":4,"charIndex":6028},{"text":"未发现工具","id":"未发现工具","depth":4,"charIndex":6215},{"text":"工具无法执行","id":"工具无法执行","depth":4,"charIndex":6376},{"text":"沙盒兼容性","id":"沙盒兼容性","depth":4,"charIndex":6521},{"text":"调试技巧","id":"调试技巧","depth":3,"charIndex":6682},{"text":"重要说明","id":"重要说明","depth":2,"charIndex":6864},{"text":"安全注意事项","id":"安全注意事项","depth":3,"charIndex":6872},{"text":"性能和资源管理","id":"性能和资源管理","depth":3,"charIndex":7036},{"text":"模式兼容性","id":"模式兼容性","depth":3,"charIndex":7169},{"text":"从工具返回富内容","id":"从工具返回富内容","depth":2,"charIndex":7363},{"text":"工作原理","id":"工作原理","depth":3,"charIndex":7523},{"text":"示例：返回文本和图像","id":"示例返回文本和图像","depth":3,"charIndex":7750},{"text":"作为斜杠命令的 MCP 提示","id":"作为斜杠命令的-mcp-提示","depth":2,"charIndex":7983},{"text":"在服务器上定义提示","id":"在服务器上定义提示","depth":3,"charIndex":8083},{"text":"调用提示","id":"调用提示","depth":3,"charIndex":8170},{"text":"使用 `gemini mcp` 管理 MCP 服务器","id":"使用-gemini-mcp-管理-mcp-服务器","depth":2,"charIndex":-1},{"text":"添加服务器 (`gemini mcp add`)","id":"添加服务器-gemini-mcp-add","depth":3,"charIndex":-1},{"text":"添加一个 stdio 服务器","id":"添加一个-stdio-服务器","depth":4,"charIndex":9175},{"text":"添加一个 HTTP 服务器","id":"添加一个-http-服务器","depth":4,"charIndex":9211},{"text":"添加一个 SSE 服务器","id":"添加一个-sse-服务器","depth":4,"charIndex":9254},{"text":"列出服务器 (`gemini mcp list`)","id":"列出服务器-gemini-mcp-list","depth":3,"charIndex":-1},{"text":"删除服务器 (`gemini mcp remove`)","id":"删除服务器-gemini-mcp-remove","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":107,"title":"内存工具 (`save_memory`)","content":"内存工具 (save_memory)#\n\n本文档描述了 VeCLI 的 save_memory 工具。\n\n\n描述#\n\n使用 save_memory 可以在您的 VeCLI 会话之间保存和回忆信息。通过 save_memory，您可以指示 CLI\n跨会话记住关键细节，从而提供个性化和有针对性的帮助。\n\n\n参数#\n\nsave_memory 接受一个参数：\n\n * fact (字符串, 必需): 要记住的特定事实或信息片段。这应该是一个清晰、独立的、用自然语言编写的陈述。\n\n\n如何在 VeCLI 中使用 save_memory#\n\n该工具将提供的 fact 追加到位于用户主目录 (~/.ve/VE.md) 中的一个特殊的 VE.md 文件中。此文件可以配置为不同的名称。\n\n添加后，这些事实会存储在 ## vecli Added Memories 部分下。此文件在后续会话中作为上下文加载，允许 CLI 回忆保存的信息。\n\n用法:\n\n\n\n\nsave_memory 示例#\n\n记住用户偏好:\n\n\n\n存储项目特定的细节:\n\n\n\n\n重要提示#\n\n * 一般用法: 此工具应用于简洁、重要的事实。它不适用于存储大量数据或对话历史。\n * 内存文件: 内存文件是一个纯文本 Markdown 文件，因此您可以根据需要手动查看和编辑它。","routePath":"/zh/tools/memory","lang":"","toc":[{"text":"描述","id":"描述","depth":2,"charIndex":53},{"text":"参数","id":"参数","depth":3,"charIndex":151},{"text":"如何在 VeCLI 中使用 `save_memory`","id":"如何在-vecli-中使用-save_memory","depth":2,"charIndex":-1},{"text":"`save_memory` 示例","id":"save_memory-示例","depth":3,"charIndex":-1},{"text":"重要提示","id":"重要提示","depth":2,"charIndex":463}],"domain":"","frontmatter":{},"version":""},{"id":108,"title":"多文件读取工具 (`read_many_files`)","content":"多文件读取工具 (read_many_files)#\n\n本文档描述了 VeCLI 的 read_many_files 工具。\n\n\n描述#\n\n使用 read_many_files 可以从由路径或 glob 模式指定的多个文件中读取内容。此工具的行为取决于提供的文件：\n\n * 对于文本文件，此工具将其内容连接成一个字符串。\n * 对于图像（例如 PNG、JPEG）、PDF、音频（MP3、WAV）和视频（MP4、MOV）文件，如果通过名称或扩展名明确请求，它会读取并以 base64\n   编码的数据形式返回。\n\nread_many_files 可用于执行诸如获取代码库概览、查找特定功能的实现位置、审阅文档或从多个配置文件中收集上下文等任务。\n\n注意: read_many_files 会查找与提供的路径或 glob 模式匹配的文件。像 \"/docs\" 这样的目录路径将返回空结果；该工具需要像\n\"/docs/*\" 或 \"/docs/*.md\" 这样的模式来识别相关文件。\n\n\n参数#\n\nread_many_files 接受以下参数：\n\n * paths (字符串列表, 必需): 一个 glob 模式或相对于工具目标目录的路径数组（例如 [\"src/**/*.ts\"]、[\"README.md\",\n   \"docs/*\", \"assets/logo.png\"]）。\n * exclude (字符串列表, 可选): 要排除的文件/目录的 glob 模式（例如 [\"**/*.log\", \"temp/\"]）。如果\n   useDefaultExcludes 为 true，这些模式将添加到默认排除列表中。\n * include (字符串列表, 可选): 要包含的额外 glob 模式。这些模式将与 paths 合并（例如 [\"*.test.ts\"]\n   用于特别添加测试文件如果它们被广泛排除，或者 [\"images/*.jpg\"] 用于包含特定图像类型）。\n * recursive (布尔值, 可选): 是否递归搜索。这主要由 glob 模式中的 ** 控制。默认为 true。\n * useDefaultExcludes (布尔值, 可选): 是否应用默认排除模式列表（例如 node_modules、.git、非图像/PDF\n   二进制文件）。默认为 true。\n * respect_git_ignore (布尔值, 可选): 查找文件时是否遵守 .gitignore 模式。默认为 true。\n\n\n如何在 VeCLI 中使用 read_many_files#\n\nread_many_files 会搜索与提供的 paths 和 include 模式匹配的文件，同时遵守 exclude 模式和默认排除列表（如果启用）。\n\n * 对于文本文件：它读取每个匹配文件的内容（尝试跳过未明确请求为图像/PDF 的二进制文件）并将其连接成一个字符串，在每个文件的内容之间使用分隔符 ---\n   {filePath} ---。默认使用 UTF-8 编码。\n * 该工具在最后一个文件之后插入 --- End of content ---。\n * 对于图像和 PDF 文件：如果通过名称或扩展名明确请求（例如 paths: [\"logo.png\"] 或 include:\n   [\"*.pdf\"]），该工具会读取文件并将其内容作为 base64 编码的字符串返回。\n * 该工具会通过检查初始内容中的空字节来尝试检测并跳过其他二进制文件。\n\n用法:\n\n\n\n\nread_many_files 示例#\n\n读取 src 目录中的所有 TypeScript 文件：\n\n\n\n读取主 README、docs 目录中的所有 Markdown 文件和一个特定的 logo 图像，排除一个特定文件：\n\n\n\n读取所有 JavaScript 文件，但明确包含测试文件和 images 文件夹中的所有 JPEG：\n\n\n\n\n重要提示#\n\n * 二进制文件处理:\n   * 图像/PDF/音频/视频文件: 该工具可以读取常见的图像类型（PNG、JPEG 等）、PDF、音频（mp3、wav）和视频（mp4、mov）文件，并以\n     base64 编码数据的形式返回。这些文件 必须 通过 paths 或 include 模式明确指定（例如，通过指定确切的文件名如 video.mp4\n     或模式如 *.mov）。\n   * 其他二进制文件: 该工具会通过检查初始内容中的空字节来尝试检测并跳过其他类型的二进制文件。该工具会从其输出中排除这些文件。\n * 性能: 读取大量文件或非常大的单个文件可能会占用大量资源。\n * 路径特异性: 确保相对于工具目标目录正确指定路径和 glob 模式。对于图像/PDF 文件，确保模式足够具体以包含它们。\n * 默认排除: 注意默认排除模式（如 node_modules、.git），如果需要覆盖它们，请使用\n   useDefaultExcludes=False，但要谨慎操作。","routePath":"/zh/tools/multi-file","lang":"","toc":[{"text":"描述","id":"描述","depth":2,"charIndex":64},{"text":"参数","id":"参数","depth":3,"charIndex":438},{"text":"如何在 VeCLI 中使用 `read_many_files`","id":"如何在-vecli-中使用-read_many_files","depth":2,"charIndex":-1},{"text":"`read_many_files` 示例","id":"read_many_files-示例","depth":2,"charIndex":-1},{"text":"重要提示","id":"重要提示","depth":2,"charIndex":1648}],"domain":"","frontmatter":{},"version":""},{"id":109,"title":"Shell 工具 (`run_shell_command`)","content":"Shell 工具 (run_shell_command)#\n\n本文档描述了 VeCLI 的 run_shell_command 工具。\n\n\n描述#\n\n使用 run_shell_command 与底层系统交互、运行脚本或执行命令行操作。run_shell_command 执行给定的 shell 命令。在\nWindows 上，该命令将使用 cmd.exe /c 执行。在其他平台上，该命令将使用 bash -c 执行。\n\n\n参数#\n\nrun_shell_command 接受以下参数：\n\n * command (字符串, 必需): 要执行的确切 shell 命令。\n * description (字符串, 可选): 命令目的的简要描述，将显示给用户。\n * directory (字符串, 可选): 执行命令的目录（相对于项目根目录）。如果未提供，则命令在项目根目录中运行。\n\n\n如何在 VeCLI 中使用 run_shell_command#\n\n使用 run_shell_command 时，该命令作为子进程执行。run_shell_command 可以使用 &\n启动后台进程。该工具返回有关执行的详细信息，包括：\n\n * Command: 执行的命令。\n * Directory: 运行命令的目录。\n * Stdout: 标准输出流的输出。\n * Stderr: 标准错误流的输出。\n * Error: 子进程报告的任何错误消息。\n * Exit Code: 命令的退出代码。\n * Signal: 如果命令被信号终止，则为信号编号。\n * Background PIDs: 启动的任何后台进程的 PID 列表。\n\n用法:\n\n\n\n\nrun_shell_command 示例#\n\n列出当前目录中的文件：\n\n\n\n在特定目录中运行脚本：\n\n\n\n启动后台服务器：\n\n\n\n\n重要提示#\n\n * 安全性: 执行命令时要小心，特别是那些由用户输入构建的命令，以防止安全漏洞。\n * 交互式命令: 避免需要交互式用户输入的命令，因为这可能导致工具挂起。如果可用，请使用非交互式标志（例如，npm init -y）。\n * 错误处理: 检查 Stderr、Error 和 Exit Code 字段以确定命令是否成功执行。\n * 后台进程: 当使用 & 在后台运行命令时，工具将立即返回，进程将在后台继续运行。Background PIDs 字段将包含后台进程的进程 ID。\n\n\n环境变量#\n\n当 run_shell_command 执行命令时，它会在子进程的环境中设置 GEMINI_CLI=1 环境变量。这允许脚本或工具检测它们是否在 VeCLI\n内运行。\n\n\n命令限制#\n\n您可以通过在配置文件中使用 tools.core 和 tools.exclude 设置来限制 run_shell_command 工具可以执行的命令。\n\n * tools.core: 要将 run_shell_command 限制为特定的一组命令，请在 tools 类别下的 core 列表中添加格式为\n   run_shell_command(<command>) 的条目。例如，\"tools\": {\"core\":\n   [\"run_shell_command(git)\"]} 将只允许 git 命令。包含通用的 run_shell_command\n   作为通配符，允许任何未明确阻止的命令。\n * tools.exclude: 要阻止特定命令，请在 tools 类别下的 exclude 列表中添加格式为\n   run_shell_command(<command>) 的条目。例如，\"tools\": {\"exclude\":\n   [\"run_shell_command(rm)\"]} 将阻止 rm 命令。\n\n验证逻辑旨在安全且灵活：\n\n 1. 命令链接禁用: 该工具会自动拆分用 &&、|| 或 ; 链接的命令，并分别验证每个部分。如果链中的任何部分被禁止，则整个命令将被阻止。\n 2. 前缀匹配: 该工具使用前缀匹配。例如，如果您允许 git，则可以运行 git status 或 git log。\n 3. 阻止列表优先: 始终首先检查 tools.exclude 列表。如果命令与被阻止的前缀匹配，则将被拒绝，即使它也与 tools.core\n    中允许的前缀匹配。\n\n\n命令限制示例#\n\n仅允许特定命令前缀\n\n要仅允许 git 和 npm 命令，并阻止所有其他命令：\n\n\n\n * git status: 允许\n * npm install: 允许\n * ls -l: 阻止\n\n阻止特定命令前缀\n\n要阻止 rm 并允许所有其他命令：\n\n\n\n * rm -rf /: 阻止\n * git status: 允许\n * npm install: 允许\n\n阻止列表优先\n\n如果命令前缀同时在 tools.core 和 tools.exclude 中，则将被阻止。\n\n\n\n * git push origin main: 阻止\n * git status: 允许\n\n阻止所有 shell 命令\n\n要阻止所有 shell 命令，请将 run_shell_command 通配符添加到 tools.exclude：\n\n\n\n * ls -l: 阻止\n * any other command: 阻止\n\n\nexcludeTools 的安全说明#\n\nrun_shell_command 的 excludeTools\n中的命令特定限制基于简单的字符串匹配，很容易被绕过。此功能不是安全机制，不应依赖它来安全地执行不受信任的代码。建议使用 coreTools\n明确选择可以执行的命令。","routePath":"/zh/tools/shell","lang":"","toc":[{"text":"描述","id":"描述","depth":2,"charIndex":69},{"text":"参数","id":"参数","depth":3,"charIndex":210},{"text":"如何在 VeCLI 中使用 `run_shell_command`","id":"如何在-vecli-中使用-run_shell_command","depth":2,"charIndex":-1},{"text":"`run_shell_command` 示例","id":"run_shell_command-示例","depth":2,"charIndex":-1},{"text":"重要提示","id":"重要提示","depth":2,"charIndex":783},{"text":"环境变量","id":"环境变量","depth":2,"charIndex":1031},{"text":"命令限制","id":"命令限制","depth":2,"charIndex":1124},{"text":"命令限制示例","id":"命令限制示例","depth":3,"charIndex":1825},{"text":"`excludeTools` 的安全说明","id":"excludetools-的安全说明","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":110,"title":"网络获取工具 (`web_fetch`)","content":"网络获取工具 (web_fetch)#\n\n本文档描述了 VeCLI 的 web_fetch 工具。\n\n\n描述#\n\n使用 web_fetch 来总结、比较或从网页中提取信息。web_fetch 工具处理嵌入在提示中的一个或多个 URL（最多 20\n个）的内容。web_fetch 接受一个自然语言提示并返回一个生成的响应。\n\n\n参数#\n\nweb_fetch 接受一个参数：\n\n * prompt (字符串, 必需): 一个包含要获取的 URL（最多 20 个）和如何处理其内容的具体说明的综合提示。例如：\"总结\n   https://example.com/article 并从 https://another.com/data 中提取要点\"。提示必须包含至少一个以\n   http:// 或 https:// 开头的 URL。\n\n\n如何在 VeCLI 中使用 web_fetch#\n\n要在 VeCLI 中使用 web_fetch，请提供一个包含 URL 的自然语言提示。该工具在获取任何 URL 之前会要求确认。确认后，该工具将通过\nGemini API 的 urlContext 处理 URL。\n\n如果 Gemini API 无法访问\nURL，该工具将回退到直接从本地机器获取内容。该工具将格式化响应，包括来源归属和引用（如果可能）。然后该工具将向用户提供响应。\n\n用法:\n\n\n\n\nweb_fetch 示例#\n\n总结单篇文章:\n\n\n\n比较两篇文章:\n\n\n\n\n重要提示#\n\n * URL 处理: web_fetch 依赖于 Gemini API 访问和处理给定 URL 的能力。\n * 输出质量: 输出的质量将取决于提示中指令的清晰度。","routePath":"/zh/tools/web-fetch","lang":"","toc":[{"text":"描述","id":"描述","depth":2,"charIndex":51},{"text":"参数","id":"参数","depth":3,"charIndex":162},{"text":"如何在 VeCLI 中使用 `web_fetch`","id":"如何在-vecli-中使用-web_fetch","depth":2,"charIndex":-1},{"text":"`web_fetch` 示例","id":"web_fetch-示例","depth":2,"charIndex":-1},{"text":"重要提示","id":"重要提示","depth":2,"charIndex":629}],"domain":"","frontmatter":{},"version":""},{"id":111,"title":"故障排除指南","content":"#\n\n本指南提供了常见问题的解决方案和调试技巧，包括以下主题：\n\n * 身份验证或登录错误\n * 常见问题解答 (FAQ)\n * 调试技巧\n * 与您遇到的问题相似的现有 GitHub Issues 或创建新 Issues\n\n\n身份验证或登录错误#\n\n * 错误: UNABLE_TO_GET_ISSUER_CERT_LOCALLY 或 unable to get local issuer\n   certificate\n   * 原因: 您可能在 corporate 网络中，该网络的防火墙会拦截和检查 SSL/TLS 流量。这通常需要 Node.js 信任自定义根 CA 证书。\n   * 解决方案: 将 NODE_EXTRA_CA_CERTS 环境变量设置为您的 corporate 根 CA 证书文件的绝对路径。\n     * 示例: export NODE_EXTRA_CA_CERTS=/path/to/your/corporate-ca.crt\n\n\n常见问题解答 (FAQ)#\n\n * 问: 如何将 VeCLI 更新到最新版本？\n   \n   * 答: 如果您是通过 npm 全局安装的，请使用命令 npm install -g @vecli/vecli@latest\n     更新。如果您是从源代码编译的，请从存储库中拉取最新更改，然后使用命令 npm run build 重新构建。\n\n * 问: VeCLI 的配置或设置文件存储在哪里？\n   \n   * 答: VeCLI 配置存储在两个 settings.json 文件中：\n     \n     1. 在您的主目录中: ~/.ve/settings.json。\n     2. 在您的项目根目录中: ./.ve/settings.json。\n     \n     有关更多详细信息，请参阅 VeCLI 配置。\n\n * 问: 为什么在我的统计输出中看不到缓存的令牌计数？\n   \n   * 答: 只有在使用缓存令牌时才会显示缓存的令牌信息。此功能适用于 API 密钥用户（vecli API 密钥或火山引擎 Vertex AI），但不适用于\n     OAuth 用户（例如火山引擎个人/企业帐户，如火山引擎 Gmail 或火山引擎工作区）。这是因为火山引擎代码助手 API\n     不支持缓存内容创建。您仍然可以使用 VeCLI 中的 /stats 命令查看总令牌使用量。\n\n\n常见错误消息和解决方案#\n\n * 错误: 启动 MCP 服务器时出现 EADDRINUSE (地址已在使用中)。\n   \n   * 原因: 另一个进程已经在使用 MCP 服务器试图绑定的端口。\n   * 解决方案: 要么停止使用该端口的其他进程，要么配置 MCP 服务器使用不同的端口。\n\n * 错误: 找不到命令 (尝试使用 vecli 运行 VeCLI 时)。\n   \n   * 原因: VeCLI 未正确安装，或者它不在您的系统 PATH 中。\n   * 解决方案: 更新方法取决于您安装 VeCLI 的方式：\n     * 如果您全局安装了 vecli，请检查您的 npm 全局二进制目录是否在您的 PATH 中。您可以使用命令 npm install -g\n       @vecli/vecli@latest 更新 VeCLI。\n     * 如果您是从源代码运行 vecli，请确保您使用正确的命令来调用它（例如，node packages/cli/dist/index.js\n       ...）。要更新 VeCLI，请从存储库中拉取最新更改，然后使用命令 npm run build 重新构建。\n\n * 错误: MODULE_NOT_FOUND 或导入错误。\n   \n   * 原因: 依赖项未正确安装，或者项目尚未构建。\n   * 解决方案:\n     1. 运行 npm install 以确保所有依赖项都存在。\n     2. 运行 npm run build 以编译项目。\n     3. 使用 npm run start 验证构建是否成功完成。\n\n * 错误: \"Operation not permitted\"、\"Permission denied\" 或类似错误。\n   \n   * 原因: 启用沙盒时，VeCLI 可能会尝试执行受沙盒配置限制的操作，例如在项目目录或系统临时目录之外进行写入。\n   * 解决方案: 有关更多信息，包括如何自定义沙盒配置，请参阅 配置: 沙盒 文档。\n\n * VeCLI 在 \"CI\" 环境中未以交互模式运行\n   \n   * 问题: 如果设置了以 CI_ 开头的环境变量（例如 CI_TOKEN），VeCLI 将不会进入交互模式（不会出现提示）。这是因为底层 UI 框架使用的\n     is-in-ci 包检测到这些变量并假定为非交互式 CI 环境。\n   * 原因: is-in-ci 包会检查是否存在 CI、CONTINUOUS_INTEGRATION 或任何以 CI_\n     为前缀的环境变量。当发现其中任何一个时，它会发出信号表明环境是非交互式的，这会阻止 VeCLI 以交互模式启动。\n   * 解决方案: 如果不需要 CLI 功能的 CI_ 前缀变量，您可以为命令暂时取消设置它。例如，env -u CI_TOKEN vecli\n\n * 从项目 .env 文件中 DEBUG 模式不工作\n   \n   * 问题: 在项目的 .env 文件中设置 DEBUG=true 不会为 vecli-cli 启用调试模式。\n   * 原因: DEBUG 和 DEBUG_MODE 变量会自动从项目 .env 文件中排除，以防止干扰 vecli-cli 行为。\n   * 解决方案: 请改用 .ve/.env 文件，或在 settings.json 中配置 advanced.excludedEnvVars\n     设置以排除更少的变量。\n\n\n退出代码#\n\nVeCLI 使用特定的退出代码来指示终止的原因。这对于脚本和自动化特别有用。\n\n退出代码   错误类型                       描述\n41     FatalAuthenticationError   身份验证过程中发生错误。\n42     FatalInputError            提供给 CLI 的输入无效或缺失。（仅限非交互模式）\n44     FatalSandboxError          沙盒环境（例如 Docker、Podman 或 Seatbelt）发生错误。\n52     FatalConfigError           配置文件 (settings.json) 无效或包含错误。\n53     FatalTurnLimitedError      会话的最大对话轮数已达到。（仅限非交互模式）\n\n\n调试技巧#\n\n * CLI 调试:\n   \n   * 在 CLI 命令中使用 --verbose 标志（如果可用）以获得更详细的输出。\n   * 检查 CLI 日志，通常位于用户特定的配置或缓存目录中。\n\n * 核心调试:\n   \n   * 检查服务器控制台输出中的错误消息或堆栈跟踪。\n   * 如果可配置，请增加日志详细程度。\n   * 如果您需要逐步执行服务器端代码，请使用 Node.js 调试工具（例如 node --inspect）。\n\n * 工具问题:\n   \n   * 如果某个特定工具失败，请尝试通过运行该工具执行的命令或操作的最简单版本来隔离问题。\n   * 对于 run_shell_command，请先检查该命令是否在您的 shell 中直接工作。\n   * 对于 文件系统工具，请验证路径是否正确并检查权限。\n\n * 预检检查:\n   \n   * 在提交代码之前，请始终运行 npm run preflight。这可以捕获许多与格式、代码规范和类型错误相关的常见问题。\n\n\n与您遇到的问题相似的现有 GitHub Issues 或创建新 Issues#\n\n如果您遇到此 故障排除指南 中未涵盖的问题，请考虑在 VeCLI GitHub 上的问题跟踪器\n中进行搜索。如果您找不到与您遇到的问题相似的问题，请考虑创建一个新的 GitHub Issue，并提供详细描述。也欢迎提交 Pull requests！","routePath":"/zh/troubleshooting","lang":"","toc":[{"text":"身份验证或登录错误","id":"身份验证或登录错误","depth":2,"charIndex":113},{"text":"常见问题解答 (FAQ)","id":"常见问题解答-faq","depth":2,"charIndex":431},{"text":"常见错误消息和解决方案","id":"常见错误消息和解决方案","depth":2,"charIndex":1027},{"text":"退出代码","id":"退出代码","depth":2,"charIndex":2486},{"text":"调试技巧","id":"调试技巧","depth":2,"charIndex":2874},{"text":"与您遇到的问题相似的现有 GitHub Issues 或创建新 Issues","id":"与您遇到的问题相似的现有-github-issues-或创建新-issues","depth":2,"charIndex":3324}],"domain":"","frontmatter":{},"version":""},{"id":112,"title":"忽略文件","content":"#\n\n本文档提供了 VeCLI 的 Gemini Ignore (.veignore) 功能的概述。\n\nVeCLI 包含自动忽略文件的功能，类似于 .gitignore (由 Git 使用) 和 .aiexclude (由 Ve Code Assist\n使用)。将路径添加到您的 .veignore 文件中将从支持此功能的工具中排除它们，但它们对其他服务 (如 Git) 仍然可见。\n\n\n工作原理#\n\n当您将路径添加到 .veignore 文件中时，支持此文件的工具将从其操作中排除匹配的文件和目录。例如，当您使用 read_many_files\n命令时，.veignore 文件中的任何路径都将被自动排除。\n\n在大多数情况下，.veignore 遵循 .gitignore 文件的约定：\n\n * 空行和以 # 开头的行将被忽略。\n * 支持标准的 glob 模式 (如 *、? 和 [])。\n * 在末尾加上 / 将只匹配目录。\n * 在开头加上 / 会将路径相对于 .veignore 文件进行锚定。\n * ! 用于否定一个模式。\n\n您可以随时更新 .veignore 文件。要应用更改，您必须重新启动 VeCLI 会话。\n\n\n如何使用 .veignore#\n\n要启用 .veignore：\n\n 1. 在项目目录的根目录中创建一个名为 .veignore 的文件。\n\n要将文件或目录添加到 .veignore：\n\n 1. 打开您的 .veignore 文件。\n 2. 添加您想要忽略的路径或文件，例如：/archive/ 或 apikeys.txt。\n\n\n.veignore 示例#\n\n您可以使用 .veignore 来忽略目录和文件：\n\n\n\n您可以在 .veignore 文件中使用 * 通配符：\n\n\n\n最后，您可以使用 ! 将文件和目录从排除中恢复：\n\n\n\n要从 .veignore 文件中删除路径，请删除相关行。","routePath":"/zh/ve-ignore","lang":"","toc":[{"text":"工作原理","id":"工作原理","depth":2,"charIndex":193},{"text":"如何使用 `.veignore`","id":"如何使用-veignore","depth":2,"charIndex":-1},{"text":"`.veignore` 示例","id":"veignore-示例","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":""}]