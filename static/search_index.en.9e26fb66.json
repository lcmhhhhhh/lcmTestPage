[{"id":0,"title":"Uninstalling the CLI","content":"#\n\nYour uninstall method depends on how you ran the CLI. Follow the instructions\nfor either npx or a global npm installation.\n\n\nMethod 1: Using npx#\n\nnpx runs packages from a temporary cache without a permanent installation. To\n\"uninstall\" the CLI, you must clear this cache, which will remove gemini-cli and\nany other packages previously executed with npx.\n\nThe npx cache is a directory named _npx inside your main npm cache folder. You\ncan find your npm cache path by running npm config get cache.\n\nFor macOS / Linux\n\n\n\nFor Windows\n\nCommand Prompt\n\n\n\nPowerShell\n\n\n\n\nMethod 2: Using npm (Global Install)#\n\nIf you installed the CLI globally (e.g., npm install -g @vecode-cli/vecode-cli),\nuse the npm uninstall command with the -g flag to remove it.\n\n\n\nThis command completely removes the package from your system.","routePath":"/en/Uninstall","lang":"en","toc":[{"text":"Method 1: Using npx","id":"method-1-using-npx","depth":2,"charIndex":127},{"text":"Method 2: Using npm (Global Install)","id":"method-2-using-npm-global-install","depth":2,"charIndex":567}],"domain":"","frontmatter":{},"version":""},{"id":1,"title":"Checkpointing","content":"#\n\nThe VeCLI includes a Checkpointing feature that automatically saves a snapshot\nof your project's state before any file modifications are made by AI-powered\ntools. This allows you to safely experiment with and apply code changes, knowing\nyou can instantly revert back to the state before the tool was run.\n\n\nHow It Works#\n\nWhen you approve a tool that modifies the file system (like write_file or\nreplace), the CLI automatically creates a \"checkpoint.\" This checkpoint\nincludes:\n\n 1. A Git Snapshot: A commit is made in a special, shadow Git repository located\n    in your home directory (~/.ve/history/<project_hash>). This snapshot\n    captures the complete state of your project files at that moment. It does\n    not interfere with your own project's Git repository.\n 2. Conversation History: The entire conversation you've had with the agent up\n    to that point is saved.\n 3. The Tool Call: The specific tool call that was about to be executed is also\n    stored.\n\nIf you want to undo the change or simply go back, you can use the /restore\ncommand. Restoring a checkpoint will:\n\n * Revert all files in your project to the state captured in the snapshot.\n * Restore the conversation history in the CLI.\n * Re-propose the original tool call, allowing you to run it again, modify it,\n   or simply ignore it.\n\nAll checkpoint data, including the Git snapshot and conversation history, is\nstored locally on your machine. The Git snapshot is stored in the shadow\nrepository while the conversation history and tool calls are saved in a JSON\nfile in your project's temporary directory, typically located at\n~/.ve/tmp/<project_hash>/checkpoints.\n\n\nEnabling the Feature#\n\nThe Checkpointing feature is disabled by default. To enable it, you can either\nuse a command-line flag or edit your settings.json file.\n\n\nUsing the Command-Line Flag#\n\nYou can enable checkpointing for the current session by using the\n--checkpointing flag when starting the VeCLI:\n\n\n\n\nUsing the settings.json File#\n\nTo enable checkpointing by default for all sessions, you need to edit your\nsettings.json file.\n\nAdd the following key to your settings.json:\n\n\n\n\nUsing the /restore Command#\n\nOnce enabled, checkpoints are created automatically. To manage them, you use the\n/restore command.\n\n\nList Available Checkpoints#\n\nTo see a list of all saved checkpoints for the current project, simply run:\n\n\n\nThe CLI will display a list of available checkpoint files. These file names are\ntypically composed of a timestamp, the name of the file being modified, and the\nname of the tool that was about to be run (e.g.,\n2025-06-22T10-00-00_000Z-my-file.txt-write_file).\n\n\nRestore a Specific Checkpoint#\n\nTo restore your project to a specific checkpoint, use the checkpoint file from\nthe list:\n\n\n\nFor example:\n\n\n\nAfter running the command, your files and conversation will be immediately\nrestored to the state they were in when the checkpoint was created, and the\noriginal tool prompt will reappear.","routePath":"/en/checkpointing","lang":"en","toc":[{"text":"How It Works","id":"how-it-works","depth":2,"charIndex":309},{"text":"Enabling the Feature","id":"enabling-the-feature","depth":2,"charIndex":1644},{"text":"Using the Command-Line Flag","id":"using-the-command-line-flag","depth":3,"charIndex":1805},{"text":"Using the `settings.json` File","id":"using-the-settingsjson-file","depth":3,"charIndex":-1},{"text":"Using the `/restore` Command","id":"using-the-restore-command","depth":2,"charIndex":-1},{"text":"List Available Checkpoints","id":"list-available-checkpoints","depth":3,"charIndex":2257},{"text":"Restore a Specific Checkpoint","id":"restore-a-specific-checkpoint","depth":3,"charIndex":2626}],"domain":"","frontmatter":{},"version":""},{"id":2,"title":"Authentication Setup","content":"#\n\nThe VeCLI requires you to authenticate with Google's AI services. On initial\nstartup you'll need to configure one of the following authentication methods:\n\n 1. Login with Volcengine AK/SK:\n    \n    * Use this option to authenticate through Volcengine's Access Key (AK) and\n      Secret Key (SK).\n    * This authentication method is suitable for scenarios where VeCLI is used\n      in server environments or automated scripts.\n    * Authentication information will be securely cached locally, eliminating\n      the need for repeated configuration in subsequent uses.\n    \n    Obtaining AK/SK:\n    \n    1. Log in to Volcengine Console\n    2. Navigate to \"Access Control\" → \"Access Keys\" page\n    3. Click \"Create Access Key\" button\n    4. The system will generate a pair of AK/SK, please save them securely\n    \n    Configuration Methods:\n    \n    Method 1: Environment Variable Configuration\n    \n    \n    \n    Method 2: Configuration File Create a configuration file\n    ~/.vecli/config.json in the user's home directory:\n    \n    \n    \n    Method 3: Command Line Parameters\n    \n    \n    \n    Verifying Configuration:\n    \n    \n\n 2. Login with Volcengine API key:\n    \n    * Obtain your API key from Volcengine: https://www.volcengine.com/\n    * Set the VECLI_API_KEY environment variable. In the following methods,\n      replace YOUR_VECLI_API_KEY with the API key you obtained from Volcengine\n      Ark Platform:\n      \n      * You can temporarily set the environment variable in your current shell\n        session using the following command:\n        \n        \n      \n      * For repeated use, you can add the environment variable to your .env\n        file.\n      \n      * Alternatively you can export the API key from your shell's configuration\n        file (like ~/.bashrc, ~/.zshrc, or ~/.profile). For example, the\n        following command adds the environment variable to a ~/.bashrc file:\n        \n        \n        \n        :warning: Be advised that when you export your API key inside your shell\n        configuration file, any other process executed from the shell can read\n        it.","routePath":"/en/cli/authentication","lang":"en","toc":[],"domain":"","frontmatter":{},"version":""},{"id":3,"title":"CLI Commands","content":"#\n\nVeCLI supports several built-in commands to help you manage your session,\ncustomize the interface, and control its behavior. These commands are prefixed\nwith a forward slash (/), an at symbol (@), or an exclamation mark (!).\n\n\nSlash commands (/)#\n\nSlash commands provide meta-level control over the CLI itself.\n\n\nBuilt-in Commands#\n\n * /bug\n   \n   * Description: File an issue about VeCLI. By default, the issue is filed\n     within the GitHub repository for VeCLI. The string you enter after /bug\n     will become the headline for the bug being filed. The default /bug behavior\n     can be modified using the advanced.bugCommand setting in your\n     .ve/settings.json files.\n\n * /chat\n   \n   * Description: Save and resume conversation history for branching\n     conversation state interactively, or resuming a previous state from a later\n     session.\n   * Sub-commands:\n     * save\n       * Description: Saves the current conversation history. You must add a\n         <tag> for identifying the conversation state.\n       * Usage: /chat save <tag>\n       * Details on Checkpoint Location: The default locations for saved chat\n         checkpoints are:\n         * Linux/macOS: ~/.ve/tmp/<project_hash>/\n         * Windows: C:\\Users\\<YourUsername>\\.ve\\tmp\\<project_hash>\\\n         * When you run /chat list, the CLI only scans these specific\n           directories to find available checkpoints.\n         * Note: These checkpoints are for manually saving and resuming\n           conversation states. For automatic checkpoints created before file\n           modifications, see the Checkpointing documentation.\n     * resume\n       * Description: Resumes a conversation from a previous save.\n       * Usage: /chat resume <tag>\n     * list\n       * Description: Lists available tags for chat state resumption.\n     * delete\n       * Description: Deletes a saved conversation checkpoint.\n       * Usage: /chat delete <tag>\n\n * /clear\n   \n   * Description: Clear the terminal screen, including the visible session\n     history and scrollback within the CLI. The underlying session data (for\n     history recall) might be preserved depending on the exact implementation,\n     but the visual display is cleared.\n   * Keyboard shortcut: Press Ctrl+L at any time to perform a clear action.\n\n * /compress\n   \n   * Description: Replace the entire chat context with a summary. This saves on\n     tokens used for future tasks while retaining a high level summary of what\n     has happened.\n\n * /copy\n   \n   * Description: Copies the last output produced by VeCLI to your clipboard,\n     for easy sharing or reuse.\n   * Note: This command requires platform-specific clipboard tools to be\n     installed.\n     * On Linux, it requires xclip or xsel. You can typically install them using\n       your system's package manager.\n     * On macOS, it requires pbcopy, and on Windows, it requires clip. These\n       tools are typically pre-installed on their respective systems.\n\n * /directory (or /dir)\n   \n   * Description: Manage workspace directories for multi-directory support.\n   * Sub-commands:\n     * add:\n       * Description: Add a directory to the workspace. The path can be absolute\n         or relative to the current working directory. Moreover, the reference\n         from home directory is supported as well.\n       * Usage: /directory add <path1>,<path2>\n       * Note: Disabled in restrictive sandbox profiles. If you're using that,\n         use --include-directories when starting the session instead.\n     * show:\n       * Description: Display all directories added by /directory add and\n         --include-directories.\n       * Usage: /directory show\n\n * /editor\n   \n   * Description: Open a dialog for selecting supported editors.\n\n * /extensions\n   \n   * Description: Lists all active extensions in the current VeCLI session. See\n     VeCLI Extensions.\n\n * /help (or /?)\n   \n   * Description: Display help information about VeCLI, including available\n     commands and their usage.\n\n * /mcp\n   \n   * Description: List configured Model Context Protocol (MCP) servers, their\n     connection status, server details, and available tools.\n   * Sub-commands:\n     * desc or descriptions:\n       * Description: Show detailed descriptions for MCP servers and tools.\n     * nodesc or nodescriptions:\n       * Description: Hide tool descriptions, showing only the tool names.\n     * schema:\n       * Description: Show the full JSON schema for the tool's configured\n         parameters.\n   * Keyboard Shortcut: Press Ctrl+T at any time to toggle between showing and\n     hiding tool descriptions.\n\n * /memory\n   \n   * Description: Manage the AI's instructional context (hierarchical memory\n     loaded from VE.md files).\n   * Sub-commands:\n     * add:\n       * Description: Adds the following text to the AI's memory. Usage: /memory\n         add <text to remember>\n     * show:\n       * Description: Display the full, concatenated content of the current\n         hierarchical memory that has been loaded from all VE.md files. This\n         lets you inspect the instructional context being provided to the\n         Volcano Engine model.\n     * refresh:\n       * Description: Reload the hierarchical instructional memory from all\n         VE.md files found in the configured locations (global,\n         project/ancestors, and sub-directories). This command updates the model\n         with the latest VE.md content.\n     * Note: For more details on how VE.md files contribute to hierarchical\n       memory, see the CLI Configuration documentation.\n\n * /restore\n   \n   * Description: Restores the project files to the state they were in just\n     before a tool was executed. This is particularly useful for undoing file\n     edits made by a tool. If run without a tool call ID, it will list available\n     checkpoints to restore from.\n   * Usage: /restore [tool_call_id]\n   * Note: Only available if the CLI is invoked with the --checkpointing option\n     or configured via settings. See Checkpointing documentation for more\n     details.\n\n * /settings\n   \n   * Description: Open the settings editor to view and modify VeCLI settings.\n   * Details: This command provides a user-friendly interface for changing\n     settings that control the behavior and appearance of VeCLI. It is\n     equivalent to manually editing the .ve/settings.json file, but with\n     validation and guidance to prevent errors.\n   * Usage: Simply run /settings and the editor will open. You can then browse\n     or search for specific settings, view their current values, and modify them\n     as desired. Changes to some settings are applied immediately, while others\n     require a restart.\n\n * /stats\n   \n   * Description: Display detailed statistics for the current VeCLI session,\n     including token usage, cached token savings (when available), and session\n     duration. Note: Cached token information is only displayed when cached\n     tokens are being used, which occurs with API key authentication but not\n     with OAuth authentication at this time.\n\n * /theme\n   \n   * Description: Open a dialog that lets you change the visual theme of VeCLI.\n\n * /auth\n   \n   * Description: Open a dialog that lets you change the authentication method.\n\n * /about\n   \n   * Description: Show version info. Please share this information when filing\n     issues.\n\n * /tools\n   \n   * Description: Display a list of tools that are currently available within\n     VeCLI.\n   * Usage: /tools [desc]\n   * Sub-commands:\n     * desc or descriptions:\n       * Description: Show detailed descriptions of each tool, including each\n         tool's name with its full description as provided to the model.\n     * nodesc or nodescriptions:\n       * Description: Hide tool descriptions, showing only the tool names.\n\n * /privacy\n   \n   * Description: Display the Privacy Notice and allow users to select whether\n     they consent to the collection of their data for service improvement\n     purposes.\n\n * /quit (or /exit)\n   \n   * Description: Exit VeCLI.\n\n * /vim\n   \n   * Description: Toggle vim mode on or off. When vim mode is enabled, the input\n     area supports vim-style navigation and editing commands in both NORMAL and\n     INSERT modes.\n   * Features:\n     * NORMAL mode: Navigate with h, j, k, l; jump by words with w, b, e; go to\n       line start/end with 0, $, ^; go to specific lines with G (or gg for first\n       line)\n     * INSERT mode: Standard text input with escape to return to NORMAL mode\n     * Editing commands: Delete with x, change with c, insert with i, a, o, O;\n       complex operations like dd, cc, dw, cw\n     * Count support: Prefix commands with numbers (e.g., 3h, 5w, 10G)\n     * Repeat last command: Use . to repeat the last editing operation\n     * Persistent setting: Vim mode preference is saved to ~/.ve/settings.json\n       and restored between sessions\n   * Status indicator: When enabled, shows [NORMAL] or [INSERT] in the footer\n\n * /init\n   \n   * Description: To help users easily create a VE.md file, this command\n     analyzes the current directory and generates a tailored context file,\n     making it simpler for them to provide project-specific instructions to the\n     Volcano Engine agent.\n\n\nCustom Commands#\n\nFor a quick start, see the example below.\n\nCustom commands allow you to save and reuse your favorite or most frequently\nused prompts as personal shortcuts within VeCLI. You can create commands that\nare specific to a single project or commands that are available globally across\nall your projects, streamlining your workflow and ensuring consistency.\n\nFile Locations & Precedence#\n\nVeCLI discovers commands from two locations, loaded in a specific order:\n\n 1. User Commands (Global): Located in ~/.ve/commands/. These commands are\n    available in any project you are working on.\n 2. Project Commands (Local): Located in ~/.ve/commands/. These commands are\n    specific to the current project and can be checked into version control to\n    be shared with your team.\n\nIf a command in the project directory has the same name as a command in the user\ndirectory, the project command will always be used. This allows projects to\noverride global commands with project-specific versions.\n\nNaming and Namespacing#\n\nThe name of a command is determined by its file path relative to its commands\ndirectory. Subdirectories are used to create namespaced commands, with the path\nseparator (/ or \\) being converted to a colon (:).\n\n * A file at ~/.ve/commands/test.toml becomes the command /test.\n * A file at <project>/.ve/commands/git/commit.toml becomes the namespaced\n   command /git:commit.\n\nTOML File Format (v1)#\n\nYour command definition files must be written in the TOML format and use the\n.toml file extension.\n\nRequired Fields#\n\n * prompt (String): The prompt that will be sent to the Volcano Engine model\n   when the command is executed. This can be a single-line or multi-line string.\n\nOptional Fields#\n\n * description (String): A brief, one-line description of what the command does.\n   This text will be displayed next to your command in the /help menu. If you\n   omit this field, a generic description will be generated from the filename.\n\nHandling Arguments#\n\nCustom commands support two powerful methods for handling arguments. The CLI\nautomatically chooses the correct method based on the content of your command's\nprompt.\n\n1. Context-Aware Injection with {{args}}#\n\nIf your prompt contains the special placeholder {{args}}, the CLI will replace\nthat placeholder with the text the user typed after the command name.\n\nThe behavior of this injection depends on where it is used:\n\nA. Raw Injection (Outside Shell Commands)\n\nWhen used in the main body of the prompt, the arguments are injected exactly as\nthe user typed them.\n\nExample (git/fix.toml):\n\n\n\nThe model receives: Please provide a code fix for the issue described here:\n\"Button is misaligned\".\n\nB. Using Arguments in Shell Commands (Inside !{...} Blocks)\n\nWhen you use {{args}} inside a shell injection block (!{...}), the arguments are\nautomatically shell-escaped before replacement. This allows you to safely pass\narguments to shell commands, ensuring the resulting command is syntactically\ncorrect and secure while preventing command injection vulnerabilities.\n\nExample (/grep-code.toml):\n\n\n\nWhen you run /grep-code It's complicated:\n\n 1. The CLI sees {{args}} used both outside and inside !{...}.\n 2. Outside: The first {{args}} is replaced raw with It's complicated.\n 3. Inside: The second {{args}} is replaced with the escaped version (e.g., on\n    Linux: \"It's complicated\").\n 4. The command executed is grep -r \"It's complicated\" ..\n 5. The CLI prompts you to confirm this exact, secure command before execution.\n 6. The final prompt is sent.\n\n2. Default Argument Handling#\n\nIf your prompt does not contain the special placeholder {{args}}, the CLI uses a\ndefault behavior for handling arguments.\n\nIf you provide arguments to the command (e.g., /mycommand arg1), the CLI will\nappend the full command you typed to the end of the prompt, separated by two\nnewlines. This allows the model to see both the original instructions and the\nspecific arguments you just provided.\n\nIf you do not provide any arguments (e.g., /mycommand), the prompt is sent to\nthe model exactly as it is, with nothing appended.\n\nExample (changelog.toml):\n\nThis example shows how to create a robust command by defining a role for the\nmodel, explaining where to find the user's input, and specifying the expected\nformat and behavior.\n\n\n\nWhen you run /changelog 1.2.0 added \"New feature\", the final text sent to the\nmodel will be the original prompt followed by two newlines and the command you\ntyped.\n\n3. Executing Shell Commands with !{...}#\n\nYou can make your commands dynamic by executing shell commands directly within\nyour prompt and injecting their output. This is ideal for gathering context from\nyour local environment, like reading file content or checking the status of Git.\n\nWhen a custom command attempts to execute a shell command, VeCLI will now prompt\nyou for confirmation before proceeding. This is a security measure to ensure\nthat only intended commands can be run.\n\nHow It Works:\n\n 1. Inject Commands: Use the !{...} syntax.\n 2. Argument Substitution: If {{args}} is present inside the block, it is\n    automatically shell-escaped (see Context-Aware Injection above).\n 3. Robust Parsing: The parser correctly handles complex shell commands that\n    include nested braces, such as JSON payloads. Note: The content inside\n    !{...} must have balanced braces ({ and }). If you need to execute a command\n    containing unbalanced braces, consider wrapping it in an external script\n    file and calling the script within the !{...} block.\n 4. Security Check and Confirmation: The CLI performs a security check on the\n    final, resolved command (after arguments are escaped and substituted). A\n    dialog will appear showing the exact command(s) to be executed.\n 5. Execution and Error Reporting: The command is executed. If the command\n    fails, the output injected into the prompt will include the error messages\n    (stderr) followed by a status line, e.g., [Shell command exited with code\n    1]. This helps the model understand the context of the failure.\n\nExample (git/commit.toml):\n\nThis command gets the staged git diff and uses it to ask the model to write a\ncommit message.\n\n\n\nWhen you run /git:commit, the CLI first executes git diff --staged, then\nreplaces !{git diff --staged} with the output of that command before sending the\nfinal, complete prompt to the model.\n\n4. Injecting File Content with @{...}#\n\nYou can directly embed the content of a file or a directory listing into your\nprompt using the @{...} syntax. This is useful for creating commands that\noperate on specific files.\n\nHow It Works:\n\n * File Injection: @{path/to/file.txt} is replaced by the content of file.txt.\n * Multimodal Support: If the path points to a supported image (e.g., PNG,\n   JPEG), PDF, audio, or video file, it will be correctly encoded and injected\n   as multimodal input. Other binary files are handled gracefully and skipped.\n * Directory Listing: @{path/to/dir} is traversed and each file present within\n   the directory and all subdirectories are inserted into the prompt. This\n   respects .gitignore and .veignore if enabled.\n * Workspace-Aware: The command searches for the path in the current directory\n   and any other workspace directories. Absolute paths are allowed if they are\n   within the workspace.\n * Processing Order: File content injection with @{...} is processed before\n   shell commands (!{...}) and argument substitution ({{args}}).\n * Parsing: The parser requires the content inside @{...} (the path) to have\n   balanced braces ({ and }).\n\nExample (review.toml):\n\nThis command injects the content of a fixed best practices file\n(docs/best-practices.md) and uses the user's arguments to provide context for\nthe review.\n\n\n\nWhen you run /review FileCommandLoader.ts, the @{docs/best-practices.md}\nplaceholder is replaced by the content of that file, and {{args}} is replaced by\nthe text you provided, before the final prompt is sent to the model.\n\n--------------------------------------------------------------------------------\n\nExample: A \"Pure Function\" Refactoring Command#\n\nLet's create a global command that asks the model to refactor a piece of code.\n\n1. Create the file and directories:\n\nFirst, ensure the user commands directory exists, then create a refactor\nsubdirectory for organization and the final TOML file.\n\n\n\n2. Add the content to the file:\n\nOpen ~/.ve/commands/refactor/pure.toml in your editor and add the following\ncontent. We are including the optional description for best practice.\n\n\n\n3. Run the Command:\n\nThat's it! You can now run your command in the CLI. First, you might add a file\nto the context, and then invoke your command:\n\n\n\nVeCLI will then execute the multi-line prompt defined in your TOML file.\n\n\nAt commands (@)#\n\nAt commands are used to include the content of files or directories as part of\nyour prompt to Volcano Engine. These commands include git-aware filtering.\n\n * @<path_to_file_or_directory>\n   \n   * Description: Inject the content of the specified file or files into your\n     current prompt. This is useful for asking questions about specific code,\n     text, or collections of files.\n   * Examples:\n     * @path/to/your/file.txt Explain this text.\n     * @src/my_project/ Summarize the code in this directory.\n     * What is this file about? @README.md\n   * Details:\n     * If a path to a single file is provided, the content of that file is read.\n     * If a path to a directory is provided, the command attempts to read the\n       content of files within that directory and any subdirectories.\n     * Spaces in paths should be escaped with a backslash (e.g., @My\\\n       Documents/file.txt).\n     * The command uses the read_many_files tool internally. The content is\n       fetched and then inserted into your query before being sent to the model.\n     * Git-aware filtering: By default, git-ignored files (like node_modules/,\n       dist/, .env, .git/) are excluded. This behavior can be changed via the\n       context.fileFiltering settings.\n     * File types: The command is intended for text-based files. While it might\n       attempt to read any file, binary files or very large files might be\n       skipped or truncated by the underlying read_many_files tool to ensure\n       performance and relevance. The tool indicates if files were skipped.\n   * Output: The CLI will show a tool call message indicating that\n     read_many_files was used, along with a message detailing the status and the\n     path(s) that were processed.\n\n * @ (Lone at symbol)\n   \n   * Description: If you type a lone @ symbol without a path, the query is\n     passed as-is to the Volcano Engine model. This might be useful if you are\n     specifically talking about the @ symbol in your prompt.\n\n\nError handling for @ commands#\n\n * If the path specified after @ is not found or is invalid, an error message\n   will be displayed, and the query might not be sent to the model, or it will\n   be sent without the file content.\n * If the read_many_files tool encounters an error (e.g., permission issues),\n   this will also be reported.\n\n\nShell mode & passthrough commands (!)#\n\nThe ! prefix lets you interact with your system's shell directly from within\nVeCLI.\n\n * !<shell_command>\n   \n   * Description: Execute the given <shell_command> using bash on Linux/macOS or\n     cmd.exe on Windows. Any output or errors from the command are displayed in\n     the terminal.\n   * Examples:\n     * !ls -la (executes ls -la and returns to VeCLI)\n     * !git status (executes git status and returns to VeCLI)\n\n * ! (Toggle shell mode)\n   \n   * Description: Typing ! on its own toggles shell mode.\n     * Entering shell mode:\n       * When active, shell mode uses a different coloring and a \"Shell Mode\n         Indicator\".\n       * While in shell mode, text you type is interpreted directly as a shell\n         command.\n     * Exiting shell mode:\n       * When exited, the UI reverts to its standard appearance and normal VeCLI\n         behavior resumes.\n\n * Caution for all ! usage: Commands you execute in shell mode have the same\n   permissions and impact as if you ran them directly in your terminal.\n\n * Environment Variable: When a command is executed via ! or in shell mode, the\n   GEMINI_CLI=1 environment variable is set in the subprocess's environment.\n   This allows scripts or tools to detect if they are being run from within the\n   VeCLI.","routePath":"/en/cli/commands","lang":"en","toc":[{"text":"Slash commands (`/`)","id":"slash-commands-","depth":2,"charIndex":-1},{"text":"Built-in Commands","id":"built-in-commands","depth":3,"charIndex":315},{"text":"Custom Commands","id":"custom-commands","depth":3,"charIndex":9188},{"text":"File Locations & Precedence","id":"file-locations--precedence","depth":4,"charIndex":9557},{"text":"Naming and Namespacing","id":"naming-and-namespacing","depth":4,"charIndex":10187},{"text":"TOML File Format (v1)","id":"toml-file-format-v1","depth":4,"charIndex":10587},{"text":"Handling Arguments","id":"handling-arguments","depth":4,"charIndex":11145},{"text":"Example: A \"Pure Function\" Refactoring Command","id":"example-a-pure-function-refactoring-command","depth":4,"charIndex":17205},{"text":"At commands (`@`)","id":"at-commands-","depth":2,"charIndex":-1},{"text":"Error handling for `@` commands","id":"error-handling-for--commands","depth":3,"charIndex":-1},{"text":"Shell mode & passthrough commands (`!`)","id":"shell-mode--passthrough-commands-","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":4,"title":"VeCLI Configuration","content":"#\n\nNote on Deprecated Configuration Format\n\nThis document describes the legacy v1 format for the settings.json file. This\nformat is now deprecated.\n\n * The new format will be supported in the stable release starting [09/10/25].\n * Automatic migration from the old format to the new format will begin on\n   [09/17/25].\n\nFor details on the new, recommended format, please see the current Configuration\ndocumentation.\n\nVeCLI offers several ways to configure its behavior, including environment\nvariables, command-line arguments, and settings files. This document outlines\nthe different configuration methods and available settings.\n\n\nConfiguration layers#\n\nConfiguration is applied in the following order of precedence (lower numbers are\noverridden by higher numbers):\n\n 1. Default values: Hardcoded defaults within the application.\n 2. System defaults file: System-wide default settings that can be overridden by\n    other settings files.\n 3. User settings file: Global settings for the current user.\n 4. Project settings file: Project-specific settings.\n 5. System settings file: System-wide settings that override all other settings\n    files.\n 6. Environment variables: System-wide or session-specific variables,\n    potentially loaded from .env files.\n 7. Command-line arguments: Values passed when launching the CLI.\n\n\nSettings files#\n\nVeCLI uses JSON settings files for persistent configuration. There are four\nlocations for these files:\n\n * System defaults file:\n   * Location: /etc/gemini-cli/system-defaults.json (Linux),\n     C:\\ProgramData\\gemini-cli\\system-defaults.json (Windows) or\n     /Library/Application Support/GeminiCli/system-defaults.json (macOS). The\n     path can be overridden using the GEMINI_CLI_SYSTEM_DEFAULTS_PATH\n     environment variable.\n   * Scope: Provides a base layer of system-wide default settings. These\n     settings have the lowest precedence and are intended to be overridden by\n     user, project, or system override settings.\n * User settings file:\n   * Location: ~/.ve/settings.json (where ~ is your home directory).\n   * Scope: Applies to all VeCLI sessions for the current user. User settings\n     override system defaults.\n * Project settings file:\n   * Location: .ve/settings.json within your project's root directory.\n   * Scope: Applies only when running VeCLI from that specific project. Project\n     settings override user settings and system defaults.\n * System settings file:\n   * Location: /etc/gemini-cli/settings.json (Linux),\n     C:\\ProgramData\\gemini-cli\\settings.json (Windows) or /Library/Application\n     Support/GeminiCli/settings.json (macOS). The path can be overridden using\n     the GEMINI_CLI_SYSTEM_SETTINGS_PATH environment variable.\n   * Scope: Applies to all VeCLI sessions on the system, for all users. System\n     settings act as overrides, taking precedence over all other settings files.\n     May be useful for system administrators at enterprises to have controls\n     over users' VeCLI setups.\n\nNote on environment variables in settings: String values within your\nsettings.json files can reference environment variables using either $VAR_NAME\nor ${VAR_NAME} syntax. These variables will be automatically resolved when the\nsettings are loaded. For example, if you have an environment variable\nMY_API_TOKEN, you could use it in settings.json like this: \"apiKey\":\n\"$MY_API_TOKEN\".\n\n> Note for Enterprise Users: For guidance on deploying and managing VeCLI in a\n> corporate environment, please see the Enterprise Configuration documentation.\n\n\nThe .ve directory in your project#\n\nIn addition to a project settings file, a project's .ve directory can contain\nother project-specific files related to VeCLI's operation, such as:\n\n * Custom sandbox profiles (e.g., .ve/sandbox-macos-custom.sb,\n   .ve/sandbox.Dockerfile).\n\n\nAvailable settings in settings.json:#\n\n * contextFileName (string or array of strings):\n   \n   * Description: Specifies the filename for context files (e.g., VE.md,\n     AGENTS.md). Can be a single filename or a list of accepted filenames.\n   * Default: VE.md\n   * Example: \"contextFileName\": \"AGENTS.md\"\n\n * bugCommand (object):\n   \n   * Description: Overrides the default URL for the /bug command.\n   * Default: \"urlTemplate\":\n     \"https://github.com/google-gemini/gemini-cli/issues/new?template=bug_report\n     .yml&title={title}&info={info}\"\n   * Properties:\n     * urlTemplate (string): A URL that can contain {title} and {info}\n       placeholders.\n   * Example:\n     \n     \n\n * fileFiltering (object):\n   \n   * Description: Controls git-aware file filtering behavior for @ commands and\n     file discovery tools.\n   * Default: \"respectGitIgnore\": true, \"enableRecursiveFileSearch\": true\n   * Properties:\n     * respectGitIgnore (boolean): Whether to respect .gitignore patterns when\n       discovering files. When set to true, git-ignored files (like\n       node_modules/, dist/, .env) are automatically excluded from @ commands\n       and file listing operations.\n     * enableRecursiveFileSearch (boolean): Whether to enable searching\n       recursively for filenames under the current tree when completing @\n       prefixes in the prompt.\n     * disableFuzzySearch (boolean): When true, disables the fuzzy search\n       capabilities when searching for files, which can improve performance on\n       projects with a large number of files.\n   * Example:\n     \n     \n\n\nTroubleshooting File Search Performance#\n\nIf you are experiencing performance issues with file searching (e.g., with @\ncompletions), especially in projects with a very large number of files, here are\na few things you can try in order of recommendation:\n\n 1. Use .veignore: Create a .veignore file in your project root to exclude\n    directories that contain a large number of files that you don't need to\n    reference (e.g., build artifacts, logs, node_modules). Reducing the total\n    number of files crawled is the most effective way to improve performance.\n\n 2. Disable Fuzzy Search: If ignoring files is not enough, you can disable fuzzy\n    search by setting disableFuzzySearch to true in your settings.json file.\n    This will use a simpler, non-fuzzy matching algorithm, which can be faster.\n\n 3. Disable Recursive File Search: As a last resort, you can disable recursive\n    file search entirely by setting enableRecursiveFileSearch to false. This\n    will be the fastest option as it avoids a recursive crawl of your project.\n    However, it means you will need to type the full path to files when using @\n    completions.\n\n * coreTools (array of strings):\n   \n   * Description: Allows you to specify a list of core tool names that should be\n     made available to the model. This can be used to restrict the set of\n     built-in tools. See Built-in Tools for a list of core tools. You can also\n     specify command-specific restrictions for tools that support it, like the\n     ShellTool. For example, \"coreTools\": [\"ShellTool(ls -l)\"] will only allow\n     the ls -l command to be executed.\n   * Default: All tools available for use by the Gemini model.\n   * Example: \"coreTools\": [\"ReadFileTool\", \"GlobTool\", \"ShellTool(ls)\"].\n\n * allowedTools (array of strings):\n   \n   * Default: undefined\n   * Description: A list of tool names that will bypass the confirmation dialog.\n     This is useful for tools that you trust and use frequently. The match\n     semantics are the same as coreTools.\n   * Example: \"allowedTools\": [\"ShellTool(git status)\"].\n\n * excludeTools (array of strings):\n   \n   * Description: Allows you to specify a list of core tool names that should be\n     excluded from the model. A tool listed in both excludeTools and coreTools\n     is excluded. You can also specify command-specific restrictions for tools\n     that support it, like the ShellTool. For example, \"excludeTools\":\n     [\"ShellTool(rm -rf)\"] will block the rm -rf command.\n   * Default: No tools excluded.\n   * Example: \"excludeTools\": [\"run_shell_command\", \"findFiles\"].\n   * Security Note: Command-specific restrictions in excludeTools for\n     run_shell_command are based on simple string matching and can be easily\n     bypassed. This feature is not a security mechanism and should not be relied\n     upon to safely execute untrusted code. It is recommended to use coreTools\n     to explicitly select commands that can be executed.\n\n * allowMCPServers (array of strings):\n   \n   * Description: Allows you to specify a list of MCP server names that should\n     be made available to the model. This can be used to restrict the set of MCP\n     servers to connect to. Note that this will be ignored if\n     --allowed-mcp-server-names is set.\n   * Default: All MCP servers are available for use by the Gemini model.\n   * Example: \"allowMCPServers\": [\"myPythonServer\"].\n   * Security Note: This uses simple string matching on MCP server names, which\n     can be modified. If you're a system administrator looking to prevent users\n     from bypassing this, consider configuring the mcpServers at the system\n     settings level such that the user will not be able to configure any MCP\n     servers of their own. This should not be used as an airtight security\n     mechanism.\n\n * excludeMCPServers (array of strings):\n   \n   * Description: Allows you to specify a list of MCP server names that should\n     be excluded from the model. A server listed in both excludeMCPServers and\n     allowMCPServers is excluded. Note that this will be ignored if\n     --allowed-mcp-server-names is set.\n   * Default: No MCP servers excluded.\n   * Example: \"excludeMCPServers\": [\"myNodeServer\"].\n   * Security Note: This uses simple string matching on MCP server names, which\n     can be modified. If you're a system administrator looking to prevent users\n     from bypassing this, consider configuring the mcpServers at the system\n     settings level such that the user will not be able to configure any MCP\n     servers of their own. This should not be used as an airtight security\n     mechanism.\n\n * autoAccept (boolean):\n   \n   * Description: Controls whether the CLI automatically accepts and executes\n     tool calls that are considered safe (e.g., read-only operations) without\n     explicit user confirmation. If set to true, the CLI will bypass the\n     confirmation prompt for tools deemed safe.\n   * Default: false\n   * Example: \"autoAccept\": true\n\n * theme (string):\n   \n   * Description: Sets the visual theme for VeCLI.\n   * Default: \"Default\"\n   * Example: \"theme\": \"GitHub\"\n\n * vimMode (boolean):\n   \n   * Description: Enables or disables vim mode for input editing. When enabled,\n     the input area supports vim-style navigation and editing commands with\n     NORMAL and INSERT modes. The vim mode status is displayed in the footer and\n     persists between sessions.\n   * Default: false\n   * Example: \"vimMode\": true\n\n * sandbox (boolean or string):\n   \n   * Description: Controls whether and how to use sandboxing for tool execution.\n     If set to true, VeCLI uses a pre-built gemini-cli-sandbox Docker image. For\n     more information, see Sandboxing.\n   * Default: false\n   * Example: \"sandbox\": \"docker\"\n\n * toolDiscoveryCommand (string):\n   \n   * Description: Defines a custom shell command for discovering tools from your\n     project. The shell command must return on stdout a JSON array of function\n     declarations. Tool wrappers are optional.\n   * Default: Empty\n   * Example: \"toolDiscoveryCommand\": \"bin/get_tools\"\n\n * toolCallCommand (string):\n   \n   * Description: Defines a custom shell command for calling a specific tool\n     that was discovered using toolDiscoveryCommand. The shell command must meet\n     the following criteria:\n     * It must take function name (exactly as in function declaration) as first\n       command line argument.\n     * It must read function arguments as JSON on stdin, analogous to\n       functionCall.args.\n     * It must return function output as JSON on stdout, analogous to\n       functionResponse.response.content.\n   * Default: Empty\n   * Example: \"toolCallCommand\": \"bin/call_tool\"\n\n * mcpServers (object):\n   \n   * Description: Configures connections to one or more Model-Context Protocol\n     (MCP) servers for discovering and using custom tools. VeCLI attempts to\n     connect to each configured MCP server to discover available tools. If\n     multiple MCP servers expose a tool with the same name, the tool names will\n     be prefixed with the server alias you defined in the configuration (e.g.,\n     serverAlias__actualToolName) to avoid conflicts. Note that the system might\n     strip certain schema properties from MCP tool definitions for\n     compatibility. At least one of command, url, or httpUrl must be provided.\n     If multiple are specified, the order of precedence is httpUrl, then url,\n     then command.\n   * Default: Empty\n   * Properties:\n     * <SERVER_NAME> (object): The server parameters for the named server.\n       * command (string, optional): The command to execute to start the MCP\n         server via standard I/O.\n       * args (array of strings, optional): Arguments to pass to the command.\n       * env (object, optional): Environment variables to set for the server\n         process.\n       * cwd (string, optional): The working directory in which to start the\n         server.\n       * url (string, optional): The URL of an MCP server that uses Server-Sent\n         Events (SSE) for communication.\n       * httpUrl (string, optional): The URL of an MCP server that uses\n         streamable HTTP for communication.\n       * headers (object, optional): A map of HTTP headers to send with requests\n         to url or httpUrl.\n       * timeout (number, optional): Timeout in milliseconds for requests to\n         this MCP server.\n       * trust (boolean, optional): Trust this server and bypass all tool call\n         confirmations.\n       * description (string, optional): A brief description of the server,\n         which may be used for display purposes.\n       * includeTools (array of strings, optional): List of tool names to\n         include from this MCP server. When specified, only the tools listed\n         here will be available from this server (allowlist behavior). If not\n         specified, all tools from the server are enabled by default.\n       * excludeTools (array of strings, optional): List of tool names to\n         exclude from this MCP server. Tools listed here will not be available\n         to the model, even if they are exposed by the server. Note:\n         excludeTools takes precedence over includeTools - if a tool is in both\n         lists, it will be excluded.\n   * Example:\n     \n     \n\n * checkpointing (object):\n   \n   * Description: Configures the checkpointing feature, which allows you to save\n     and restore conversation and file states. See the Checkpointing\n     documentation for more details.\n   * Default: {\"enabled\": false}\n   * Properties:\n     * enabled (boolean): When true, the /restore command is available.\n\n * preferredEditor (string):\n   \n   * Description: Specifies the preferred editor to use for viewing diffs.\n   * Default: vscode\n   * Example: \"preferredEditor\": \"vscode\"\n\n * telemetry (object)\n   \n   * Description: Configures logging and metrics collection for VeCLI. For more\n     information, see Telemetry.\n   * Default: {\"enabled\": false, \"target\": \"local\", \"otlpEndpoint\":\n     \"http://localhost:4317\", \"logPrompts\": true}\n   * Properties:\n     * enabled (boolean): Whether or not telemetry is enabled.\n     * target (string): The destination for collected telemetry. Supported\n       values are local and gcp.\n     * otlpEndpoint (string): The endpoint for the OTLP Exporter.\n     * logPrompts (boolean): Whether or not to include the content of user\n       prompts in the logs.\n   * Example:\n     \n     \n\n * usageStatisticsEnabled (boolean):\n   \n   * Description: Enables or disables the collection of usage statistics. See\n     Usage Statistics for more information.\n   * Default: true\n   * Example:\n     \n     \n\n * hideTips (boolean):\n   \n   * Description: Enables or disables helpful tips in the CLI interface.\n   \n   * Default: false\n   \n   * Example:\n     \n     \n\n * hideBanner (boolean):\n   \n   * Description: Enables or disables the startup banner (ASCII art logo) in the\n     CLI interface.\n   \n   * Default: false\n   \n   * Example:\n     \n     \n\n * maxSessionTurns (number):\n   \n   * Description: Sets the maximum number of turns for a session. If the session\n     exceeds this limit, the CLI will stop processing and start a new chat.\n   * Default: -1 (unlimited)\n   * Example:\n     \n     \n\n * summarizeToolOutput (object):\n   \n   * Description: Enables or disables the summarization of tool output. You can\n     specify the token budget for the summarization using the tokenBudget\n     setting.\n   * Note: Currently only the run_shell_command tool is supported.\n   * Default: {} (Disabled by default)\n   * Example:\n     \n     \n\n * excludedProjectEnvVars (array of strings):\n   \n   * Description: Specifies environment variables that should be excluded from\n     being loaded from project .env files. This prevents project-specific\n     environment variables (like DEBUG=true) from interfering with gemini-cli\n     behavior. Variables from .ve/.env files are never excluded.\n   * Default: [\"DEBUG\", \"DEBUG_MODE\"]\n   * Example:\n     \n     \n\n * includeDirectories (array of strings):\n   \n   * Description: Specifies an array of additional absolute or relative paths to\n     include in the workspace context. Missing directories will be skipped with\n     a warning by default. Paths can use ~ to refer to the user's home\n     directory. This setting can be combined with the --include-directories\n     command-line flag.\n   * Default: []\n   * Example:\n     \n     \n\n * loadMemoryFromIncludeDirectories (boolean):\n   \n   * Description: Controls the behavior of the /memory refresh command. If set\n     to true, VE.md files should be loaded from all directories that are added.\n     If set to false, VE.md should only be loaded from the current directory.\n   * Default: false\n   * Example:\n     \n     \n\n * chatCompression (object):\n   \n   * Description: Controls the settings for chat history compression, both\n     automatic and when manually invoked through the /compress command.\n   * Properties:\n     * contextPercentageThreshold (number): A value between 0 and 1 that\n       specifies the token threshold for compression as a percentage of the\n       model's total token limit. For example, a value of 0.6 will trigger\n       compression when the chat history exceeds 60% of the token limit.\n   * Example:\n     \n     \n\n * showLineNumbers (boolean):\n   \n   * Description: Controls whether line numbers are displayed in code blocks in\n     the CLI output.\n   * Default: true\n   * Example:\n     \n     \n\n * accessibility (object):\n   \n   * Description: Configures accessibility features for the CLI.\n   * Properties:\n     * screenReader (boolean): Enables screen reader mode, which adjusts the TUI\n       for better compatibility with screen readers. This can also be enabled\n       with the --screen-reader command-line flag, which will take precedence\n       over the setting.\n     * disableLoadingPhrases (boolean): Disables the display of loading phrases\n       during operations.\n   * Default: {\"screenReader\": false, \"disableLoadingPhrases\": false}\n   * Example:\n     \n     \n\n\nExample settings.json:#\n\n\n\n\nShell History#\n\nThe CLI keeps a history of shell commands you run. To avoid conflicts between\ndifferent projects, this history is stored in a project-specific directory\nwithin your user's home folder.\n\n * Location: ~/.ve/tmp/<project_hash>/shell_history\n   * <project_hash> is a unique identifier generated from your project's root\n     path.\n   * The history is stored in a file named shell_history.\n\n\nEnvironment Variables & .env Files#\n\nEnvironment variables are a common way to configure applications, especially for\nsensitive information like API keys or for settings that might change between\nenvironments. For authentication setup, see the Authentication documentation\nwhich covers all available authentication methods.\n\nThe CLI automatically loads environment variables from an .env file. The loading\norder is:\n\n 1. .env file in the current working directory.\n 2. If not found, it searches upwards in parent directories until it finds an\n    .env file or reaches the project root (identified by a .git folder) or the\n    home directory.\n 3. If still not found, it looks for ~/.env (in the user's home directory).\n\nEnvironment Variable Exclusion: Some environment variables (like DEBUG and\nDEBUG_MODE) are automatically excluded from being loaded from project .env files\nto prevent interference with gemini-cli behavior. Variables from .ve/.env files\nare never excluded. You can customize this behavior using the\nexcludedProjectEnvVars setting in your settings.json file.\n\n * GEMINI_API_KEY:\n   * Your API key for the Gemini API.\n   * One of several available authentication methods.\n   * Set this in your shell profile (e.g., ~/.bashrc, ~/.zshrc) or an .env file.\n * GEMINI_MODEL:\n   * Specifies the default Gemini model to use.\n   * Overrides the hardcoded default\n   * Example: export GEMINI_MODEL=\"gemini-2.5-flash\"\n * GOOGLE_API_KEY:\n   * Your Google Cloud API key.\n   * Required for using Vertex AI in express mode.\n   * Ensure you have the necessary permissions.\n   * Example: export GOOGLE_API_KEY=\"YOUR_GOOGLE_API_KEY\".\n * GOOGLE_CLOUD_PROJECT:\n   * Your Google Cloud Project ID.\n   * Required for using Code Assist or Vertex AI.\n   * If using Vertex AI, ensure you have the necessary permissions in this\n     project.\n   * Cloud Shell Note: When running in a Cloud Shell environment, this variable\n     defaults to a special project allocated for Cloud Shell users. If you have\n     GOOGLE_CLOUD_PROJECT set in your global environment in Cloud Shell, it will\n     be overridden by this default. To use a different project in Cloud Shell,\n     you must define GOOGLE_CLOUD_PROJECT in a .env file.\n   * Example: export GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_ID\".\n * GOOGLE_APPLICATION_CREDENTIALS (string):\n   * Description: The path to your Google Application Credentials JSON file.\n   * Example: export\n     GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your/credentials.json\"\n * OTLP_GOOGLE_CLOUD_PROJECT:\n   * Your Google Cloud Project ID for Telemetry in Google Cloud\n   * Example: export OTLP_GOOGLE_CLOUD_PROJECT=\"YOUR_PROJECT_ID\".\n * GOOGLE_CLOUD_LOCATION:\n   * Your Google Cloud Project Location (e.g., us-central1).\n   * Required for using Vertex AI in non express mode.\n   * Example: export GOOGLE_CLOUD_LOCATION=\"YOUR_PROJECT_LOCATION\".\n * GEMINI_SANDBOX:\n   * Alternative to the sandbox setting in settings.json.\n   * Accepts true, false, docker, podman, or a custom command string.\n * SEATBELT_PROFILE (macOS specific):\n   * Switches the Seatbelt (sandbox-exec) profile on macOS.\n   * permissive-open: (Default) Restricts writes to the project folder (and a\n     few other folders, see\n     packages/cli/src/utils/sandbox-macos-permissive-open.sb) but allows other\n     operations.\n   * strict: Uses a strict profile that declines operations by default.\n   * <profile_name>: Uses a custom profile. To define a custom profile, create a\n     file named sandbox-macos-<profile_name>.sb in your project's .ve/ directory\n     (e.g., my-project/.ve/sandbox-macos-custom.sb).\n * DEBUG or DEBUG_MODE (often used by underlying libraries or the CLI itself):\n   * Set to true or 1 to enable verbose debug logging, which can be helpful for\n     troubleshooting.\n   * Note: These variables are automatically excluded from project .env files by\n     default to prevent interference with gemini-cli behavior. Use .ve/.env\n     files if you need to set these for gemini-cli specifically.\n * NO_COLOR:\n   * Set to any value to disable all color output in the CLI.\n * CLI_TITLE:\n   * Set to a string to customize the title of the CLI.\n * CODE_ASSIST_ENDPOINT:\n   * Specifies the endpoint for the code assist server.\n   * This is useful for development and testing.\n\n\nCommand-Line Arguments#\n\nArguments passed directly when running the CLI can override other configurations\nfor that specific session.\n\n * --model <model_name> (-m <model_name>):\n   * Specifies the Gemini model to use for this session.\n   * Example: npm start -- --model gemini-1.5-pro-latest\n * --prompt <your_prompt> (-p <your_prompt>):\n   * Used to pass a prompt directly to the command. This invokes VeCLI in a\n     non-interactive mode.\n * --prompt-interactive <your_prompt> (-i <your_prompt>):\n   * Starts an interactive session with the provided prompt as the initial\n     input.\n   * The prompt is processed within the interactive session, not before it.\n   * Cannot be used when piping input from stdin.\n   * Example: gemini -i \"explain this code\"\n * --sandbox (-s):\n   * Enables sandbox mode for this session.\n * --sandbox-image:\n   * Sets the sandbox image URI.\n * --debug (-d):\n   * Enables debug mode for this session, providing more verbose output.\n * --all-files (-a):\n   * If set, recursively includes all files within the current directory as\n     context for the prompt.\n * --help (or -h):\n   * Displays help information about command-line arguments.\n * --show-memory-usage:\n   * Displays the current memory usage.\n * --yolo:\n   * Enables YOLO mode, which automatically approves all tool calls.\n * --approval-mode <mode>:\n   * Sets the approval mode for tool calls. Available modes:\n     * default: Prompt for approval on each tool call (default behavior)\n     * auto_edit: Automatically approve edit tools (replace, write_file) while\n       prompting for others\n     * yolo: Automatically approve all tool calls (equivalent to --yolo)\n   * Cannot be used together with --yolo. Use --approval-mode=yolo instead of\n     --yolo for the new unified approach.\n   * Example: gemini --approval-mode auto_edit\n * --allowed-tools <tool1,tool2,...>:\n   * A comma-separated list of tool names that will bypass the confirmation\n     dialog.\n   * Example: gemini --allowed-tools \"ShellTool(git status)\"\n * --telemetry:\n   * Enables telemetry.\n * --telemetry-target:\n   * Sets the telemetry target. See telemetry for more information.\n * --telemetry-otlp-endpoint:\n   * Sets the OTLP endpoint for telemetry. See telemetry for more information.\n * --telemetry-otlp-protocol:\n   * Sets the OTLP protocol for telemetry (grpc or http). Defaults to grpc. See\n     telemetry for more information.\n * --telemetry-log-prompts:\n   * Enables logging of prompts for telemetry. See telemetry for more\n     information.\n * --checkpointing:\n   * Enables checkpointing.\n * --extensions <extension_name ...> (-e <extension_name ...>):\n   * Specifies a list of extensions to use for the session. If not provided, all\n     available extensions are used.\n   * Use the special term gemini -e none to disable all extensions.\n   * Example: gemini -e my-extension -e my-other-extension\n * --list-extensions (-l):\n   * Lists all available extensions and exits.\n * --proxy:\n   * Sets the proxy for the CLI.\n   * Example: --proxy http://localhost:7890.\n * --include-directories <dir1,dir2,...>:\n   * Includes additional directories in the workspace for multi-directory\n     support.\n   * Can be specified multiple times or as comma-separated values.\n   * 5 directories can be added at maximum.\n   * Example: --include-directories /path/to/project1,/path/to/project2 or\n     --include-directories /path/to/project1 --include-directories\n     /path/to/project2\n * --screen-reader:\n   * Enables screen reader mode for accessibility.\n * --version:\n   * Displays the version of the CLI.\n\n\nContext Files (Hierarchical Instructional Context)#\n\nWhile not strictly configuration for the CLI's behavior, context files\n(defaulting to GEMINI.md but configurable via the contextFileName setting) are\ncrucial for configuring the instructional context (also referred to as \"memory\")\nprovided to the Gemini model. This powerful feature allows you to give\nproject-specific instructions, coding style guides, or any relevant background\ninformation to the AI, making its responses more tailored and accurate to your\nneeds. The CLI includes UI elements, such as an indicator in the footer showing\nthe number of loaded context files, to keep you informed about the active\ncontext.\n\n * Purpose: These Markdown files contain instructions, guidelines, or context\n   that you want the Gemini model to be aware of during your interactions. The\n   system is designed to manage this instructional context hierarchically.\n\n\nExample Context File Content (e.g., GEMINI.md)#\n\nHere's a conceptual example of what a context file at the root of a TypeScript\nproject might contain:\n\n\n\nThis example demonstrates how you can provide general project context, specific\ncoding conventions, and even notes about particular files or components. The\nmore relevant and precise your context files are, the better the AI can assist\nyou. Project-specific context files are highly encouraged to establish\nconventions and context.\n\n * Hierarchical Loading and Precedence: The CLI implements a sophisticated\n   hierarchical memory system by loading context files (e.g., GEMINI.md) from\n   several locations. Content from files lower in this list (more specific)\n   typically overrides or supplements content from files higher up (more\n   general). The exact concatenation order and final context can be inspected\n   using the /memory show command. The typical loading order is:\n   1. Global Context File:\n      * Location: ~/.ve/<contextFileName> (e.g., ~/.ve/GEMINI.md in your user\n        home directory).\n      * Scope: Provides default instructions for all your projects.\n   2. Project Root & Ancestors Context Files:\n      * Location: The CLI searches for the configured context file in the\n        current working directory and then in each parent directory up to either\n        the project root (identified by a .git folder) or your home directory.\n      * Scope: Provides context relevant to the entire project or a significant\n        portion of it.\n   3. Sub-directory Context Files (Contextual/Local):\n      * Location: The CLI also scans for the configured context file in\n        subdirectories below the current working directory (respecting common\n        ignore patterns like node_modules, .git, etc.). The breadth of this\n        search is limited to 200 directories by default, but can be configured\n        with a memoryDiscoveryMaxDirs field in your settings.json file.\n      * Scope: Allows for highly specific instructions relevant to a particular\n        component, module, or subsection of your project.\n * Concatenation & UI Indication: The contents of all found context files are\n   concatenated (with separators indicating their origin and path) and provided\n   as part of the system prompt to the Gemini model. The CLI footer displays the\n   count of loaded context files, giving you a quick visual cue about the active\n   instructional context.\n * Importing Content: You can modularize your context files by importing other\n   Markdown files using the @path/to/file.md syntax. For more details, see the\n   Memory Import Processor documentation.\n * Commands for Memory Management:\n   * Use /memory refresh to force a re-scan and reload of all context files from\n     all configured locations. This updates the AI's instructional context.\n   * Use /memory show to display the combined instructional context currently\n     loaded, allowing you to verify the hierarchy and content being used by the\n     AI.\n   * See the Commands documentation for full details on the /memory command and\n     its sub-commands (show and refresh).\n\nBy understanding and utilizing these configuration layers and the hierarchical\nnature of context files, you can effectively manage the AI's memory and tailor\nthe VeCLI's responses to your specific needs and projects.\n\n\nSandboxing#\n\nThe VeCLI can execute potentially unsafe operations (like shell commands and\nfile modifications) within a sandboxed environment to protect your system.\n\nSandboxing is disabled by default, but you can enable it in a few ways:\n\n * Using --sandbox or -s flag.\n * Setting GEMINI_SANDBOX environment variable.\n * Sandbox is enabled when using --yolo or --approval-mode=yolo by default.\n\nBy default, it uses a pre-built gemini-cli-sandbox Docker image.\n\nFor project-specific sandboxing needs, you can create a custom Dockerfile at\n.ve/sandbox.Dockerfile in your project's root directory. This Dockerfile can be\nbased on the base sandbox image:\n\n\n\nWhen .ve/sandbox.Dockerfile exists, you can use BUILD_SANDBOX environment\nvariable when running VeCLI to automatically build the custom sandbox image:\n\n\n\n\nUsage Statistics#\n\nTo help us improve the VeCLI, we collect anonymized usage statistics. This data\nhelps us understand how the CLI is used, identify common issues, and prioritize\nnew features.\n\nWhat we collect:\n\n * Tool Calls: We log the names of the tools that are called, whether they\n   succeed or fail, and how long they take to execute. We do not collect the\n   arguments passed to the tools or any data returned by them.\n * API Requests: We log the Gemini model used for each request, the duration of\n   the request, and whether it was successful. We do not collect the content of\n   the prompts or responses.\n * Session Information: We collect information about the configuration of the\n   CLI, such as the enabled tools and the approval mode.\n\nWhat we DON'T collect:\n\n * Personally Identifiable Information (PII): We do not collect any personal\n   information, such as your name, email address, or API keys.\n * Prompt and Response Content: We do not log the content of your prompts or the\n   responses from the Gemini model.\n * File Content: We do not log the content of any files that are read or written\n   by the CLI.\n\nHow to opt out:\n\nYou can opt out of usage statistics collection at any time by setting the\nusageStatisticsEnabled property to false in your settings.json file:\n\n","routePath":"/en/cli/configuration-v1","lang":"en","toc":[{"text":"Configuration layers","id":"configuration-layers","depth":2,"charIndex":630},{"text":"Settings files","id":"settings-files","depth":2,"charIndex":1321},{"text":"The `.ve` directory in your project","id":"the-ve-directory-in-your-project","depth":3,"charIndex":-1},{"text":"Available settings in `settings.json`:","id":"available-settings-in-settingsjson","depth":3,"charIndex":-1},{"text":"Troubleshooting File Search Performance","id":"troubleshooting-file-search-performance","depth":3,"charIndex":5371},{"text":"Example `settings.json`:","id":"example-settingsjson","depth":3,"charIndex":-1},{"text":"Shell History","id":"shell-history","depth":2,"charIndex":19342},{"text":"Environment Variables & `.env` Files","id":"environment-variables--env-files","depth":2,"charIndex":-1},{"text":"Command-Line Arguments","id":"command-line-arguments","depth":2,"charIndex":24011},{"text":"Context Files (Hierarchical Instructional Context)","id":"context-files-hierarchical-instructional-context","depth":2,"charIndex":27572},{"text":"Example Context File Content (e.g., `GEMINI.md`)","id":"example-context-file-content-eg-geminimd","depth":3,"charIndex":-1},{"text":"Sandboxing","id":"sandboxing","depth":2,"charIndex":31812},{"text":"Usage Statistics","id":"usage-statistics","depth":2,"charIndex":32621}],"domain":"","frontmatter":{},"version":""},{"id":5,"title":"VeCLI Configuration","content":"#\n\nNote on New Configuration Format\n\nThe format of the settings.json file has been updated to a new, more organized\nstructure.\n\n * The new format will be supported in the stable release starting [09/10/25].\n * Automatic migration from the old format to the new format will begin on\n   [09/17/25].\n\nFor details on the previous format, please see the v1 Configuration\ndocumentation.\n\nVeCLI offers several ways to configure its behavior, including environment\nvariables, command-line arguments, and settings files. This document outlines\nthe different configuration methods and available settings.\n\n\nConfiguration layers#\n\nConfiguration is applied in the following order of precedence (lower numbers are\noverridden by higher numbers):\n\n 1. Default values: Hardcoded defaults within the application.\n 2. System defaults file: System-wide default settings that can be overridden by\n    other settings files.\n 3. User settings file: Global settings for the current user.\n 4. Project settings file: Project-specific settings.\n 5. System settings file: System-wide settings that override all other settings\n    files.\n 6. Environment variables: System-wide or session-specific variables,\n    potentially loaded from .env files.\n 7. Command-line arguments: Values passed when launching the CLI.\n\n\nSettings files#\n\nVeCLI uses JSON settings files for persistent configuration. There are four\nlocations for these files:\n\n * System defaults file:\n   * Location: /etc/vecli/system-defaults.json (Linux),\n     C:\\ProgramData\\vecli\\system-defaults.json (Windows) or /Library/Application\n     Support/GeminiCli/system-defaults.json (macOS). The path can be overridden\n     using the VE_CLI_SYSTEM_DEFAULTS_PATH environment variable.\n   * Scope: Provides a base layer of system-wide default settings. These\n     settings have the lowest precedence and are intended to be overridden by\n     user, project, or system override settings.\n * User settings file:\n   * Location: ~/.ve/settings.json (where ~ is your home directory).\n   * Scope: Applies to all VeCLI sessions for the current user. User settings\n     override system defaults.\n * Project settings file:\n   * Location: .ve/settings.json within your project's root directory.\n   * Scope: Applies only when running VeCLI from that specific project. Project\n     settings override user settings and system defaults.\n * System settings file:\n   * Location: /etc/vecli/settings.json (Linux),\n     C:\\ProgramData\\vecli\\settings.json (Windows) or /Library/Application\n     Support/VeCli/settings.json (macOS). The path can be overridden using the\n     VE_CLI_SYSTEM_SETTINGS_PATH environment variable.\n   * Scope: Applies to all VeCLI sessions on the system, for all users. System\n     settings act as overrides, taking precedence over all other settings files.\n     May be useful for system administrators at enterprises to have controls\n     over users' VeCLI setups.\n\nNote on environment variables in settings: String values within your\nsettings.json files can reference environment variables using either $VAR_NAME\nor ${VAR_NAME} syntax. These variables will be automatically resolved when the\nsettings are loaded. For example, if you have an environment variable\nMY_API_TOKEN, you could use it in settings.json like this: \"apiKey\":\n\"$MY_API_TOKEN\".\n\n> Note for Enterprise Users: For guidance on deploying and managing VeCLI in a\n> corporate environment, please see the Enterprise Configuration documentation.\n\n\nThe .ve directory in your project#\n\nIn addition to a project settings file, a project's .ve directory can contain\nother project-specific files related to VeCLI's operation, such as:\n\n * Custom sandbox profiles (e.g., .ve/sandbox-macos-custom.sb,\n   .ve/sandbox.Dockerfile).\n\n\nAvailable settings in settings.json#\n\nSettings are organized into categories. All settings should be placed within\ntheir corresponding top-level category object in your settings.json file.\n\ngeneral#\n\n * general.preferredEditor (string):\n   \n   * Description: The preferred editor to open files in.\n   * Default: undefined\n\n * general.vimMode (boolean):\n   \n   * Description: Enable Vim keybindings.\n   * Default: false\n\n * general.disableAutoUpdate (boolean):\n   \n   * Description: Disable automatic updates.\n   * Default: false\n\n * general.disableUpdateNag (boolean):\n   \n   * Description: Disable update notification prompts.\n   * Default: false\n\n * general.checkpointing.enabled (boolean):\n   \n   * Description: Enable session checkpointing for recovery.\n   * Default: false\n\nui#\n\n * ui.theme (string):\n   \n   * Description: The color theme for the UI. See Themes for available options.\n   * Default: undefined\n\n * ui.customThemes (object):\n   \n   * Description: Custom theme definitions.\n   * Default: {}\n\n * ui.hideWindowTitle (boolean):\n   \n   * Description: Hide the window title bar.\n   * Default: false\n\n * ui.hideTips (boolean):\n   \n   * Description: Hide helpful tips in the UI.\n   * Default: false\n\n * ui.hideBanner (boolean):\n   \n   * Description: Hide the application banner.\n   * Default: false\n\n * ui.hideFooter (boolean):\n   \n   * Description: Hide the footer from the UI.\n   * Default: false\n\n * ui.showMemoryUsage (boolean):\n   \n   * Description: Display memory usage information in the UI.\n   * Default: false\n\n * ui.showLineNumbers (boolean):\n   \n   * Description: Show line numbers in the chat.\n   * Default: false\n\n * ui.showCitations (boolean):\n   \n   * Description: Show citations for generated text in the chat.\n   * Default: false\n\n * ui.accessibility.disableLoadingPhrases (boolean):\n   \n   * Description: Disable loading phrases for accessibility.\n   * Default: false\n\nide#\n\n * ide.enabled (boolean):\n   \n   * Description: Enable IDE integration mode.\n   * Default: false\n\n * ide.hasSeenNudge (boolean):\n   \n   * Description: Whether the user has seen the IDE integration nudge.\n   * Default: false\n\nprivacy#\n\n * privacy.usageStatisticsEnabled (boolean):\n   * Description: Enable collection of usage statistics.\n   * Default: true\n\nmodel#\n\n * model.name (string):\n   \n   * Description: The model to use for conversations.\n   * Default: undefined\n\n * model.maxSessionTurns (number):\n   \n   * Description: Maximum number of user/model/tool turns to keep in a session.\n     -1 means unlimited.\n   * Default: -1\n\n * model.summarizeToolOutput (object):\n   \n   * Description: Enables or disables the summarization of tool output. You can\n     specify the token budget for the summarization using the tokenBudget\n     setting. Note: Currently only the run_shell_command tool is supported. For\n     example {\"run_shell_command\": {\"tokenBudget\": 2000}}\n   * Default: undefined\n\n * model.chatCompression.contextPercentageThreshold (number):\n   \n   * Description: Sets the threshold for chat history compression as a\n     percentage of the model's total token limit. This is a value between 0 and\n     1 that applies to both automatic compression and the manual /compress\n     command. For example, a value of 0.6 will trigger compression when the chat\n     history exceeds 60% of the token limit.\n   * Default: 0.7\n\n * model.skipNextSpeakerCheck (boolean):\n   \n   * Description: Skip the next speaker check.\n   * Default: false\n\ncontext#\n\n * context.fileName (string or array of strings):\n   \n   * Description: The name of the context file(s).\n   * Default: undefined\n\n * context.importFormat (string):\n   \n   * Description: The format to use when importing memory.\n   * Default: undefined\n\n * context.discoveryMaxDirs (number):\n   \n   * Description: Maximum number of directories to search for memory.\n   * Default: 200\n\n * context.includeDirectories (array):\n   \n   * Description: Additional directories to include in the workspace context.\n     Missing directories will be skipped with a warning.\n   * Default: []\n\n * context.loadFromIncludeDirectories (boolean):\n   \n   * Description: Controls the behavior of the /memory refresh command. If set\n     to true, VE.md files should be loaded from all directories that are added.\n     If set to false, VE.md should only be loaded from the current directory.\n   * Default: false\n\n * context.fileFiltering.respectGitIgnore (boolean):\n   \n   * Description: Respect .gitignore files when searching.\n   * Default: true\n\n * context.fileFiltering.respectGeminiIgnore (boolean):\n   \n   * Description: Respect .veignore files when searching.\n   * Default: true\n\n * context.fileFiltering.enableRecursiveFileSearch (boolean):\n   \n   * Description: Whether to enable searching recursively for filenames under\n     the current tree when completing @ prefixes in the prompt.\n   * Default: true\n\ntools#\n\n * tools.sandbox (boolean or string):\n   \n   * Description: Sandbox execution environment (can be a boolean or a path\n     string).\n   * Default: undefined\n\n * tools.usePty (boolean):\n   \n   * Description: Use node-pty for shell command execution. Fallback to\n     child_process still applies.\n   * Default: false\n\n * tools.core (array of strings):\n   \n   * Description: This can be used to restrict the set of built-in tools with an\n     allowlist. See Built-in Tools for a list of core tools. The match semantics\n     are the same as tools.allowed.\n   * Default: undefined\n\n * tools.exclude (array of strings):\n   \n   * Description: Tool names to exclude from discovery.\n   * Default: undefined\n\n * tools.allowed (array of strings):\n   \n   * Description: A list of tool names that will bypass the confirmation dialog.\n     This is useful for tools that you trust and use frequently. For example,\n     [\"run_shell_command(git)\", \"run_shell_command(npm test)\"] will skip the\n     confirmation dialog to run any git and npm test commands. See Shell Tool\n     command restrictions for details on prefix matching, command chaining, etc.\n   * Default: undefined\n\n * tools.discoveryCommand (string):\n   \n   * Description: Command to run for tool discovery.\n   * Default: undefined\n\n * tools.callCommand (string):\n   \n   * Description: Defines a custom shell command for calling a specific tool\n     that was discovered using tools.discoveryCommand. The shell command must\n     meet the following criteria:\n     * It must take function name (exactly as in function declaration) as first\n       command line argument.\n     * It must read function arguments as JSON on stdin, analogous to\n       functionCall.args.\n     * It must return function output as JSON on stdout, analogous to\n       functionResponse.response.content.\n   * Default: undefined\n\nmcp#\n\n * mcp.serverCommand (string):\n   \n   * Description: Command to start an MCP server.\n   * Default: undefined\n\n * mcp.allowed (array of strings):\n   \n   * Description: An allowlist of MCP servers to allow.\n   * Default: undefined\n\n * mcp.excluded (array of strings):\n   \n   * Description: A denylist of MCP servers to exclude.\n   * Default: undefined\n\nsecurity#\n\n * security.folderTrust.enabled (boolean):\n   \n   * Description: Setting to track whether Folder trust is enabled.\n   * Default: false\n\n * security.auth.selectedType (string):\n   \n   * Description: The currently selected authentication type.\n   * Default: undefined\n\n * security.auth.enforcedType (string):\n   \n   * Description: The required auth type (useful for enterprises).\n   * Default: undefined\n\n * security.auth.useExternal (boolean):\n   \n   * Description: Whether to use an external authentication flow.\n   * Default: undefined\n\nadvanced#\n\n * advanced.autoConfigureMemory (boolean):\n   \n   * Description: Automatically configure Node.js memory limits.\n   * Default: false\n\n * advanced.dnsResolutionOrder (string):\n   \n   * Description: The DNS resolution order.\n   * Default: undefined\n\n * advanced.excludedEnvVars (array of strings):\n   \n   * Description: Environment variables to exclude from project context.\n   * Default: [\"DEBUG\",\"DEBUG_MODE\"]\n\n * advanced.bugCommand (object):\n   \n   * Description: Configuration for the bug report command.\n   * Default: undefined\n\nmcpServers#\n\nConfigures connections to one or more Model-Context Protocol (MCP) servers for\ndiscovering and using custom tools. VeCLI attempts to connect to each configured\nMCP server to discover available tools. If multiple MCP servers expose a tool\nwith the same name, the tool names will be prefixed with the server alias you\ndefined in the configuration (e.g., serverAlias__actualToolName) to avoid\nconflicts. Note that the system might strip certain schema properties from MCP\ntool definitions for compatibility. At least one of command, url, or httpUrl\nmust be provided. If multiple are specified, the order of precedence is httpUrl,\nthen url, then command.\n\n * mcpServers.<SERVER_NAME> (object): The server parameters for the named\n   server.\n   * command (string, optional): The command to execute to start the MCP server\n     via standard I/O.\n   * args (array of strings, optional): Arguments to pass to the command.\n   * env (object, optional): Environment variables to set for the server\n     process.\n   * cwd (string, optional): The working directory in which to start the server.\n   * url (string, optional): The URL of an MCP server that uses Server-Sent\n     Events (SSE) for communication.\n   * httpUrl (string, optional): The URL of an MCP server that uses streamable\n     HTTP for communication.\n   * headers (object, optional): A map of HTTP headers to send with requests to\n     url or httpUrl.\n   * timeout (number, optional): Timeout in milliseconds for requests to this\n     MCP server.\n   * trust (boolean, optional): Trust this server and bypass all tool call\n     confirmations.\n   * description (string, optional): A brief description of the server, which\n     may be used for display purposes.\n   * includeTools (array of strings, optional): List of tool names to include\n     from this MCP server. When specified, only the tools listed here will be\n     available from this server (allowlist behavior). If not specified, all\n     tools from the server are enabled by default.\n   * excludeTools (array of strings, optional): List of tool names to exclude\n     from this MCP server. Tools listed here will not be available to the model,\n     even if they are exposed by the server. Note: excludeTools takes precedence\n     over includeTools - if a tool is in both lists, it will be excluded.\n\ntelemetry#\n\nConfigures logging and metrics collection for VeCLI. For more information, see\nTelemetry.\n\n * Properties:\n   * enabled (boolean): Whether or not telemetry is enabled.\n   * target (string): The destination for collected telemetry. Supported values\n     are local and gcp.\n   * otlpEndpoint (string): The endpoint for the OTLP Exporter.\n   * otlpProtocol (string): The protocol for the OTLP Exporter (grpc or http).\n   * logPrompts (boolean): Whether or not to include the content of user prompts\n     in the logs.\n   * outfile (string): The file to write telemetry to when target is local.\n\n\nExample settings.json#\n\nHere is an example of a settings.json file with the nested structure, new as of\nv0.3.0:\n\n\n\n\nShell History#\n\nThe CLI keeps a history of shell commands you run. To avoid conflicts between\ndifferent projects, this history is stored in a project-specific directory\nwithin your user's home folder.\n\n * Location: ~/.ve/tmp/<project_hash>/shell_history\n   * <project_hash> is a unique identifier generated from your project's root\n     path.\n   * The history is stored in a file named shell_history.\n\n\nEnvironment Variables & .env Files#\n\nEnvironment variables are a common way to configure applications, especially for\nsensitive information like API keys or for settings that might change between\nenvironments. For authentication setup, see the Authentication documentation\nwhich covers all available authentication methods.\n\nThe CLI automatically loads environment variables from an .env file. The loading\norder is:\n\n 1. .env file in the current working directory.\n 2. If not found, it searches upwards in parent directories until it finds an\n    .env file or reaches the project root (identified by a .git folder) or the\n    home directory.\n 3. If still not found, it looks for ~/.env (in the user's home directory).\n\nEnvironment Variable Exclusion: Some environment variables (like DEBUG and\nDEBUG_MODE) are automatically excluded from being loaded from project .env files\nto prevent interference with vecli behavior. Variables from .ve/.env files are\nnever excluded. You can customize this behavior using the\nadvanced.excludedEnvVars setting in your settings.json file.\n\n * GEMINI_API_KEY:\n   * Your API key for the Volcano Engine API.\n   * One of several available authentication methods.\n   * Set this in your shell profile (e.g., ~/.bashrc, ~/.zshrc) or an .env file.\n * GEMINI_MODEL:\n   * Specifies the default Volcano Engine model to use.\n   * Overrides the hardcoded default\n   * Example: export VECLI_MODEL=\"deepseek-v3-1\"\n * GOOGLE_API_KEY:\n   * Your Volcano Engine API key.\n   * Required for using Vertex AI in express mode.\n   * Ensure you have the necessary permissions.\n   * Example: export ARK_API_KEY=\"YOUR_Ark_API_KEY\".\n * GOOGLE_CLOUD_PROJECT:\n   * Your Volcano Engine Project ID.\n   * Required for using Code Assist or Vertex AI.\n   * If using Vertex AI, ensure you have the necessary permissions in this\n     project.\n   * Cloud Shell Note: When running in a Cloud Shell environment, this variable\n     defaults to a special project allocated for Cloud Shell users. If you have\n     VOLCANO_ENGINE_PROJECT set in your global environment in Cloud Shell, it\n     will be overridden by this default. To use a different project in Cloud\n     Shell, you must define VOLCANO_ENGINE_PROJECT in a .env file.\n   * Example: export VOLCANO_ENGINE_PROJECT=\"YOUR_PROJECT_ID\".\n * GOOGLE_APPLICATION_CREDENTIALS (string):\n   * Description: The path to your Volcano Engine Application Credentials JSON\n     file.\n   * Example: export\n     VOLCANO_ENGINE_APPLICATION_CREDENTIALS=\"/path/to/your/credentials.json\"\n * OTLP_VOLCANO_ENGINE_CLOUD_PROJECT:\n   * Your Volcano Engine Project ID for Telemetry in Volcano Engine\n   * Example: export OTLP_VOLCANO_ENGINE_PROJECT=\"YOUR_PROJECT_ID\".\n * VOLCANO_ENGINE_LOCATION:\n   * Your Volcano Engine Project Location (e.g., us-central1).\n   * Required for using Vertex AI in non express mode.\n   * Example: export VOLCANO_ENGINE_LOCATION=\"YOUR_PROJECT_LOCATION\".\n * GEMINI_SANDBOX:\n   * Alternative to the sandbox setting in settings.json.\n   * Accepts true, false, docker, podman, or a custom command string.\n * SEATBELT_PROFILE (macOS specific):\n   * Switches the Seatbelt (sandbox-exec) profile on macOS.\n   * permissive-open: (Default) Restricts writes to the project folder (and a\n     few other folders, see\n     packages/cli/src/utils/sandbox-macos-permissive-open.sb) but allows other\n     operations.\n   * strict: Uses a strict profile that declines operations by default.\n   * <profile_name>: Uses a custom profile. To define a custom profile, create a\n     file named sandbox-macos-<profile_name>.sb in your project's .ve/ directory\n     (e.g., my-project/.ve/sandbox-macos-custom.sb).\n * DEBUG or DEBUG_MODE (often used by underlying libraries or the CLI itself):\n   * Set to true or 1 to enable verbose debug logging, which can be helpful for\n     troubleshooting.\n   * Note: These variables are automatically excluded from project .env files by\n     default to prevent interference with vecli behavior. Use .ve/.env files if\n     you need to set these for vecli specifically.\n * NO_COLOR:\n   * Set to any value to disable all color output in the CLI.\n * CLI_TITLE:\n   * Set to a string to customize the title of the CLI.\n * CODE_ASSIST_ENDPOINT:\n   * Specifies the endpoint for the code assist server.\n   * This is useful for development and testing.\n\n\nCommand-Line Arguments#\n\nArguments passed directly when running the CLI can override other configurations\nfor that specific session.\n\n * --model <model_name> (-m <model_name>):\n   * Specifies the Volcano Engine model to use for this session.\n   * Example: npm start -- --model deepseek-v3-1\n * --prompt <your_prompt> (-p <your_prompt>):\n   * Used to pass a prompt directly to the command. This invokes VeCLI in a\n     non-interactive mode.\n * --prompt-interactive <your_prompt> (-i <your_prompt>):\n   * Starts an interactive session with the provided prompt as the initial\n     input.\n   * The prompt is processed within the interactive session, not before it.\n   * Cannot be used when piping input from stdin.\n   * Example: vecli -i \"explain this code\"\n * --sandbox (-s):\n   * Enables sandbox mode for this session.\n * --sandbox-image:\n   * Sets the sandbox image URI.\n * --debug (-d):\n   * Enables debug mode for this session, providing more verbose output.\n * --all-files (-a):\n   * If set, recursively includes all files within the current directory as\n     context for the prompt.\n * --help (or -h):\n   * Displays help information about command-line arguments.\n * --show-memory-usage:\n   * Displays the current memory usage.\n * --yolo:\n   * Enables YOLO mode, which automatically approves all tool calls.\n * --approval-mode <mode>:\n   * Sets the approval mode for tool calls. Available modes:\n     * default: Prompt for approval on each tool call (default behavior)\n     * auto_edit: Automatically approve edit tools (replace, write_file) while\n       prompting for others\n     * yolo: Automatically approve all tool calls (equivalent to --yolo)\n   * Cannot be used together with --yolo. Use --approval-mode=yolo instead of\n     --yolo for the new unified approach.\n   * Example: vecli --approval-mode auto_edit\n * --allowed-tools <tool1,tool2,...>:\n   * A comma-separated list of tool names that will bypass the confirmation\n     dialog.\n   * Example: vecli --allowed-tools \"ShellTool(git status)\"\n * --telemetry:\n   * Enables telemetry.\n * --telemetry-target:\n   * Sets the telemetry target. See telemetry for more information.\n * --telemetry-otlp-endpoint:\n   * Sets the OTLP endpoint for telemetry. See telemetry for more information.\n * --telemetry-otlp-protocol:\n   * Sets the OTLP protocol for telemetry (grpc or http). Defaults to grpc. See\n     telemetry for more information.\n * --telemetry-log-prompts:\n   * Enables logging of prompts for telemetry. See telemetry for more\n     information.\n * --checkpointing:\n   * Enables checkpointing.\n * --extensions <extension_name ...> (-e <extension_name ...>):\n   * Specifies a list of extensions to use for the session. If not provided, all\n     available extensions are used.\n   * Use the special term vecli -e none to disable all extensions.\n   * Example: vecli -e my-extension -e my-other-extension\n * --list-extensions (-l):\n   * Lists all available extensions and exits.\n * --proxy:\n   * Sets the proxy for the CLI.\n   * Example: --proxy http://localhost:7890.\n * --include-directories <dir1,dir2,...>:\n   * Includes additional directories in the workspace for multi-directory\n     support.\n   * Can be specified multiple times or as comma-separated values.\n   * 5 directories can be added at maximum.\n   * Example: --include-directories /path/to/project1,/path/to/project2 or\n     --include-directories /path/to/project1 --include-directories\n     /path/to/project2\n * --screen-reader:\n   * Enables screen reader mode, which adjusts the TUI for better compatibility\n     with screen readers.\n * --version:\n   * Displays the version of the CLI.\n\n\nContext Files (Hierarchical Instructional Context)#\n\nWhile not strictly configuration for the CLI's behavior, context files\n(defaulting to vecli.md but configurable via the context.fileName setting) are\ncrucial for configuring the instructional context (also referred to as \"memory\")\nprovided to the Volcano Engine model. This powerful feature allows you to give\nproject-specific instructions, coding style guides, or any relevant background\ninformation to the AI, making its responses more tailored and accurate to your\nneeds. The CLI includes UI elements, such as an indicator in the footer showing\nthe number of loaded context files, to keep you informed about the active\ncontext.\n\n * Purpose: These Markdown files contain instructions, guidelines, or context\n   that you want the Volcano Engine model to be aware of during your\n   interactions. The system is designed to manage this instructional context\n   hierarchically.\n\n\nExample Context File Content (e.g., VE.md)#\n\nHere's a conceptual example of what a context file at the root of a TypeScript\nproject might contain:\n\n\n\nThis example demonstrates how you can provide general project context, specific\ncoding conventions, and even notes about particular files or components. The\nmore relevant and precise your context files are, the better the AI can assist\nyou. Project-specific context files are highly encouraged to establish\nconventions and context.\n\n * Hierarchical Loading and Precedence: The CLI implements a sophisticated\n   hierarchical memory system by loading context files (e.g., VE.md) from\n   several locations. Content from files lower in this list (more specific)\n   typically overrides or supplements content from files higher up (more\n   general). The exact concatenation order and final context can be inspected\n   using the /memory show command. The typical loading order is:\n   1. Global Context File:\n      * Location: ~/.ve/<configured-context-filename> (e.g., ~/.ve/VE.md in your\n        user home directory).\n      * Scope: Provides default instructions for all your projects.\n   2. Project Root & Ancestors Context Files:\n      * Location: The CLI searches for the configured context file in the\n        current working directory and then in each parent directory up to either\n        the project root (identified by a .git folder) or your home directory.\n      * Scope: Provides context relevant to the entire project or a significant\n        portion of it.\n   3. Sub-directory Context Files (Contextual/Local):\n      * Location: The CLI also scans for the configured context file in\n        subdirectories below the current working directory (respecting common\n        ignore patterns like node_modules, .git, etc.). The breadth of this\n        search is limited to 200 directories by default, but can be configured\n        with the context.discoveryMaxDirs setting in your settings.json file.\n      * Scope: Allows for highly specific instructions relevant to a particular\n        component, module, or subsection of your project.\n * Concatenation & UI Indication: The contents of all found context files are\n   concatenated (with separators indicating their origin and path) and provided\n   as part of the system prompt to the Volcano Engine model. The CLI footer\n   displays the count of loaded context files, giving you a quick visual cue\n   about the active instructional context.\n * Importing Content: You can modularize your context files by importing other\n   Markdown files using the @path/to/file.md syntax. For more details, see the\n   Memory Import Processor documentation.\n * Commands for Memory Management:\n   * Use /memory refresh to force a re-scan and reload of all context files from\n     all configured locations. This updates the AI's instructional context.\n   * Use /memory show to display the combined instructional context currently\n     loaded, allowing you to verify the hierarchy and content being used by the\n     AI.\n   * See the Commands documentation for full details on the /memory command and\n     its sub-commands (show and refresh).\n\nBy understanding and utilizing these configuration layers and the hierarchical\nnature of context files, you can effectively manage the AI's memory and tailor\nthe VeCLI's responses to your specific needs and projects.\n\n\nSandboxing#\n\nThe VeCLI can execute potentially unsafe operations (like shell commands and\nfile modifications) within a sandboxed environment to protect your system.\n\nSandboxing is disabled by default, but you can enable it in a few ways:\n\n * Using --sandbox or -s flag.\n * Setting VECLI_SANDBOX environment variable.\n * Sandbox is enabled when using --yolo or --approval-mode=yolo by default.\n\nBy default, it uses a pre-built vecli-sandbox Docker image.\n\nFor project-specific sandboxing needs, you can create a custom Dockerfile at\n.ve/sandbox.Dockerfile in your project's root directory. This Dockerfile can be\nbased on the base sandbox image:\n\n\n\nWhen .ve/sandbox.Dockerfile exists, you can use BUILD_SANDBOX environment\nvariable when running VeCLI to automatically build the custom sandbox image:\n\n\n\n\nUsage Statistics#\n\nTo help us improve the VeCLI, we collect anonymized usage statistics. This data\nhelps us understand how the CLI is used, identify common issues, and prioritize\nnew features.\n\nWhat we collect:\n\n * Tool Calls: We log the names of the tools that are called, whether they\n   succeed or fail, and how long they take to execute. We do not collect the\n   arguments passed to the tools or any data returned by them.\n * API Requests: We log the Volcano Engine model used for each request, the\n   duration of the request, and whether it was successful. We do not collect the\n   content of the prompts or responses.\n * Session Information: We collect information about the configuration of the\n   CLI, such as the enabled tools and the approval mode.\n\nWhat we DON'T collect:\n\n * Personally Identifiable Information (PII): We do not collect any personal\n   information, such as your name, email address, or API keys.\n * Prompt and Response Content: We do not log the content of your prompts or the\n   responses from the Volcano Engine model.\n * File Content: We do not log the content of any files that are read or written\n   by the CLI.\n\nHow to opt out:\n\nYou can opt out of usage statistics collection at any time by setting the\nusageStatisticsEnabled property to false under the privacy category in your\nsettings.json file:\n\n","routePath":"/en/cli/configuration","lang":"en","toc":[{"text":"Configuration layers","id":"configuration-layers","depth":2,"charIndex":596},{"text":"Settings files","id":"settings-files","depth":2,"charIndex":1287},{"text":"The `.ve` directory in your project","id":"the-ve-directory-in-your-project","depth":3,"charIndex":-1},{"text":"Available settings in `settings.json`","id":"available-settings-in-settingsjson","depth":3,"charIndex":-1},{"text":"`general`","id":"general","depth":4,"charIndex":-1},{"text":"`ui`","id":"ui","depth":4,"charIndex":-1},{"text":"`ide`","id":"ide","depth":4,"charIndex":-1},{"text":"`privacy`","id":"privacy","depth":4,"charIndex":-1},{"text":"`model`","id":"model","depth":4,"charIndex":-1},{"text":"`context`","id":"context","depth":4,"charIndex":-1},{"text":"`tools`","id":"tools","depth":4,"charIndex":-1},{"text":"`mcp`","id":"mcp","depth":4,"charIndex":-1},{"text":"`security`","id":"security","depth":4,"charIndex":-1},{"text":"`advanced`","id":"advanced","depth":4,"charIndex":-1},{"text":"`mcpServers`","id":"mcpservers","depth":4,"charIndex":-1},{"text":"`telemetry`","id":"telemetry","depth":4,"charIndex":-1},{"text":"Example `settings.json`","id":"example-settingsjson","depth":3,"charIndex":-1},{"text":"Shell History","id":"shell-history","depth":2,"charIndex":14915},{"text":"Environment Variables & `.env` Files","id":"environment-variables--env-files","depth":2,"charIndex":-1},{"text":"Command-Line Arguments","id":"command-line-arguments","depth":2,"charIndex":19628},{"text":"Context Files (Hierarchical Instructional Context)","id":"context-files-hierarchical-instructional-context","depth":2,"charIndex":23239},{"text":"Example Context File Content (e.g., `VE.md`)","id":"example-context-file-content-eg-vemd","depth":3,"charIndex":-1},{"text":"Sandboxing","id":"sandboxing","depth":2,"charIndex":27512},{"text":"Usage Statistics","id":"usage-statistics","depth":2,"charIndex":28315}],"domain":"","frontmatter":{},"version":""},{"id":6,"title":"VeCLI for the Enterprise","content":"#\n\nThis document outlines configuration patterns and best practices for deploying\nand managing VeCLI in an enterprise environment. By leveraging system-level\nsettings, administrators can enforce security policies, manage tool access, and\nensure a consistent experience for all users.\n\n> A Note on Security: The patterns described in this document are intended to\n> help administrators create a more controlled and secure environment for using\n> VeCLI. However, they should not be considered a foolproof security boundary. A\n> determined user with sufficient privileges on their local machine may still be\n> able to circumvent these configurations. These measures are designed to\n> prevent accidental misuse and enforce corporate policy in a managed\n> environment, not to defend against a malicious actor with local administrative\n> rights.\n\n\nCentralized Configuration: The System Settings File#\n\nThe most powerful tools for enterprise administration are the system-wide\nsettings files. These files allow you to define a baseline configuration\n(system-defaults.json) and a set of overrides (settings.json) that apply to all\nusers on a machine. For a complete overview of configuration options, see the\nConfiguration documentation.\n\nSettings are merged from four files. The precedence order for single-value\nsettings (like theme) is:\n\n 1. System Defaults (system-defaults.json)\n 2. User Settings (~/.ve/settings.json)\n 3. Workspace Settings (<project>/.ve/settings.json)\n 4. System Overrides (settings.json)\n\nThis means the System Overrides file has the final say. For settings that are\narrays (includeDirectories) or objects (mcpServers), the values are merged.\n\nExample of Merging and Precedence:\n\nHere is how settings from different levels are combined.\n\n * System Defaults system-defaults.json:\n   \n   \n\n * User settings.json (~/.ve/settings.json):\n   \n   \n\n * Workspace settings.json (<project>/.ve/settings.json):\n   \n   \n\n * System Overrides settings.json:\n   \n   \n\nThis results in the following merged configuration:\n\n * Final Merged Configuration:\n   \n   \n\nWhy:\n\n * theme: The value from the system overrides (system-enforced-theme) is used,\n   as it has the highest precedence.\n\n * mcpServers: The objects are merged. The corp-server definition from the\n   system overrides takes precedence over the user's definition. The unique\n   user-tool and project-tool are included.\n\n * includeDirectories: The arrays are concatenated in the order of System\n   Defaults, User, Workspace, and then System Overrides.\n\n * Location:\n   \n   * Linux: /etc/gemini-cli/settings.json\n   * Windows: C:\\ProgramData\\gemini-cli\\settings.json\n   * macOS: /Library/Application Support/GeminiCli/settings.json\n   * The path can be overridden using the GEMINI_CLI_SYSTEM_SETTINGS_PATH\n     environment variable.\n\n * Control: This file should be managed by system administrators and protected\n   with appropriate file permissions to prevent unauthorized modification by\n   users.\n\nBy using the system settings file, you can enforce the security and\nconfiguration patterns described below.\n\n\nRestricting Tool Access#\n\nYou can significantly enhance security by controlling which tools the Gemini\nmodel can use. This is achieved through the tools.core and tools.exclude\nsettings. For a list of available tools, see the Tools documentation.\n\n\nAllowlisting with coreTools#\n\nThe most secure approach is to explicitly add the tools and commands that users\nare permitted to execute to an allowlist. This prevents the use of any tool not\non the approved list.\n\nExample: Allow only safe, read-only file operations and listing files.\n\n\n\n\nBlocklisting with excludeTools#\n\nAlternatively, you can add specific tools that are considered dangerous in your\nenvironment to a blocklist.\n\nExample: Prevent the use of the shell tool for removing files.\n\n\n\nSecurity Note: Blocklisting with excludeTools is less secure than allowlisting\nwith coreTools, as it relies on blocking known-bad commands, and clever users\nmay find ways to bypass simple string-based blocks. Allowlisting is the\nrecommended approach.\n\n\nManaging Custom Tools (MCP Servers)#\n\nIf your organization uses custom tools via Model-Context Protocol (MCP) servers,\nit is crucial to understand how server configurations are managed to apply\nsecurity policies effectively.\n\n\nHow MCP Server Configurations are Merged#\n\nVeCLI loads settings.json files from three levels: System, Workspace, and User.\nWhen it comes to the mcpServers object, these configurations are merged:\n\n 1. Merging: The lists of servers from all three levels are combined into a\n    single list.\n 2. Precedence: If a server with the same name is defined at multiple levels\n    (e.g., a server named corp-api exists in both system and user settings), the\n    definition from the highest-precedence level is used. The order of\n    precedence is: System > Workspace > User.\n\nThis means a user cannot override the definition of a server that is already\ndefined in the system-level settings. However, they can add new servers with\nunique names.\n\n\nEnforcing a Catalog of Tools#\n\nThe security of your MCP tool ecosystem depends on a combination of defining the\ncanonical servers and adding their names to an allowlist.\n\n\nRestricting Tools Within an MCP Server#\n\nFor even greater security, especially when dealing with third-party MCP servers,\nyou can restrict which specific tools from a server are exposed to the model.\nThis is done using the includeTools and excludeTools properties within a\nserver's definition. This allows you to use a subset of tools from a server\nwithout allowing potentially dangerous ones.\n\nFollowing the principle of least privilege, it is highly recommended to use\nincludeTools to create an allowlist of only the necessary tools.\n\nExample: Only allow the code-search and get-ticket-details tools from a\nthird-party MCP server, even if the server offers other tools like\ndelete-ticket.\n\n\n\nMore Secure Pattern: Define and Add to Allowlist in System Settings#\n\nTo create a secure, centrally-managed catalog of tools, the system administrator\nmust do both of the following in the system-level settings.json file:\n\n 1. Define the full configuration for every approved server in the mcpServers\n    object. This ensures that even if a user defines a server with the same\n    name, the secure system-level definition will take precedence.\n 2. Add the names of those servers to an allowlist using the mcp.allowed\n    setting. This is a critical security step that prevents users from running\n    any servers that are not on this list. If this setting is omitted, the CLI\n    will merge and allow any server defined by the user.\n\nExample System settings.json:\n\n 1. Add the names of all approved servers to an allowlist. This will prevent\n    users from adding their own servers.\n\n 2. Provide the canonical definition for each server on the allowlist.\n\n\n\nThis pattern is more secure because it uses both definition and an allowlist.\nAny server a user defines will either be overridden by the system definition (if\nit has the same name) or blocked because its name is not in the mcp.allowed\nlist.\n\n\nLess Secure Pattern: Omitting the Allowlist#\n\nIf the administrator defines the mcpServers object but fails to also specify the\nmcp.allowed allowlist, users may add their own servers.\n\nExample System settings.json:\n\nThis configuration defines servers but does not enforce the allowlist. The\nadministrator has NOT included the \"mcp.allowed\" setting.\n\n\n\nIn this scenario, a user can add their own server in their local settings.json.\nBecause there is no mcp.allowed list to filter the merged results, the user's\nserver will be added to the list of available tools and allowed to run.\n\n\nEnforcing Sandboxing for Security#\n\nTo mitigate the risk of potentially harmful operations, you can enforce the use\nof sandboxing for all tool execution. The sandbox isolates tool execution in a\ncontainerized environment.\n\nExample: Force all tool execution to happen within a Docker sandbox.\n\n\n\nYou can also specify a custom, hardened Docker image for the sandbox using the\n--sandbox-image command-line argument or by building a custom sandbox.Dockerfile\nas described in the Sandboxing documentation.\n\n\nControlling Network Access via Proxy#\n\nIn corporate environments with strict network policies, you can configure VeCLI\nto route all outbound traffic through a corporate proxy. This can be set via an\nenvironment variable, but it can also be enforced for custom tools via the\nmcpServers configuration.\n\nExample (for an MCP Server):\n\n\n\n\nTelemetry and Auditing#\n\nFor auditing and monitoring purposes, you can configure VeCLI to send telemetry\ndata to a central location. This allows you to track tool usage and other\nevents. For more information, see the telemetry documentation.\n\nExample: Enable telemetry and send it to a local OTLP collector. If otlpEndpoint\nis not specified, it defaults to http://localhost:4317.\n\n\n\nNote: Ensure that logPrompts is set to false in an enterprise setting to avoid\ncollecting potentially sensitive information from user prompts.\n\n\nAuthentication#\n\nYou can enforce a specific authentication method for all users by setting the\nenforcedAuthType in the system-level settings.json file. This prevents users\nfrom choosing a different authentication method. See the Authentication docs for\nmore details.\n\nExample: Enforce the use of Google login for all users.\n\n\n\nIf a user has a different authentication method configured, they will be\nprompted to switch to the enforced method. In non-interactive mode, the CLI will\nexit with an error if the configured authentication method does not match the\nenforced one.\n\n\nPutting It All Together: Example System settings.json#\n\nHere is an example of a system settings.json file that combines several of the\npatterns discussed above to create a secure, controlled environment for VeCLI.\n\n\n\nThis configuration:\n\n * Forces all tool execution into a Docker sandbox.\n * Strictly uses an allowlist for a small set of safe shell commands and file\n   tools.\n * Defines and allows a single corporate MCP server for custom tools.\n * Enables telemetry for auditing, without logging prompt content.\n * Redirects the /bug command to an internal ticketing system.\n * Disables general usage statistics collection.","routePath":"/en/cli/enterprise","lang":"en","toc":[{"text":"Centralized Configuration: The System Settings File","id":"centralized-configuration-the-system-settings-file","depth":2,"charIndex":841},{"text":"Restricting Tool Access","id":"restricting-tool-access","depth":2,"charIndex":3071},{"text":"Allowlisting with `coreTools`","id":"allowlisting-with-coretools","depth":3,"charIndex":-1},{"text":"Blocklisting with `excludeTools`","id":"blocklisting-with-excludetools","depth":3,"charIndex":-1},{"text":"Managing Custom Tools (MCP Servers)","id":"managing-custom-tools-mcp-servers","depth":2,"charIndex":4068},{"text":"How MCP Server Configurations are Merged","id":"how-mcp-server-configurations-are-merged","depth":3,"charIndex":4295},{"text":"Enforcing a Catalog of Tools","id":"enforcing-a-catalog-of-tools","depth":3,"charIndex":5031},{"text":"Restricting Tools Within an MCP Server","id":"restricting-tools-within-an-mcp-server","depth":3,"charIndex":5203},{"text":"More Secure Pattern: Define and Add to Allowlist in System Settings","id":"more-secure-pattern-define-and-add-to-allowlist-in-system-settings","depth":4,"charIndex":5897},{"text":"Less Secure Pattern: Omitting the Allowlist","id":"less-secure-pattern-omitting-the-allowlist","depth":3,"charIndex":7096},{"text":"Enforcing Sandboxing for Security","id":"enforcing-sandboxing-for-security","depth":2,"charIndex":7679},{"text":"Controlling Network Access via Proxy","id":"controlling-network-access-via-proxy","depth":2,"charIndex":8182},{"text":"Telemetry and Auditing","id":"telemetry-and-auditing","depth":2,"charIndex":8516},{"text":"Authentication","id":"authentication","depth":2,"charIndex":9044},{"text":"Putting It All Together: Example System `settings.json`","id":"putting-it-all-together-example-system-settingsjson","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":7,"title":"VeCLI","content":"#\n\nWithin VeCLI, packages/cli is the frontend for users to send and receive prompts\nwith the Gemini AI model and its associated tools. For a general overview of\nVeCLI, see the main documentation page.\n\n\nNavigating this section#\n\n * Authentication: A guide to setting up authentication with Google's AI\n   services.\n * Commands: A reference for VeCLI commands (e.g., /help, /tools, /theme).\n * Configuration: A guide to tailoring VeCLI behavior using configuration files.\n * Enterprise: A guide to enterprise configuration.\n * Token Caching: Optimize API costs through token caching.\n * Themes: A guide to customizing the CLI's appearance with different themes.\n * Tutorials: A tutorial showing how to use VeCLI to automate a development\n   task.\n\n\nNon-interactive mode#\n\nVeCLI can be run in a non-interactive mode, which is useful for scripting and\nautomation. In this mode, you pipe input to the CLI, it executes the command,\nand then it exits.\n\nThe following example pipes a command to VeCLI from your terminal:\n\n\n\nVeCLI executes the command and prints the output to your terminal. Note that you\ncan achieve the same behavior by using the --prompt or -p flag. For example:\n\n","routePath":"/en/cli/","lang":"en","toc":[{"text":"Navigating this section","id":"navigating-this-section","depth":2,"charIndex":202},{"text":"Non-interactive mode","id":"non-interactive-mode","depth":2,"charIndex":747}],"domain":"","frontmatter":{},"version":""},{"id":8,"title":"Themes","content":"#\n\nVeCLI supports a variety of themes to customize its color scheme and appearance.\nYou can change the theme to suit your preferences via the /theme command or\n\"theme\": configuration setting.\n\n\nAvailable Themes#\n\nVeCLI comes with a selection of pre-defined themes, which you can list using the\n/theme command within VeCLI:\n\n * Dark Themes:\n   * ANSI\n   * Atom One\n   * Ayu\n   * Default\n   * Dracula\n   * GitHub\n * Light Themes:\n   * ANSI Light\n   * Ayu Light\n   * Default Light\n   * GitHub Light\n   * Google Code\n   * Xcode\n\n\nChanging Themes#\n\n 1. Enter /theme into VeCLI.\n 2. A dialog or selection prompt appears, listing the available themes.\n 3. Using the arrow keys, select a theme. Some interfaces might offer a live\n    preview or highlight as you select.\n 4. Confirm your selection to apply the theme.\n\nNote: If a theme is defined in your settings.json file (either by name or by a\nfile path), you must remove the \"theme\" setting from the file before you can\nchange the theme using the /theme command.\n\n\nTheme Persistence#\n\nSelected themes are saved in VeCLI's configuration so your preference is\nremembered across sessions.\n\n--------------------------------------------------------------------------------\n\n\nCustom Color Themes#\n\nVeCLI allows you to create your own custom color themes by specifying them in\nyour settings.json file. This gives you full control over the color palette used\nin the CLI.\n\n\nHow to Define a Custom Theme#\n\nAdd a customThemes block to your user, project, or system settings.json file.\nEach custom theme is defined as an object with a unique name and a set of color\nkeys. For example:\n\n\n\nColor keys:\n\n * Background\n * Foreground\n * LightBlue\n * AccentBlue\n * AccentPurple\n * AccentCyan\n * AccentGreen\n * AccentYellow\n * AccentRed\n * Comment\n * Gray\n * DiffAdded (optional, for added lines in diffs)\n * DiffRemoved (optional, for removed lines in diffs)\n * DiffModified (optional, for modified lines in diffs)\n\nRequired Properties:\n\n * name (must match the key in the customThemes object and be a string)\n * type (must be the string \"custom\")\n * Background\n * Foreground\n * LightBlue\n * AccentBlue\n * AccentPurple\n * AccentCyan\n * AccentGreen\n * AccentYellow\n * AccentRed\n * Comment\n * Gray\n\nYou can use either hex codes (e.g., #FF0000) or standard CSS color names (e.g.,\ncoral, teal, blue) for any color value. See CSS color names for a full list of\nsupported names.\n\nYou can define multiple custom themes by adding more entries to the customThemes\nobject.\n\n\nLoading Themes from a File#\n\nIn addition to defining custom themes in settings.json, you can also load a\ntheme directly from a JSON file by specifying the file path in your\nsettings.json. This is useful for sharing themes or keeping them separate from\nyour main configuration.\n\nTo load a theme from a file, set the theme property in your settings.json to the\npath of your theme file:\n\n\n\nThe theme file must be a valid JSON file that follows the same structure as a\ncustom theme defined in settings.json.\n\nExample my-theme.json:\n\n\n\nSecurity Note: For your safety, VeCLI will only load theme files that are\nlocated within your home directory. If you attempt to load a theme from outside\nyour home directory, a warning will be displayed and the theme will not be\nloaded. This is to prevent loading potentially malicious theme files from\nuntrusted sources.\n\n\nExample Custom Theme#\n\n\nUsing Your Custom Theme#\n\n * Select your custom theme using the /theme command in VeCLI. Your custom theme\n   will appear in the theme selection dialog.\n * Or, set it as the default by adding \"theme\": \"MyCustomTheme\" to the ui object\n   in your settings.json.\n * Custom themes can be set at the user, project, or system level, and follow\n   the same configuration precedence as other settings.\n\n--------------------------------------------------------------------------------\n\n\nDark Themes#\n\n\nANSI#\n\n\nAtom OneDark#\n\n\nAyu#\n\n\nDefault#\n\n\nDracula#\n\n\nGitHub#\n\n\nLight Themes#\n\n\nANSI Light#\n\n\nAyu Light#\n\n\nDefault Light#\n\n\nGitHub Light#\n\n\nGoogle Code#\n\n\nXcode#","routePath":"/en/cli/themes","lang":"en","toc":[{"text":"Available Themes","id":"available-themes","depth":2,"charIndex":193},{"text":"Changing Themes","id":"changing-themes","depth":3,"charIndex":525},{"text":"Theme Persistence","id":"theme-persistence","depth":3,"charIndex":1010},{"text":"Custom Color Themes","id":"custom-color-themes","depth":2,"charIndex":1215},{"text":"How to Define a Custom Theme","id":"how-to-define-a-custom-theme","depth":3,"charIndex":1410},{"text":"Loading Themes from a File","id":"loading-themes-from-a-file","depth":3,"charIndex":2492},{"text":"Example Custom Theme","id":"example-custom-theme","depth":3,"charIndex":3347},{"text":"Using Your Custom Theme","id":"using-your-custom-theme","depth":3,"charIndex":3371},{"text":"Dark Themes","id":"dark-themes","depth":2,"charIndex":3849},{"text":"ANSI","id":"ansi","depth":3,"charIndex":3864},{"text":"Atom OneDark","id":"atom-onedark","depth":3,"charIndex":3872},{"text":"Ayu","id":"ayu","depth":3,"charIndex":3888},{"text":"Default","id":"default","depth":3,"charIndex":3895},{"text":"Dracula","id":"dracula","depth":3,"charIndex":3906},{"text":"GitHub","id":"github","depth":3,"charIndex":3917},{"text":"Light Themes","id":"light-themes","depth":2,"charIndex":3927},{"text":"ANSI Light","id":"ansi-light","depth":3,"charIndex":3943},{"text":"Ayu Light","id":"ayu-light","depth":3,"charIndex":3957},{"text":"Default Light","id":"default-light","depth":3,"charIndex":3970},{"text":"GitHub Light","id":"github-light","depth":3,"charIndex":3987},{"text":"Google Code","id":"google-code","depth":3,"charIndex":4003},{"text":"Xcode","id":"xcode","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":9,"title":"Token Caching and Cost Optimization","content":"#\n\nVeCLI automatically optimizes API costs through token caching when using API key\nauthentication (Gemini API key or Vertex AI). This feature reuses previous\nsystem instructions and context to reduce the number of tokens processed in\nsubsequent requests.\n\nToken caching is available for:\n\n * API key users (Gemini API key)\n * Vertex AI users (with project and location setup)\n\nToken caching is not available for:\n\n * OAuth users (Google Personal/Enterprise accounts) - the Code Assist API does\n   not support cached content creation at this time\n\nYou can view your token usage and cached token savings using the /stats command.\nWhen cached tokens are available, they will be displayed in the stats output.","routePath":"/en/cli/token-caching","lang":"en","toc":[],"domain":"","frontmatter":{},"version":""},{"id":10,"title":"Tutorials","content":"#\n\nThis page contains tutorials for interacting with VeCLI.\n\n\nSetting up a Model Context Protocol (MCP) server#\n\nCAUTION\n\nBefore using a third-party MCP server, ensure you trust its source and\nunderstand the tools it provides. Your use of third-party servers is at your own\nrisk.\n\nThis tutorial demonstrates how to set up a MCP server, using the GitHub MCP\nserver as an example. The GitHub MCP server provides tools for interacting with\nGitHub repositories, such as creating issues and commenting on pull requests.\n\n\nPrerequisites#\n\nBefore you begin, ensure you have the following installed and configured:\n\n * Docker: Install and run Docker.\n * GitHub Personal Access Token (PAT): Create a new classic or fine-grained PAT\n   with the necessary scopes.\n\n\nGuide#\n\nConfigure the MCP server in settings.json#\n\nIn your project's root directory, create or open the .ve/settings.json file.\nWithin the file, add the mcpServers configuration block, which provides\ninstructions for how to launch the GitHub MCP server.\n\n\n\nSet your GitHub token#\n\nCAUTION\n\nUsing a broadly scoped personal access token that has access to personal and\nprivate repositories can lead to information from the private repository being\nleaked into the public repository. We recommend using a fine-grained access\ntoken that doesn't share access to both public and private repositories.\n\nUse an environment variable to store your GitHub PAT:\n\n\n\nVeCLI uses this value in the mcpServers configuration that you defined in the\nsettings.json file.\n\nLaunch VeCLI and verify the connection#\n\nWhen you launch VeCLI, it automatically reads your configuration and launches\nthe GitHub MCP server in the background. You can then use natural language\nprompts to ask VeCLI to perform GitHub actions. For example:\n\n","routePath":"/en/cli/tutorials","lang":"en","toc":[{"text":"Setting up a Model Context Protocol (MCP) server","id":"setting-up-a-model-context-protocol-mcp-server","depth":2,"charIndex":61},{"text":"Prerequisites","id":"prerequisites","depth":3,"charIndex":516},{"text":"Guide","id":"guide","depth":3,"charIndex":754},{"text":"Configure the MCP server in `settings.json`","id":"configure-the-mcp-server-in-settingsjson","depth":4,"charIndex":-1},{"text":"Set your GitHub token","id":"set-your-github-token","depth":4,"charIndex":1012},{"text":"Launch VeCLI and verify the connection","id":"launch-vecli-and-verify-the-connection","depth":4,"charIndex":1507}],"domain":"","frontmatter":{},"version":""},{"id":11,"title":"VeCLI Core","content":"#\n\nVeCLI's core package (packages/core) is the backend portion of VeCLI, handling\ncommunication with the Volcano Engine API, managing tools, and processing\nrequests sent from packages/cli. For a general overview of VeCLI, see the main\ndocumentation page.\n\n\nNavigating this section#\n\n * Core tools API: Information on how tools are defined, registered, and used by\n   the core.\n * Memory Import Processor: Documentation for the modular VE.md import feature\n   using @file.md syntax.\n\n\nRole of the core#\n\nWhile the packages/cli portion of VeCLI provides the user interface,\npackages/core is responsible for:\n\n * Volcano Engine API interaction: Securely communicating with the Volcano\n   Engine API, sending user prompts, and receiving model responses.\n * Prompt engineering: Constructing effective prompts for the Volcano Engine\n   model, potentially incorporating conversation history, tool definitions, and\n   instructional context from VE.md files.\n * Tool management & orchestration:\n   * Registering available tools (e.g., file system tools, shell command\n     execution).\n   * Interpreting tool use requests from the Volcano Engine model.\n   * Executing the requested tools with the provided arguments.\n   * Returning tool execution results to the Volcano Engine model for further\n     processing.\n * Session and state management: Keeping track of the conversation state,\n   including history and any relevant context required for coherent\n   interactions.\n * Configuration: Managing core-specific configurations, such as API key access,\n   model selection, and tool settings.\n\n\nSecurity considerations#\n\nThe core plays a vital role in security:\n\n * API key management: It handles the VOLCENGINE_API_KEY and ensures it's used\n   securely when communicating with the Volcano Engine API.\n * Tool execution: When tools interact with the local system (e.g.,\n   run_shell_command), the core (and its underlying tool implementations) must\n   do so with appropriate caution, often involving sandboxing mechanisms to\n   prevent unintended modifications.\n\n\nChat history compression#\n\nTo ensure that long conversations don't exceed the token limits of the Volcano\nEngine model, the core includes a chat history compression feature.\n\nWhen a conversation approaches the token limit for the configured model, the\ncore automatically compresses the conversation history before sending it to the\nmodel. This compression is designed to be lossless in terms of the information\nconveyed, but it reduces the overall number of tokens used.\n\nYou can find the token limits for each model in the Volcano Engine AI\ndocumentation.\n\n\nModel fallback#\n\nVeCLI includes a model fallback mechanism to ensure that you can continue to use\nthe CLI even if the default \"pro\" model is rate-limited.\n\n\nFile discovery service#\n\nThe file discovery service is responsible for finding files in the project that\nare relevant to the current context. It is used by the @ command and other tools\nthat need to access files.\n\n\nMemory discovery service#\n\nThe memory discovery service is responsible for finding and loading the VE.md\nfiles that provide context to the model. It searches for these files in a\nhierarchical manner, starting from the current working directory and moving up\nto the project root and the user's home directory. It also searches in\nsubdirectories.\n\nThis allows you to have global, project-level, and component-level context\nfiles, which are all combined to provide the model with the most relevant\ninformation.\n\nYou can use the /memory command to show, add, and refresh the content of loaded\nVE.md files.","routePath":"/en/core/","lang":"en","toc":[{"text":"Navigating this section","id":"navigating-this-section","depth":2,"charIndex":256},{"text":"Role of the core","id":"role-of-the-core","depth":2,"charIndex":483},{"text":"Security considerations","id":"security-considerations","depth":2,"charIndex":1582},{"text":"Chat history compression","id":"chat-history-compression","depth":2,"charIndex":2051},{"text":"Model fallback","id":"model-fallback","depth":2,"charIndex":2610},{"text":"File discovery service","id":"file-discovery-service","depth":2,"charIndex":2767},{"text":"Memory discovery service","id":"memory-discovery-service","depth":2,"charIndex":2982}],"domain":"","frontmatter":{},"version":""},{"id":12,"title":"Memory Import Processor","content":"#\n\nThe Memory Import Processor is a feature that allows you to modularize your\nVE.md files by importing content from other files using the @file.md syntax.\n\n\nOverview#\n\nThis feature enables you to break down large VE.md files into smaller, more\nmanageable components that can be reused across different contexts. The import\nprocessor supports both relative and absolute paths, with built-in safety\nfeatures to prevent circular imports and ensure file access security.\n\n\nSyntax#\n\nUse the @ symbol followed by the path to the file you want to import:\n\n\n\n\nSupported Path Formats#\n\n\nRelative Paths#\n\n * @./file.md - Import from the same directory\n * @../file.md - Import from parent directory\n * @./components/file.md - Import from subdirectory\n\n\nAbsolute Paths#\n\n * @/absolute/path/to/file.md - Import using absolute path\n\n\nExamples#\n\n\nBasic Import#\n\n\n\n\nNested Imports#\n\nThe imported files can themselves contain imports, creating a nested structure:\n\n\n\n\n\n\nSafety Features#\n\n\nCircular Import Detection#\n\nThe processor automatically detects and prevents circular imports:\n\n\n\n\nFile Access Security#\n\nThe validateImportPath function ensures that imports are only allowed from\nspecified directories, preventing access to sensitive files outside the allowed\nscope.\n\n\nMaximum Import Depth#\n\nTo prevent infinite recursion, there's a configurable maximum import depth\n(default: 5 levels).\n\n\nError Handling#\n\n\nMissing Files#\n\nIf a referenced file doesn't exist, the import will fail gracefully with an\nerror comment in the output.\n\n\nFile Access Errors#\n\nPermission issues or other file system errors are handled gracefully with\nappropriate error messages.\n\n\nCode Region Detection#\n\nThe import processor uses the marked library to detect code blocks and inline\ncode spans, ensuring that @ imports inside these regions are properly ignored.\nThis provides robust handling of nested code blocks and complex Markdown\nstructures.\n\n\nImport Tree Structure#\n\nThe processor returns an import tree that shows the hierarchy of imported files,\nsimilar to Claude's /memory feature. This helps users debug problems with their\nVE.md files by showing which files were read and their import relationships.\n\nExample tree structure:\n\n\n\nThe tree preserves the order that files were imported and shows the complete\nimport chain for debugging purposes.\n\n\nComparison to Claude Code's /memory (claude.md) Approach#\n\nClaude Code's /memory feature (as seen in claude.md) produces a flat, linear\ndocument by concatenating all included files, always marking file boundaries\nwith clear comments and path names. It does not explicitly present the import\nhierarchy, but the LLM receives all file contents and paths, which is sufficient\nfor reconstructing the hierarchy if needed.\n\nNote: The import tree is mainly for clarity during development and has limited\nrelevance to LLM consumption.\n\n\nAPI Reference#\n\n\nprocessImports(content, basePath, debugMode?, importState?)#\n\nProcesses import statements in VE.md content.\n\nParameters:\n\n * content (string): The content to process for imports\n * basePath (string): The directory path where the current file is located\n * debugMode (boolean, optional): Whether to enable debug logging (default:\n   false)\n * importState (ImportState, optional): State tracking for circular import\n   prevention\n\nReturns: Promise - Object containing processed content and import tree\n\n\nProcessImportsResult#\n\n\n\n\nMemoryFile#\n\n\n\n\nvalidateImportPath(importPath, basePath, allowedDirectories)#\n\nValidates import paths to ensure they are safe and within allowed directories.\n\nParameters:\n\n * importPath (string): The import path to validate\n * basePath (string): The base directory for resolving relative paths\n * allowedDirectories (string[]): Array of allowed directory paths\n\nReturns: boolean - Whether the import path is valid\n\n\nfindProjectRoot(startDir)#\n\nFinds the project root by searching for a .git directory upwards from the given\nstart directory. Implemented as an async function using non-blocking file system\nAPIs to avoid blocking the Node.js event loop.\n\nParameters:\n\n * startDir (string): The directory to start searching from\n\nReturns: Promise - The project root directory (or the start directory if no .git\nis found)\n\n\nBest Practices#\n\n 1. Use descriptive file names for imported components\n 2. Keep imports shallow - avoid deeply nested import chains\n 3. Document your structure - maintain a clear hierarchy of imported files\n 4. Test your imports - ensure all referenced files exist and are accessible\n 5. Use relative paths when possible for better portability\n\n\nTroubleshooting#\n\n\nCommon Issues#\n\n 1. Import not working: Check that the file exists and the path is correct\n 2. Circular import warnings: Review your import structure for circular\n    references\n 3. Permission errors: Ensure the files are readable and within allowed\n    directories\n 4. Path resolution issues: Use absolute paths if relative paths aren't\n    resolving correctly\n\n\nDebug Mode#\n\nEnable debug mode to see detailed logging of the import process:\n\n","routePath":"/en/core/memport","lang":"en","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":157},{"text":"Syntax","id":"syntax","depth":2,"charIndex":469},{"text":"Supported Path Formats","id":"supported-path-formats","depth":2,"charIndex":552},{"text":"Relative Paths","id":"relative-paths","depth":3,"charIndex":578},{"text":"Absolute Paths","id":"absolute-paths","depth":3,"charIndex":742},{"text":"Examples","id":"examples","depth":2,"charIndex":820},{"text":"Basic Import","id":"basic-import","depth":3,"charIndex":832},{"text":"Nested Imports","id":"nested-imports","depth":3,"charIndex":850},{"text":"Safety Features","id":"safety-features","depth":2,"charIndex":953},{"text":"Circular Import Detection","id":"circular-import-detection","depth":3,"charIndex":972},{"text":"File Access Security","id":"file-access-security","depth":3,"charIndex":1071},{"text":"Maximum Import Depth","id":"maximum-import-depth","depth":3,"charIndex":1258},{"text":"Error Handling","id":"error-handling","depth":2,"charIndex":1379},{"text":"Missing Files","id":"missing-files","depth":3,"charIndex":1397},{"text":"File Access Errors","id":"file-access-errors","depth":3,"charIndex":1520},{"text":"Code Region Detection","id":"code-region-detection","depth":2,"charIndex":1645},{"text":"Import Tree Structure","id":"import-tree-structure","depth":2,"charIndex":1913},{"text":"Comparison to Claude Code's `/memory` (`claude.md`) Approach","id":"comparison-to-claude-codes-memory-claudemd-approach","depth":2,"charIndex":-1},{"text":"API Reference","id":"api-reference","depth":2,"charIndex":2847},{"text":"`processImports(content, basePath, debugMode?, importState?)`","id":"processimportscontent-basepath-debugmode-importstate","depth":3,"charIndex":-1},{"text":"`ProcessImportsResult`","id":"processimportsresult","depth":3,"charIndex":-1},{"text":"`MemoryFile`","id":"memoryfile","depth":3,"charIndex":-1},{"text":"`validateImportPath(importPath, basePath, allowedDirectories)`","id":"validateimportpathimportpath-basepath-alloweddirectories","depth":3,"charIndex":-1},{"text":"`findProjectRoot(startDir)`","id":"findprojectrootstartdir","depth":3,"charIndex":-1},{"text":"Best Practices","id":"best-practices","depth":2,"charIndex":4212},{"text":"Troubleshooting","id":"troubleshooting","depth":2,"charIndex":4559},{"text":"Common Issues","id":"common-issues","depth":3,"charIndex":4578},{"text":"Debug Mode","id":"debug-mode","depth":3,"charIndex":4942}],"domain":"","frontmatter":{},"version":""},{"id":13,"title":"VeCLI Core: Tools API","content":"#\n\nThe VeCLI core (packages/core) features a robust system for defining,\nregistering, and executing tools. These tools extend the capabilities of the\nVolcano Engine model, allowing it to interact with the local environment, fetch\nweb content, and perform various actions beyond simple text generation.\n\n\nCore Concepts#\n\n * Tool (tools.ts): An interface and base class (BaseTool) that defines the\n   contract for all tools. Each tool must have:\n   \n   * name: A unique internal name (used in API calls to Volcano Engine).\n   * displayName: A user-friendly name.\n   * description: A clear explanation of what the tool does, which is provided\n     to the Volcano Engine model.\n   * parameterSchema: A JSON schema defining the parameters that the tool\n     accepts. This is crucial for the Volcano Engine model to understand how to\n     call the tool correctly.\n   * validateToolParams(): A method to validate incoming parameters.\n   * getDescription(): A method to provide a human-readable description of what\n     the tool will do with specific parameters before execution.\n   * shouldConfirmExecute(): A method to determine if user confirmation is\n     required before execution (e.g., for potentially destructive operations).\n   * execute(): The core method that performs the tool's action and returns a\n     ToolResult.\n\n * ToolResult (tools.ts): An interface defining the structure of a tool's\n   execution outcome:\n   \n   * llmContent: The factual content to be included in the history sent back to\n     the LLM for context. This can be a simple string or a PartListUnion (an\n     array of Part objects and strings) for rich content.\n   * returnDisplay: A user-friendly string (often Markdown) or a special object\n     (like FileDiff) for display in the CLI.\n\n * Returning Rich Content: Tools are not limited to returning simple text. The\n   llmContent can be a PartListUnion, which is an array that can contain a mix\n   of Part objects (for images, audio, etc.) and strings. This allows a single\n   tool execution to return multiple pieces of rich content.\n\n * Tool Registry (tool-registry.ts): A class (ToolRegistry) responsible for:\n   \n   * Registering Tools: Holding a collection of all available built-in tools\n     (e.g., ReadFileTool, ShellTool).\n   * Discovering Tools: It can also discover tools dynamically:\n     * Command-based Discovery: If tools.discoveryCommand is configured in\n       settings, this command is executed. It's expected to output JSON\n       describing custom tools, which are then registered as DiscoveredTool\n       instances.\n     * MCP-based Discovery: If mcp.serverCommand is configured, the registry can\n       connect to a Model Context Protocol (MCP) server to list and register\n       tools (DiscoveredMCPTool).\n   * Providing Schemas: Exposing the FunctionDeclaration schemas of all\n     registered tools to the Volcano Engine model, so it knows what tools are\n     available and how to use them.\n   * Retrieving Tools: Allowing the core to get a specific tool by name for\n     execution.\n\n\nBuilt-in Tools#\n\nThe core comes with a suite of pre-defined tools, typically found in\npackages/core/src/tools/. These include:\n\n * File System Tools:\n   * LSTool (ls.ts): Lists directory contents.\n   * ReadFileTool (read-file.ts): Reads the content of a single file. It takes\n     an absolute_path parameter, which must be an absolute path.\n   * WriteFileTool (write-file.ts): Writes content to a file.\n   * GrepTool (grep.ts): Searches for patterns in files.\n   * GlobTool (glob.ts): Finds files matching glob patterns.\n   * EditTool (edit.ts): Performs in-place modifications to files (often\n     requiring confirmation).\n   * ReadManyFilesTool (read-many-files.ts): Reads and concatenates content from\n     multiple files or glob patterns (used by the @ command in CLI).\n * Execution Tools:\n   * ShellTool (shell.ts): Executes arbitrary shell commands (requires careful\n     sandboxing and user confirmation).\n * Web Tools:\n   * WebFetchTool (web-fetch.ts): Fetches content from a URL.\n   * WebSearchTool (web-search.ts): Performs a web search.\n * Memory Tools:\n   * MemoryTool (memoryTool.ts): Interacts with the AI's memory.\n\nEach of these tools extends BaseTool and implements the required methods for its\nspecific functionality.\n\n\nTool Execution Flow#\n\n 1. Model Request: The Volcano Engine model, based on the user's prompt and the\n    provided tool schemas, decides to use a tool and returns a FunctionCall part\n    in its response, specifying the tool name and arguments.\n 2. Core Receives Request: The core parses this FunctionCall.\n 3. Tool Retrieval: It looks up the requested tool in the ToolRegistry.\n 4. Parameter Validation: The tool's validateToolParams() method is called.\n 5. Confirmation (if needed):\n    * The tool's shouldConfirmExecute() method is called.\n    * If it returns details for confirmation, the core communicates this back to\n      the CLI, which prompts the user.\n    * The user's decision (e.g., proceed, cancel) is sent back to the core.\n 6. Execution: If validated and confirmed (or if no confirmation is needed), the\n    core calls the tool's execute() method with the provided arguments and an\n    AbortSignal (for potential cancellation).\n 7. Result Processing: The ToolResult from execute() is received by the core.\n 8. Response to Model: The llmContent from the ToolResult is packaged as a\n    FunctionResponse and sent back to the Volcano Engine model so it can\n    continue generating a user-facing response.\n 9. Display to User: The returnDisplay from the ToolResult is sent to the CLI to\n    show the user what the tool did.\n\n\nExtending with Custom Tools#\n\nWhile direct programmatic registration of new tools by users isn't explicitly\ndetailed as a primary workflow in the provided files for typical end-users, the\narchitecture supports extension through:\n\n * Command-based Discovery: Advanced users or project administrators can define\n   a tools.discoveryCommand in settings.json. This command, when run by the\n   VeCLI core, should output a JSON array of FunctionDeclaration objects. The\n   core will then make these available as DiscoveredTool instances. The\n   corresponding tools.callCommand would then be responsible for actually\n   executing these custom tools.\n * MCP Server(s): For more complex scenarios, one or more MCP servers can be set\n   up and configured via the mcpServers setting in settings.json. The VeCLI core\n   can then discover and use tools exposed by these servers. As mentioned, if\n   you have multiple MCP servers, the tool names will be prefixed with the\n   server name from your configuration (e.g., serverAlias__actualToolName).\n\nThis tool system provides a flexible and powerful way to augment the Volcano\nEngine model's capabilities, making the VeCLI a versatile assistant for a wide\nrange of tasks.","routePath":"/en/core/tools-api","lang":"en","toc":[{"text":"Core Concepts","id":"core-concepts","depth":2,"charIndex":303},{"text":"Built-in Tools","id":"built-in-tools","depth":2,"charIndex":3034},{"text":"Tool Execution Flow","id":"tool-execution-flow","depth":2,"charIndex":4272},{"text":"Extending with Custom Tools","id":"extending-with-custom-tools","depth":2,"charIndex":5609}],"domain":"","frontmatter":{},"version":""},{"id":14,"title":"VeCLI Execution and Deployment","content":"#\n\nThis document describes how to run VeCLI and explains the deployment\narchitecture that VeCLI uses.\n\n\nRunning VeCLI#\n\nThere are several ways to run VeCLI. The option you choose depends on how you\nintend to use VeCLI.\n\n--------------------------------------------------------------------------------\n\n\n1. Standard installation (Recommended for typical users)#\n\nThis is the recommended way for end-users to install VeCLI. It involves\ndownloading the VeCLI package from the NPM registry.\n\n * Global install:\n   \n   \n   \n   Then, run the CLI from anywhere:\n   \n   \n\n * NPX execution:\n   \n   \n\n--------------------------------------------------------------------------------\n\n\n2. Running in a sandbox (Docker/Podman)#\n\nFor security and isolation, VeCLI can be run inside a container. This is the\ndefault way that the CLI executes tools that might have side effects.\n\n * Directly from the Registry: You can run the published sandbox image directly.\n   This is useful for environments where you only have Docker and want to run\n   the CLI.\n   \n   \n\n * Using the --sandbox flag: If you have VeCLI installed locally (using the\n   standard installation described above), you can instruct it to run inside the\n   sandbox container.\n   \n   \n\n--------------------------------------------------------------------------------\n\n\n3. Running from source (Recommended for VeCLI contributors)#\n\nContributors to the project will want to run the CLI directly from the source\ncode.\n\n * Development Mode: This method provides hot-reloading and is useful for active\n   development.\n   \n   \n\n * Production-like mode (Linked package): This method simulates a global\n   installation by linking your local package. It's useful for testing a local\n   build in a production workflow.\n   \n   \n\n--------------------------------------------------------------------------------\n\n\n4. Running the latest VeCLI commit from GitHub#\n\nYou can run the most recently committed version of VeCLI directly from the\nGitHub repository. This is useful for testing features still in development.\n\n\n\n\nDeployment architecture#\n\nThe execution methods described above are made possible by the following\narchitectural components and processes:\n\nNPM packages\n\nVeCLI project is a monorepo that publishes two core packages to the NPM\nregistry:\n\n * @vecli/vecli-core: The backend, handling logic and tool execution.\n * @vecli/vecli: The user-facing frontend.\n\nThese packages are used when performing the standard installation and when\nrunning VeCLI from the source.\n\nBuild and packaging processes\n\nThere are two distinct build processes used, depending on the distribution\nchannel:\n\n * NPM publication: For publishing to the NPM registry, the TypeScript source\n   code in @vecli/vecli-core and @vecli/vecli is transpiled into standard\n   JavaScript using the TypeScript Compiler (tsc). The resulting dist/ directory\n   is what gets published in the NPM package. This is a standard approach for\n   TypeScript libraries.\n\n * GitHub npx execution: When running the latest version of VeCLI directly from\n   GitHub, a different process is triggered by the prepare script in\n   package.json. This script uses esbuild to bundle the entire application and\n   its dependencies into a single, self-contained JavaScript file. This bundle\n   is created on-the-fly on the user's machine and is not checked into the\n   repository.\n\nDocker sandbox image\n\nThe Docker-based execution method is supported by the vecli-sandbox container\nimage. This image is published to a container registry and contains a\npre-installed, global version of VeCLI.\n\n\nRelease process#\n\nThe release process is automated through GitHub Actions. The release workflow\nperforms the following actions:\n\n 1. Build the NPM packages using tsc.\n 2. Publish the NPM packages to the artifact registry.\n 3. Create GitHub releases with bundled assets.","routePath":"/en/deployment","lang":"en","toc":[{"text":"Running VeCLI","id":"running-vecli","depth":2,"charIndex":103},{"text":"1. Standard installation (Recommended for typical users)","id":"1-standard-installation-recommended-for-typical-users","depth":3,"charIndex":302},{"text":"2. Running in a sandbox (Docker/Podman)","id":"2-running-in-a-sandbox-dockerpodman","depth":3,"charIndex":673},{"text":"3. Running from source (Recommended for VeCLI contributors)","id":"3-running-from-source-recommended-for-vecli-contributors","depth":3,"charIndex":1314},{"text":"4. Running the latest VeCLI commit from GitHub","id":"4-running-the-latest-vecli-commit-from-github","depth":3,"charIndex":1846},{"text":"Deployment architecture","id":"deployment-architecture","depth":2,"charIndex":2051},{"text":"Release process","id":"release-process","depth":2,"charIndex":3572}],"domain":"","frontmatter":{},"version":""},{"id":15,"title":"Example Proxy Script","content":"#\n\nThe following is an example of a proxy script that can be used with the\nGEMINI_SANDBOX_PROXY_COMMAND environment variable. This script only allows HTTPS\nconnections to example.com:443 and declines all other requests.\n\n","routePath":"/en/examples/proxy-script","lang":"en","toc":[],"domain":"","frontmatter":{},"version":""},{"id":16,"title":"Variables","content":"VeCLI Extensions#\n\nVeCLI supports extensions that can be used to configure and extend its\nfunctionality.\n\n\nHow it works#\n\nOn startup, VeCLI looks for extensions in two locations:\n\n 1. <workspace>/.vecli/extensions\n 2. <home>/.vecli/extensions\n\nVeCLI loads all extensions from both locations. If an extension with the same\nname exists in both locations, the extension in the workspace directory takes\nprecedence.\n\nWithin each location, individual extensions exist as a directory that contains a\nvecli-extension.json file. For example:\n\n<workspace>/.vecli/extensions/my-extension/vecli-extension.json\n\n\nvecli-extension.json#\n\nThe vecli-extension.json file contains the configuration for the extension. The\nfile has the following structure:\n\n\n\n * name: The name of the extension. This is used to uniquely identify the\n   extension and for conflict resolution when extension commands have the same\n   name as user or project commands.\n * version: The version of the extension.\n * mcpServers: A map of MCP servers to configure. The key is the name of the\n   server, and the value is the server configuration. These servers will be\n   loaded on startup just like MCP servers configured in a settings.json file.\n   If both an extension and a settings.json file configure an MCP server with\n   the same name, the server defined in the settings.json file takes precedence.\n * contextFileName: The name of the file that contains the context for the\n   extension. This will be used to load the context from the workspace. If this\n   property is not used but a VE.md file is present in your extension directory,\n   then that file will be loaded.\n * excludeTools: An array of tool names to exclude from the model. You can also\n   specify command-specific restrictions for tools that support it, like the\n   run_shell_command tool. For example, \"excludeTools\": [\"run_shell_command(rm\n   -rf)\"] will block the rm -rf command.\n\nWhen VeCLI starts, it loads all the extensions and merges their configurations.\nIf there are any conflicts, the workspace configuration takes precedence.\n\n\nExtension Commands#\n\nExtensions can provide custom commands by placing TOML files in a commands/\nsubdirectory within the extension directory. These commands follow the same\nformat as user and project custom commands and use standard naming conventions.\n\n\nExample#\n\nAn extension named gcp with the following structure:\n\n\n\nWould provide these commands:\n\n * /deploy - Shows as [gcp] Custom command from deploy.toml in help\n * /gcs:sync - Shows as [gcp] Custom command from sync.toml in help\n\n\nConflict Resolution#\n\nExtension commands have the lowest precedence. When a conflict occurs with user\nor project commands:\n\n 1. No conflict: Extension command uses its natural name (e.g., /deploy)\n 2. With conflict: Extension command is renamed with the extension prefix (e.g.,\n    /gcp.deploy)\n\nFor example, if both a user and the gcp extension define a deploy command:\n\n * /deploy - Executes the user's deploy command\n * /gcp.deploy - Executes the extension's deploy command (marked with [gcp] tag)\n\n\nVariables#\n\nVeCLI extensions allow variable substitution in vecli-extension.json. This can\nbe useful if e.g., you need the current directory to run an MCP server using\n\"cwd\": \"${extensionPath}${/}run.ts\".\n\nSupported variables:\n\nVARIABLE                   DESCRIPTION\n${extensionPath}           The fully-qualified path of the extension in the user's\n                           filesystem e.g.,\n                           '/Users/username/.vecli/extensions/example-extension'. This\n                           will not unwrap symlinks.\n${/} or ${pathSeparator}   The path separator (differs per OS).","routePath":"/en/extension","lang":"en","toc":[{"text":"How it works","id":"how-it-works","depth":2,"charIndex":106},{"text":"`vecli-extension.json`","id":"vecli-extensionjson","depth":3,"charIndex":-1},{"text":"Extension Commands","id":"extension-commands","depth":2,"charIndex":2067},{"text":"Example","id":"example","depth":3,"charIndex":2322},{"text":"Conflict Resolution","id":"conflict-resolution","depth":3,"charIndex":2557}],"domain":"","frontmatter":{},"version":""},{"id":17,"title":"Ignoring Files","content":"#\n\nThis document provides an overview of the Gemini Ignore (.veignore) feature of\nthe VeCLI.\n\nThe VeCLI includes the ability to automatically ignore files, similar to\n.gitignore (used by Git) and .aiexclude (used by Ve Code Assist). Adding paths\nto your .veignore file will exclude them from tools that support this feature,\nalthough they will still be visible to other services (such as Git).\n\n\nHow it works#\n\nWhen you add a path to your .veignore file, tools that respect this file will\nexclude matching files and directories from their operations. For example, when\nyou use the read_many_files command, any paths in your .veignore file will be\nautomatically excluded.\n\nFor the most part, .veignore follows the conventions of .gitignore files:\n\n * Blank lines and lines starting with # are ignored.\n * Standard glob patterns are supported (such as *, ?, and []).\n * Putting a / at the end will only match directories.\n * Putting a / at the beginning anchors the path relative to the .veignore file.\n * ! negates a pattern.\n\nYou can update your .veignore file at any time. To apply the changes, you must\nrestart your VeCLI session.\n\n\nHow to use .veignore#\n\nTo enable .veignore:\n\n 1. Create a file named .veignore in the root of your project directory.\n\nTo add a file or directory to .veignore:\n\n 1. Open your .veignore file.\n 2. Add the path or file you want to ignore, for example: /archive/ or\n    apikeys.txt.\n\n\n.veignore examples#\n\nYou can use .veignore to ignore directories and files:\n\n\n\nYou can use wildcards in your .veignore file with *:\n\n\n\nFinally, you can exclude files and directories from exclusion with !:\n\n\n\nTo remove paths from your .veignore file, delete the relevant lines.","routePath":"/en/gemini-ignore","lang":"en","toc":[{"text":"How it works","id":"how-it-works","depth":2,"charIndex":395},{"text":"How to use `.veignore`","id":"how-to-use-veignore","depth":2,"charIndex":-1},{"text":"`.veignore` examples","id":"veignore-examples","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":18,"title":"IDE Integration","content":"#\n\nVeCLI can integrate with your IDE to provide a more seamless and context-aware\nexperience. This integration allows the CLI to understand your workspace better\nand enables powerful features like native in-editor diffing.\n\nCurrently, the only supported IDE is Visual Studio Code and other editors that\nsupport VS Code extensions.\n\n\nFeatures#\n\n * Workspace Context: The CLI automatically gains awareness of your workspace to\n   provide more relevant and accurate responses. This context includes:\n   \n   * The 10 most recently accessed files in your workspace.\n   * Your active cursor position.\n   * Any text you have selected (up to a 16KB limit; longer selections will be\n     truncated).\n\n * Native Diffing: When Volcano Engine suggests code modifications, you can view\n   the changes directly within your IDE's native diff viewer. This allows you to\n   review, edit, and accept or reject the suggested changes seamlessly.\n\n * VS Code Commands: You can access VeCLI features directly from the VS Code\n   Command Palette (Cmd+Shift+P or Ctrl+Shift+P):\n   \n   * VeCLI: Run: Starts a new VeCLI session in the integrated terminal.\n   * VeCLI: Accept Diff: Accepts the changes in the active diff editor.\n   * VeCLI: Close Diff Editor: Rejects the changes and closes the active diff\n     editor.\n   * VeCLI: View Third-Party Notices: Displays the third-party notices for the\n     extension.\n\n\nInstallation and Setup#\n\nThere are three ways to set up the IDE integration:\n\n\n1. Automatic Nudge (Recommended)#\n\nWhen you run VeCLI inside a supported editor, it will automatically detect your\nenvironment and prompt you to connect. Answering \"Yes\" will automatically run\nthe necessary setup, which includes installing the companion extension and\nenabling the connection.\n\n\n2. Manual Installation from CLI#\n\nIf you previously dismissed the prompt or want to install the extension\nmanually, you can run the following command inside VeCLI:\n\n\n\nThis will find the correct extension for your IDE and install it.\n\n\n3. Manual Installation from a Marketplace#\n\nYou can also install the extension directly from a marketplace.\n\n * For Visual Studio Code: Install from the VS Code Marketplace.\n * For VS Code Forks: To support forks of VS Code, the extension is also\n   published on the Open VSX Registry. Follow your editor's instructions for\n   installing extensions from this registry.\n\n> NOTE: The \"VeCLI Companion\" extension may appear towards the bottom of search\n> results. If you don't see it immediately, try scrolling down or sorting by\n> \"Newly Published\".\n> \n> After manually installing the extension, you must run /ide enable in the CLI\n> to activate the integration.\n\n\nUsage#\n\n\nEnabling and Disabling#\n\nYou can control the IDE integration from within the CLI:\n\n * To enable the connection to the IDE, run:\n   \n   \n\n * To disable the connection, run:\n   \n   \n\nWhen enabled, VeCLI will automatically attempt to connect to the IDE companion\nextension.\n\n\nChecking the Status#\n\nTo check the connection status and see the context the CLI has received from the\nIDE, run:\n\n\n\nIf connected, this command will show the IDE it's connected to and a list of\nrecently opened files it is aware of.\n\n(Note: The file list is limited to 10 recently accessed files within your\nworkspace and only includes local files on disk.)\n\n\nWorking with Diffs#\n\nWhen you ask Volcano Engine to modify a file, it can open a diff view directly\nin your editor.\n\nTo accept a diff, you can perform any of the following actions:\n\n * Click the checkmark icon in the diff editor's title bar.\n * Save the file (e.g., with Cmd+S or Ctrl+S).\n * Open the Command Palette and run VeCLI: Accept Diff.\n * Respond with yes in the CLI when prompted.\n\nTo reject a diff, you can:\n\n * Click the 'x' icon in the diff editor's title bar.\n * Close the diff editor tab.\n * Open the Command Palette and run VeCLI: Close Diff Editor.\n * Respond with no in the CLI when prompted.\n\nYou can also modify the suggested changes directly in the diff view before\naccepting them.\n\nIf you select ‘Yes, allow always’ in the CLI, changes will no longer show up in\nthe IDE as they will be auto-accepted.\n\n\nUsing with Sandboxing#\n\nIf you are using VeCLI within a sandbox, please be aware of the following:\n\n * On macOS: The IDE integration requires network access to communicate with the\n   IDE companion extension. You must use a Seatbelt profile that allows network\n   access.\n * In a Docker Container: If you run VeCLI inside a Docker (or Podman)\n   container, the IDE integration can still connect to the VS Code extension\n   running on your host machine. The CLI is configured to automatically find the\n   IDE server on host.docker.internal. No special configuration is usually\n   required, but you may need to ensure your Docker networking setup allows\n   connections from the container to the host.\n\n\nTroubleshooting#\n\nIf you encounter issues with IDE integration, here are some common error\nmessages and how to resolve them.\n\n\nConnection Errors#\n\n * Message: 🔴 Disconnected: Failed to connect to IDE companion extension in\n   [IDE Name]. Please ensure the extension is running. To install the extension,\n   run /ide install.\n   \n   * Cause: VeCLI could not find the necessary environment variables\n     (VE_CLI_IDE_WORKSPACE_PATH or VE_CLI_IDE_SERVER_PORT) to connect to the\n     IDE. This usually means the IDE companion extension is not running or did\n     not initialize correctly.\n   * Solution:\n     1. Make sure you have installed the VeCLI Companion extension in your IDE\n        and that it is enabled.\n     2. Open a new terminal window in your IDE to ensure it picks up the correct\n        environment.\n\n * Message: 🔴 Disconnected: IDE connection error. The connection was lost\n   unexpectedly. Please try reconnecting by running /ide enable\n   \n   * Cause: The connection to the IDE companion was lost.\n   * Solution: Run /ide enable to try and reconnect. If the issue continues,\n     open a new terminal window or restart your IDE.\n\n\nConfiguration Errors#\n\n * Message: 🔴 Disconnected: Directory mismatch. VeCLI is running in a different\n   location than the open workspace in [IDE Name]. Please run the CLI from one\n   of the following directories: [List of directories]\n   \n   * Cause: The CLI's current working directory is outside the workspace you\n     have open in your IDE.\n   * Solution: cd into the same directory that is open in your IDE and restart\n     the CLI.\n\n * Message: 🔴 Disconnected: To use this feature, please open a workspace folder\n   in [IDE Name] and try again.\n   \n   * Cause: You have no workspace open in your IDE.\n   * Solution: Open a workspace in your IDE and restart the CLI.\n\n\nGeneral Errors#\n\n * Message: IDE integration is not supported in your current environment. To use\n   this feature, run VeCLI in one of these supported IDEs: [List of IDEs]\n   \n   * Cause: You are running VeCLI in a terminal or environment that is not a\n     supported IDE.\n   * Solution: Run VeCLI from the integrated terminal of a supported IDE, like\n     VS Code.\n\n * Message: No installer is available for IDE. Please install the VeCLI\n   Companion extension manually from the marketplace.\n   \n   * Cause: You ran /ide install, but the CLI does not have an automated\n     installer for your specific IDE.\n   * Solution: Open your IDE's extension marketplace, search for \"VeCLI\n     Companion\", and install it manually.","routePath":"/en/ide-integration","lang":"en","toc":[{"text":"Features","id":"features","depth":2,"charIndex":332},{"text":"Installation and Setup","id":"installation-and-setup","depth":2,"charIndex":1389},{"text":"1. Automatic Nudge (Recommended)","id":"1-automatic-nudge-recommended","depth":3,"charIndex":1468},{"text":"2. Manual Installation from CLI","id":"2-manual-installation-from-cli","depth":3,"charIndex":1763},{"text":"3. Manual Installation from a Marketplace","id":"3-manual-installation-from-a-marketplace","depth":3,"charIndex":1998},{"text":"Usage","id":"usage","depth":2,"charIndex":2661},{"text":"Enabling and Disabling","id":"enabling-and-disabling","depth":3,"charIndex":2670},{"text":"Checking the Status","id":"checking-the-status","depth":3,"charIndex":2943},{"text":"Working with Diffs","id":"working-with-diffs","depth":3,"charIndex":3301},{"text":"Using with Sandboxing","id":"using-with-sandboxing","depth":2,"charIndex":4126},{"text":"Troubleshooting","id":"troubleshooting","depth":2,"charIndex":4827},{"text":"Connection Errors","id":"connection-errors","depth":3,"charIndex":4954},{"text":"Configuration Errors","id":"configuration-errors","depth":3,"charIndex":5975},{"text":"General Errors","id":"general-errors","depth":3,"charIndex":6652}],"domain":"","frontmatter":{},"version":""},{"id":20,"title":"Integration Tests","content":"#\n\nThis document provides information about the integration testing framework used\nin this project.\n\n\nOverview#\n\nThe integration tests are designed to validate the end-to-end functionality of\nthe VeCLI. They execute the built binary in a controlled environment and verify\nthat it behaves as expected when interacting with the file system.\n\nThese tests are located in the integration-tests directory and are run using a\ncustom test runner.\n\n\nRunning the tests#\n\nThe integration tests are not run as part of the default npm run test command.\nThey must be run explicitly using the npm run test:integration:all script.\n\nThe integration tests can also be run using the following shortcut:\n\n\n\n\nRunning a specific set of tests#\n\nTo run a subset of test files, you can use npm run <integration test command>\n<file_name1> .... where is either test:e2e or test:integration* and <file_name>\nis any of the .test.js files in the integration-tests/ directory. For example,\nthe following command runs list_directory.test.js and write_file.test.js:\n\n\n\n\nRunning a single test by name#\n\nTo run a single test by its name, use the --test-name-pattern flag:\n\n\n\n\nRunning all tests#\n\nTo run the entire suite of integration tests, use the following command:\n\n\n\n\nSandbox matrix#\n\nThe all command will run tests for no sandboxing, docker and podman. Each\nindividual type can be run using the following commands:\n\n\n\n\n\n\n\n\nDiagnostics#\n\nThe integration test runner provides several options for diagnostics to help\ntrack down test failures.\n\n\nKeeping test output#\n\nYou can preserve the temporary files created during a test run for inspection.\nThis is useful for debugging issues with file system operations.\n\nTo keep the test output set the KEEP_OUTPUT environment variable to true.\n\n\n\nWhen output is kept, the test runner will print the path to the unique directory\nfor the test run.\n\n\nVerbose output#\n\nFor more detailed debugging, set the VERBOSE environment variable to true.\n\n\n\nWhen using VERBOSE=true and KEEP_OUTPUT=true in the same command, the output is\nstreamed to the console and also saved to a log file within the test's temporary\ndirectory.\n\nThe verbose output is formatted to clearly identify the source of the logs:\n\n\n\n\nLinting and formatting#\n\nTo ensure code quality and consistency, the integration test files are linted as\npart of the main build process. You can also manually run the linter and\nauto-fixer.\n\n\nRunning the linter#\n\nTo check for linting errors, run the following command:\n\n\n\nYou can include the :fix flag in the command to automatically fix any fixable\nlinting errors:\n\n\n\n\nDirectory structure#\n\nThe integration tests create a unique directory for each test run inside the\n.integration-tests directory. Within this directory, a subdirectory is created\nfor each test file, and within that, a subdirectory is created for each\nindividual test case.\n\nThis structure makes it easy to locate the artifacts for a specific test run,\nfile, or case.\n\n\n\n\nContinuous integration#\n\nTo ensure the integration tests are always run, a GitHub Actions workflow is\ndefined in .github/workflows/e2e.yml. This workflow automatically runs the\nintegrations tests for pull requests against the main branch, or when a pull\nrequest is added to a merge queue.\n\nThe workflow runs the tests in different sandboxing environments to ensure VeCLI\nis tested across each:\n\n * sandbox:none: Runs the tests without any sandboxing.\n * sandbox:docker: Runs the tests in a Docker container.\n * sandbox:podman: Runs the tests in a Podman container.","routePath":"/en/integration-tests","lang":"en","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":101},{"text":"Running the tests","id":"running-the-tests","depth":2,"charIndex":440},{"text":"Running a specific set of tests","id":"running-a-specific-set-of-tests","depth":2,"charIndex":687},{"text":"Running a single test by name","id":"running-a-single-test-by-name","depth":3,"charIndex":1036},{"text":"Running all tests","id":"running-all-tests","depth":3,"charIndex":1140},{"text":"Sandbox matrix","id":"sandbox-matrix","depth":3,"charIndex":1237},{"text":"Diagnostics","id":"diagnostics","depth":2,"charIndex":1393},{"text":"Keeping test output","id":"keeping-test-output","depth":3,"charIndex":1512},{"text":"Verbose output","id":"verbose-output","depth":3,"charIndex":1857},{"text":"Linting and formatting","id":"linting-and-formatting","depth":2,"charIndex":2205},{"text":"Running the linter","id":"running-the-linter","depth":3,"charIndex":2398},{"text":"Directory structure","id":"directory-structure","depth":2,"charIndex":2576},{"text":"Continuous integration","id":"continuous-integration","depth":2,"charIndex":2946}],"domain":"","frontmatter":{},"version":""},{"id":21,"title":"Welcome to VeCLI documentation","content":"#\n\nThis documentation provides a comprehensive guide to installing, using, and\ndeveloping VeCLI. This tool lets you interact with Volcano Engine models through\na command-line interface.\n\n\nOverview#\n\nVeCLI brings the capabilities of Volcano Engine models to your terminal in an\ninteractive Read-Eval-Print Loop (REPL) environment. VeCLI consists of a\nclient-side application (packages/cli) that communicates with a local server\n(packages/core), which in turn manages requests to the Volcano Engine API and\nits AI models. VeCLI also contains a variety of tools for tasks such as\nperforming file system operations, running shells, and web fetching, which are\nmanaged by packages/core.\n\n\nNavigating the documentation#\n\nThis documentation is organized into the following sections:\n\n * Execution and Deployment: Information for running VeCLI.\n * Architecture Overview: Understand the high-level design of VeCLI, including\n   its components and how they interact.\n * CLI Usage: Documentation for packages/cli.\n   * CLI Introduction: Overview of the command-line interface.\n   * Commands: Description of available CLI commands.\n   * Configuration: Information on configuring the CLI.\n   * Checkpointing: Documentation for the checkpointing feature.\n   * Extensions: How to extend the CLI with new functionality.\n   * IDE Integration: Connect the CLI to your editor.\n   * Telemetry: Overview of telemetry in the CLI.\n * Core Details: Documentation for packages/core.\n   * Core Introduction: Overview of the core component.\n   * Tools API: Information on how the core manages and exposes tools.\n * Tools:\n   * Tools Overview: Overview of the available tools.\n   * File System Tools: Documentation for the read_file and write_file tools.\n   * Multi-File Read Tool: Documentation for the read_many_files tool.\n   * Shell Tool: Documentation for the run_shell_command tool.\n   * Web Fetch Tool: Documentation for the web_fetch tool.\n   * Memory Tool: Documentation for the save_memory tool.\n * Contributing & Development Guide: Information for contributors and\n   developers, including setup, building, testing, and coding conventions.\n * NPM: Details on how the project's packages are structured\n * Troubleshooting Guide: Find solutions to common problems and FAQs.\n * Terms of Service and Privacy Notice: Information on the terms of service and\n   privacy notices applicable to your use of VeCLI.\n * Releases: Information on the project's releases and deployment cadence.\n\nWe hope this documentation helps you make the most of the VeCLI!","routePath":"/en/introduction","lang":"en","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":187},{"text":"Navigating the documentation","id":"navigating-the-documentation","depth":2,"charIndex":683}],"domain":"","frontmatter":{},"version":""},{"id":22,"title":"Automation and Triage Processes","content":"#\n\nThis document provides a detailed overview of the automated processes we use to\nmanage and triage issues and pull requests. Our goal is to provide prompt\nfeedback and ensure that contributions are reviewed and integrated efficiently.\nUnderstanding this automation will help you as a contributor know what to expect\nand how to best interact with our repository bots.\n\n\nGuiding Principle: Issues and Pull Requests#\n\nFirst and foremost, almost every Pull Request (PR) should be linked to a\ncorresponding Issue. The issue describes the \"what\" and the \"why\" (the bug or\nfeature), while the PR is the \"how\" (the implementation). This separation helps\nus track work, prioritize features, and maintain clear historical context. Our\nautomation is built around this principle.\n\n--------------------------------------------------------------------------------\n\n\nDetailed Automation Workflows#\n\nHere is a breakdown of the specific automation workflows that run in our\nrepository.\n\n\n1. When you open an Issue: Automated Issue Triage#\n\nThis is the first bot you will interact with when you create an issue. Its job\nis to perform an initial analysis and apply the correct labels.\n\n * Workflow File: .github/workflows/gemini-automated-issue-triage.yml\n * When it runs: Immediately after an issue is created or reopened.\n * What it does:\n   * It uses a Gemini model to analyze the issue's title and body against a\n     detailed set of guidelines.\n   * Applies one area/* label: Categorizes the issue into a functional area of\n     the project (e.g., area/ux, area/models, area/platform).\n   * Applies one kind/* label: Identifies the type of issue (e.g., kind/bug,\n     kind/enhancement, kind/question).\n   * Applies one priority/* label: Assigns a priority from P0 (critical) to P3\n     (low) based on the described impact.\n   * May apply status/need-information: If the issue lacks critical details\n     (like logs or reproduction steps), it will be flagged for more information.\n   * May apply status/need-retesting: If the issue references a CLI version that\n     is more than six versions old, it will be flagged for retesting on a\n     current version.\n * What you should do:\n   * Fill out the issue template as completely as possible. The more detail you\n     provide, the more accurate the triage will be.\n   * If the status/need-information label is added, please provide the requested\n     details in a comment.\n\n\n2. When you open a Pull Request: Continuous Integration (CI)#\n\nThis workflow ensures that all changes meet our quality standards before they\ncan be merged.\n\n * Workflow File: .github/workflows/ci.yml\n * When it runs: On every push to a pull request.\n * What it does:\n   * Lint: Checks that your code adheres to our project's formatting and style\n     rules.\n   * Test: Runs our full suite of automated tests across macOS, Windows, and\n     Linux, and on multiple Node.js versions. This is the most time-consuming\n     part of the CI process.\n   * Post Coverage Comment: After all tests have successfully passed, a bot will\n     post a comment on your PR. This comment provides a summary of how well your\n     changes are covered by tests.\n * What you should do:\n   * Ensure all CI checks pass. A green checkmark ✅ will appear next to your\n     commit when everything is successful.\n   * If a check fails (a red \"X\" ❌), click the \"Details\" link next to the failed\n     check to view the logs, identify the problem, and push a fix.\n\n\n3. Ongoing Triage for Pull Requests: PR Auditing and Label Sync#\n\nThis workflow runs periodically to ensure all open PRs are correctly linked to\nissues and have consistent labels.\n\n * Workflow File: .github/workflows/gemini-scheduled-pr-triage.yml\n * When it runs: Every 15 minutes on all open pull requests.\n * What it does:\n   * Checks for a linked issue: The bot scans your PR description for a keyword\n     that links it to an issue (e.g., Fixes #123, Closes #456).\n   * Adds status/need-issue: If no linked issue is found, the bot will add the\n     status/need-issue label to your PR. This is a clear signal that an issue\n     needs to be created and linked.\n   * Synchronizes labels: If an issue is linked, the bot ensures the PR's labels\n     perfectly match the issue's labels. It will add any missing labels and\n     remove any that don't belong, and it will remove the status/need-issue\n     label if it was present.\n * What you should do:\n   * Always link your PR to an issue. This is the most important step. Add a\n     line like Resolves #<issue-number> to your PR description.\n   * This will ensure your PR is correctly categorized and moves through the\n     review process smoothly.\n\n\n4. Ongoing Triage for Issues: Scheduled Issue Triage#\n\nThis is a fallback workflow to ensure that no issue gets missed by the triage\nprocess.\n\n * Workflow File: .github/workflows/gemini-scheduled-issue-triage.yml\n * When it runs: Every hour on all open issues.\n * What it does:\n   * It actively seeks out issues that either have no labels at all or still\n     have the status/need-triage label.\n   * It then triggers the same powerful Gemini-based analysis as the initial\n     triage bot to apply the correct labels.\n * What you should do:\n   * You typically don't need to do anything. This workflow is a safety net to\n     ensure every issue is eventually categorized, even if the initial triage\n     fails.\n\n\n5. Release Automation#\n\nThis workflow handles the process of packaging and publishing new versions of\nthe VeCLI.\n\n * Workflow File: .github/workflows/release.yml\n * When it runs: On a daily schedule for \"nightly\" releases, and manually for\n   official patch/minor releases.\n * What it does:\n   * Automatically builds the project, bumps the version numbers, and publishes\n     the packages to npm.\n   * Creates a corresponding release on GitHub with generated release notes.\n * What you should do:\n   * As a contributor, you don't need to do anything for this process. You can\n     be confident that once your PR is merged into the main branch, your changes\n     will be included in the very next nightly release.\n\nWe hope this detailed overview is helpful. If you have any questions about our\nautomation or processes, please don't hesitate to ask!","routePath":"/en/issue-and-pr-automation","lang":"en","toc":[{"text":"Guiding Principle: Issues and Pull Requests","id":"guiding-principle-issues-and-pull-requests","depth":2,"charIndex":370},{"text":"Detailed Automation Workflows","id":"detailed-automation-workflows","depth":2,"charIndex":853},{"text":"1. When you open an Issue: `Automated Issue Triage`","id":"1-when-you-open-an-issue-automated-issue-triage","depth":3,"charIndex":-1},{"text":"2. When you open a Pull Request: `Continuous Integration (CI)`","id":"2-when-you-open-a-pull-request-continuous-integration-ci","depth":3,"charIndex":-1},{"text":"3. Ongoing Triage for Pull Requests: `PR Auditing and Label Sync`","id":"3-ongoing-triage-for-pull-requests-pr-auditing-and-label-sync","depth":3,"charIndex":-1},{"text":"4. Ongoing Triage for Issues: `Scheduled Issue Triage`","id":"4-ongoing-triage-for-issues-scheduled-issue-triage","depth":3,"charIndex":-1},{"text":"5. Release Automation","id":"5-release-automation","depth":3,"charIndex":5352}],"domain":"","frontmatter":{},"version":""},{"id":23,"title":"VeCLI Keyboard Shortcuts","content":"#\n\nThis document lists the available keyboard shortcuts in the VeCLI.\n\n\nGeneral#\n\nSHORTCUT   DESCRIPTION\nEsc        Close dialogs and suggestions.\nCtrl+C     Cancel the ongoing request and clear the input. Press twice\n           to exit the application.\nCtrl+D     Exit the application if the input is empty. Press twice to\n           confirm.\nCtrl+L     Clear the screen.\nCtrl+O     Toggle the display of the debug console.\nCtrl+S     Allows long responses to print fully, disabling truncation.\n           Use your terminal's scrollback to view the entire output.\nCtrl+T     Toggle the display of tool descriptions.\nCtrl+Y     Toggle auto-approval (YOLO mode) for all tool calls.\n\n\nInput Prompt#\n\nSHORTCUT                                       DESCRIPTION\n!                                              Toggle shell mode when the input is empty.\n\\ (at end of line) + Enter                     Insert a newline.\nDown Arrow                                     Navigate down through the input history.\nEnter                                          Submit the current prompt.\nMeta+Delete / Ctrl+Delete                      Delete the word to the right of the cursor.\nTab                                            Autocomplete the current suggestion if one exists.\nUp Arrow                                       Navigate up through the input history.\nCtrl+A / Home                                  Move the cursor to the beginning of the line.\nCtrl+B / Left Arrow                            Move the cursor one character to the left.\nCtrl+C                                         Clear the input prompt\nEsc (double press)                             Clear the input prompt.\nCtrl+D / Delete                                Delete the character to the right of the cursor.\nCtrl+E / End                                   Move the cursor to the end of the line.\nCtrl+F / Right Arrow                           Move the cursor one character to the right.\nCtrl+H / Backspace                             Delete the character to the left of the cursor.\nCtrl+K                                         Delete from the cursor to the end of the line.\nCtrl+Left Arrow / Meta+Left Arrow / Meta+B     Move the cursor one word to the left.\nCtrl+N                                         Navigate down through the input history.\nCtrl+P                                         Navigate up through the input history.\nCtrl+Right Arrow / Meta+Right Arrow / Meta+F   Move the cursor one word to the right.\nCtrl+U                                         Delete from the cursor to the beginning of the line.\nCtrl+V                                         Paste clipboard content. If the clipboard contains an image,\n                                               it will be saved and a reference to it will be inserted in\n                                               the prompt.\nCtrl+W / Meta+Backspace / Ctrl+Backspace       Delete the word to the left of the cursor.\nCtrl+X / Meta+Enter                            Open the current input in an external editor.\n\n\nSuggestions#\n\nSHORTCUT      DESCRIPTION\nDown Arrow    Navigate down through the suggestions.\nTab / Enter   Accept the selected suggestion.\nUp Arrow      Navigate up through the suggestions.\n\n\nRadio Button Select#\n\nSHORTCUT         DESCRIPTION\nDown Arrow / j   Move selection down.\nEnter            Confirm selection.\nUp Arrow / k     Move selection up.\n1-9              Select an item by its number.\n(multi-digit)    For items with numbers greater than 9, press the digits in\n                 quick succession to select the corresponding item.\n\n\nIDE Integration#\n\nSHORTCUT   DESCRIPTION\nCtrl+G     See context CLI received from IDE","routePath":"/en/keyboard-shortcuts","lang":"en","toc":[{"text":"General","id":"general","depth":2,"charIndex":71},{"text":"Input Prompt","id":"input-prompt","depth":2,"charIndex":682},{"text":"Suggestions","id":"suggestions","depth":2,"charIndex":3038},{"text":"Radio Button Select","id":"radio-button-select","depth":2,"charIndex":3230},{"text":"IDE Integration","id":"ide-integration","depth":2,"charIndex":3584}],"domain":"","frontmatter":{},"version":""},{"id":24,"title":"Package Overview","content":"#\n\nThis monorepo contains two main packages: @vecli/vecli and @vecli/vecli-core.\n\n\n@vecli/vecli#\n\nThis is the main package for the VeCLI. It is responsible for the user\ninterface, command parsing, and all other user-facing functionality.\n\nWhen this package is published, it is bundled into a single executable file.\nThis bundle includes all of the package's dependencies, including\n@vecli/vecli-core. This means that whether a user installs the package with npm\ninstall -g @vecli/vecli or runs it directly with npx @vecli/vecli, they are\nusing this single, self-contained executable.\n\n\n@vecli/vecli-core#\n\nThis package contains the core logic for interacting with the Volcano Engine\nAPI. It is responsible for making API requests, handling authentication, and\nmanaging the local cache.\n\nThis package is not bundled. When it is published, it is published as a standard\nNode.js package with its own dependencies. This allows it to be used as a\nstandalone package in other projects, if needed. All transpiled js code in the\ndist folder is included in the package.\n\n\nNPM Workspaces#\n\nThis project uses NPM Workspaces to manage the packages within this monorepo.\nThis simplifies development by allowing us to manage dependencies and run\nscripts across multiple packages from the root of the project.\n\n\nHow it Works#\n\nThe root package.json file defines the workspaces for this project:\n\n\n\nThis tells NPM that any folder inside the packages directory is a separate\npackage that should be managed as part of the workspace.\n\n\nBenefits of Workspaces#\n\n * Simplified Dependency Management: Running npm install from the root of the\n   project will install all dependencies for all packages in the workspace and\n   link them together. This means you don't need to run npm install in each\n   package's directory.\n * Automatic Linking: Packages within the workspace can depend on each other.\n   When you run npm install, NPM will automatically create symlinks between the\n   packages. This means that when you make changes to one package, the changes\n   are immediately available to other packages that depend on it.\n * Simplified Script Execution: You can run scripts in any package from the root\n   of the project using the --workspace flag. For example, to run the build\n   script in the cli package, you can run npm run build --workspace\n   @vecli/vecli.","routePath":"/en/npm","lang":"en","toc":[{"text":"`@vecli/vecli`","id":"veclivecli","depth":2,"charIndex":-1},{"text":"`@vecli/vecli-core`","id":"veclivecli-core","depth":2,"charIndex":-1},{"text":"NPM Workspaces","id":"npm-workspaces","depth":2,"charIndex":1062},{"text":"How it Works","id":"how-it-works","depth":3,"charIndex":1296},{"text":"Benefits of Workspaces","id":"benefits-of-workspaces","depth":3,"charIndex":1516}],"domain":"","frontmatter":{},"version":""},{"id":25,"title":"VeCLI: Quotas and Pricing","content":"#\n\nVeCLI offers a generous free tier that covers the use cases for many individual\ndevelopers. For enterprise / professional usage, or if you need higher limits,\nthere are multiple possible avenues depending on what type of account you use to\nauthenticate.\n\nSee privacy and terms for details on Privacy policy and Terms of Service.\n\nNote: published prices are list price; additional negotiated commercial\ndiscounting may apply.\n\nThis article outlines the specific quotas and pricing applicable to the VeCLI\nwhen using different authentication methods.\n\nGenerally, there are three categories to choose from:\n\n * Free Usage: Ideal for experimentation and light use.\n * Paid Tier (fixed price): For individual developers or enterprises who need\n   more generous daily quotas and predictable costs.\n * Pay-As-You-Go: The most flexible option for professional use, long-running\n   tasks, or when you need full control over your usage.\n\n\nFree Usage#\n\nYour journey begins with a generous free tier, perfect for experimentation and\nlight use.\n\nYour free usage limits depend on your authorization type.\n\n\nLog in with Google (Ve Code Assist for Individuals)#\n\nFor users who authenticate by using their Google account to access Ve Code\nAssist for individuals. This includes:\n\n * 1000 model requests / user / day\n * 60 model requests / user / minute\n * Model requests will be made across the Gemini model family as determined by\n   VeCLI.\n\nLearn more at Ve Code Assist for Individuals Limits.\n\n\nLog in with Gemini API Key (Unpaid)#\n\nIf you are using a Gemini API key, you can also benefit from a free tier. This\nincludes:\n\n * 250 model requests / user / day\n * 10 model requests / user / minute\n * Model requests to Flash model only.\n\nLearn more at Gemini API Rate Limits.\n\n\nLog in with Vertex AI (Express Mode)#\n\nVertex AI offers an Express Mode without the need to enable billing. This\nincludes:\n\n * 90 days before you need to enable billing.\n * Quotas and models are variable and specific to your account.\n\nLearn more at Vertex AI Express Mode Limits.\n\n\nPaid tier: Higher limits for a fixed cost#\n\nIf you use up your initial number of requests, you can upgrade your plan to\ncontinue to benefit from VeCLI by using the Standard or Enterprise editions of\nVe Code Assist by signing up here. Quotas and pricing are based on a fixed price\nsubscription with assigned license seats. For predictable costs, you can log in\nwith Google. This includes:\n\n * Standard:\n   * 1500 model requests / user / day\n   * 120 model requests / user / minute\n * Enterprise:\n   * 2000 model requests / user / day\n   * 120 model requests / user / minute\n * Model requests will be made across the Gemini model family as determined by\n   VeCLI.\n\nLearn more at Ve Code Assist Standard and Enterprise license Limits.\n\n\nPay As You Go#\n\nIf you hit your daily request limits or exhaust your Gemini Pro quota even after\nupgrading, the most flexible solution is to switch to a pay-as-you-go model,\nwhere you pay for the specific amount of processing you use. This is the\nrecommended path for uninterrupted access.\n\nTo do this, log in using a Gemini API key or Vertex AI.\n\n * Vertex AI (Regular Mode):\n   * Quota: Governed by a dynamic shared quota system or pre-purchased\n     provisioned throughput.\n   * Cost: Based on model and token usage.\n\nLearn more at Vertex AI Dynamic Shared Quota and Vertex AI Pricing.\n\n * Gemini API key:\n   * Quota: Varies by pricing tier.\n   * Cost: Varies by pricing tier and model/token usage.\n\nLearn more at Gemini API Rate Limits, Gemini API Pricing\n\nIt’s important to highlight that when using an API key, you pay per token/call.\nThis can be more expensive for many small calls with few tokens, but it's the\nonly way to ensure your workflow isn't interrupted by quota limits.\n\n\nGoogle One and Ultra plans, Gemini for Workspace plans#\n\nThese plans currently apply only to the use of Gemini web-based products\nprovided by Google-based experiences (for example, the Gemini web app or the\nFlow video editor). These plans do not apply to the API usage which powers the\nVeCLI. Supporting these plans is under active consideration for future support.\n\n\nTips to Avoid High Costs#\n\nWhen using a Pay as you Go API key, be mindful of your usage to avoid unexpected\ncosts.\n\n * Don't blindly accept every suggestion, especially for computationally\n   intensive tasks like refactoring large codebases.\n * Be intentional with your prompts and commands. You are paying per call, so\n   think about the most efficient way to get the job done.\n\n\nGemini API vs. Vertex#\n\n * Gemini API (gemini developer api): This is the fastest way to use the Gemini\n   models directly.\n * Vertex AI: This is the enterprise-grade platform for building, deploying, and\n   managing Gemini models with specific security and control requirements.\n\n\nUnderstanding your usage#\n\nA summary of model usage is available through the /stats command and presented\non exit at the end of a session.","routePath":"/en/quota-and-pricing","lang":"en","toc":[{"text":"Free Usage","id":"free-usage","depth":2,"charIndex":931},{"text":"Log in with Google (Ve Code Assist for Individuals)","id":"log-in-with-google-ve-code-assist-for-individuals","depth":3,"charIndex":1095},{"text":"Log in with Gemini API Key (Unpaid)","id":"log-in-with-gemini-api-key-unpaid","depth":3,"charIndex":1482},{"text":"Log in with Vertex AI (Express Mode)","id":"log-in-with-vertex-ai-express-mode","depth":3,"charIndex":1762},{"text":"Paid tier: Higher limits for a fixed cost","id":"paid-tier-higher-limits-for-a-fixed-cost","depth":2,"charIndex":2044},{"text":"Pay As You Go","id":"pay-as-you-go","depth":2,"charIndex":2778},{"text":"Google One and Ultra plans, Gemini for Workspace plans","id":"google-one-and-ultra-plans-gemini-for-workspace-plans","depth":2,"charIndex":3767},{"text":"Tips to Avoid High Costs","id":"tips-to-avoid-high-costs","depth":2,"charIndex":4135},{"text":"Gemini API vs. Vertex","id":"gemini-api-vs-vertex","depth":2,"charIndex":4516},{"text":"Understanding your usage","id":"understanding-your-usage","depth":2,"charIndex":4798}],"domain":"","frontmatter":{},"version":""},{"id":26,"title":"VeCLI Releases","content":"#\n\n\nRelease Cadence and Tags#\n\nA new version has been released.\n\n\nNightly#\n\n","routePath":"/en/releases","lang":"en","toc":[{"text":"Release Cadence and Tags","id":"release-cadence-and-tags","depth":2,"charIndex":3},{"text":"Nightly","id":"nightly","depth":3,"charIndex":65}],"domain":"","frontmatter":{},"version":""},{"id":27,"title":"Sandboxing in the VeCLI","content":"#\n\nThis document provides a guide to sandboxing in the VeCLI, including\nprerequisites, quickstart, and configuration.\n\n\nPrerequisites#\n\nBefore using sandboxing, you need to install and set up the VeCLI:\n\n\n\nTo verify the installation\n\n\n\n\nOverview of sandboxing#\n\nSandboxing isolates potentially dangerous operations (such as shell commands or\nfile modifications) from your host system, providing a security barrier between\nAI operations and your environment.\n\nThe benefits of sandboxing include:\n\n * Security: Prevent accidental system damage or data loss.\n * Isolation: Limit file system access to project directory.\n * Consistency: Ensure reproducible environments across different systems.\n * Safety: Reduce risk when working with untrusted code or experimental\n   commands.\n\n\nSandboxing methods#\n\nYour ideal method of sandboxing may differ depending on your platform and your\npreferred container solution.\n\n\n1. macOS Seatbelt (macOS only)#\n\nLightweight, built-in sandboxing using sandbox-exec.\n\nDefault profile: permissive-open - restricts writes outside project directory\nbut allows most other operations.\n\n\n2. Container-based (Docker/Podman)#\n\nCross-platform sandboxing with complete process isolation.\n\nNote: Requires building the sandbox image locally or using a published image\nfrom your organization's registry.\n\n\nQuickstart#\n\n\n\n\nConfiguration#\n\n\nEnable sandboxing (in order of precedence)#\n\n 1. Command flag: -s or --sandbox\n 2. Environment variable: GEMINI_SANDBOX=true|docker|podman|sandbox-exec\n 3. Settings file: \"sandbox\": true in the tools object of your settings.json\n    file (e.g., {\"tools\": {\"sandbox\": true}}).\n\n\nmacOS Seatbelt profiles#\n\nBuilt-in profiles (set via SEATBELT_PROFILE env var):\n\n * permissive-open (default): Write restrictions, network allowed\n * permissive-closed: Write restrictions, no network\n * permissive-proxied: Write restrictions, network via proxy\n * restrictive-open: Strict restrictions, network allowed\n * restrictive-closed: Maximum restrictions\n\n\nCustom Sandbox Flags#\n\nFor container-based sandboxing, you can inject custom flags into the docker or\npodman command using the SANDBOX_FLAGS environment variable. This is useful for\nadvanced configurations, such as disabling security features for specific use\ncases.\n\nExample (Podman):\n\nTo disable SELinux labeling for volume mounts, you can set the following:\n\n\n\nMultiple flags can be provided as a space-separated string:\n\n\n\n\nLinux UID/GID handling#\n\nThe sandbox automatically handles user permissions on Linux. Override these\npermissions with:\n\n\n\n\nTroubleshooting#\n\n\nCommon issues#\n\n\"Operation not permitted\"\n\n * Operation requires access outside sandbox.\n * Try more permissive profile or add mount points.\n\nMissing commands\n\n * Add to custom Dockerfile.\n * Install via sandbox.bashrc.\n\nNetwork issues\n\n * Check sandbox profile allows network.\n * Verify proxy configuration.\n\n\nDebug mode#\n\n\n\nNote: If you have DEBUG=true in a project's .env file, it won't affect\ngemini-cli due to automatic exclusion. Use .ve/.env files for gemini-cli\nspecific debug settings.\n\n\nInspect sandbox#\n\n\n\n\nSecurity notes#\n\n * Sandboxing reduces but doesn't eliminate all risks.\n * Use the most restrictive profile that allows your work.\n * Container overhead is minimal after first build.\n * GUI applications may not work in sandboxes.\n\n\nRelated documentation#\n\n * Configuration: Full configuration options.\n * Commands: Available commands.\n * Troubleshooting: General troubleshooting.","routePath":"/en/sandbox","lang":"en","toc":[{"text":"Prerequisites","id":"prerequisites","depth":2,"charIndex":119},{"text":"Overview of sandboxing","id":"overview-of-sandboxing","depth":2,"charIndex":236},{"text":"Sandboxing methods","id":"sandboxing-methods","depth":2,"charIndex":778},{"text":"1. macOS Seatbelt (macOS only)","id":"1-macos-seatbelt-macos-only","depth":3,"charIndex":910},{"text":"2. Container-based (Docker/Podman)","id":"2-container-based-dockerpodman","depth":3,"charIndex":1111},{"text":"Quickstart","id":"quickstart","depth":2,"charIndex":1322},{"text":"Configuration","id":"configuration","depth":2,"charIndex":1338},{"text":"Enable sandboxing (in order of precedence)","id":"enable-sandboxing-in-order-of-precedence","depth":3,"charIndex":1355},{"text":"macOS Seatbelt profiles","id":"macos-seatbelt-profiles","depth":3,"charIndex":1633},{"text":"Custom Sandbox Flags","id":"custom-sandbox-flags","depth":3,"charIndex":1998},{"text":"Linux UID/GID handling","id":"linux-uidgid-handling","depth":2,"charIndex":2426},{"text":"Troubleshooting","id":"troubleshooting","depth":2,"charIndex":2549},{"text":"Common issues","id":"common-issues","depth":3,"charIndex":2568},{"text":"Debug mode","id":"debug-mode","depth":3,"charIndex":2879},{"text":"Inspect sandbox","id":"inspect-sandbox","depth":3,"charIndex":3065},{"text":"Security notes","id":"security-notes","depth":2,"charIndex":3086},{"text":"Related documentation","id":"related-documentation","depth":2,"charIndex":3318}],"domain":"","frontmatter":{},"version":""},{"id":28,"title":"VeCLI Observability Guide","content":"#\n\nTelemetry provides data about VeCLI's performance, health, and usage. By\nenabling it, you can monitor operations, debug issues, and optimize tool usage\nthrough traces, metrics, and structured logs.\n\nVeCLI's telemetry system is built on the OpenTelemetry (OTEL) standard, allowing\nyou to send data to any compatible backend.\n\n\nEnabling telemetry#\n\nYou can enable telemetry in multiple ways. Configuration is primarily managed\nvia the .ve/settings.json file and environment variables, but CLI flags can\noverride these settings for a specific session.\n\n\nOrder of precedence#\n\nThe following lists the precedence for applying telemetry settings, with items\nlisted higher having greater precedence:\n\n 1. CLI flags (for vecli command):\n    \n    * --telemetry / --no-telemetry: Overrides telemetry.enabled.\n    * --telemetry-target <local|ve>: Overrides telemetry.target.\n    * --telemetry-otlp-endpoint <URL>: Overrides telemetry.otlpEndpoint.\n    * --telemetry-log-prompts / --no-telemetry-log-prompts: Overrides\n      telemetry.logPrompts.\n    * --telemetry-outfile <path>: Redirects telemetry output to a file. See\n      Exporting to a file.\n\n 2. Environment variables:\n    \n    * OTEL_EXPORTER_OTLP_ENDPOINT: Overrides telemetry.otlpEndpoint.\n\n 3. Workspace settings file (.ve/settings.json): Values from the telemetry\n    object in this project-specific file.\n\n 4. User settings file (~/.ve/settings.json): Values from the telemetry object\n    in this global user file.\n\n 5. Defaults: applied if not set by any of the above.\n    \n    * telemetry.enabled: false\n    * telemetry.target: local\n    * telemetry.otlpEndpoint: http://localhost:4317\n    * telemetry.logPrompts: true\n\nFor the npm run telemetry -- --target=<ve|local> script: The --target argument\nto this script only overrides the telemetry.target for the duration and purpose\nof that script (i.e., choosing which collector to start). It does not\npermanently change your settings.json. The script will first look at\nsettings.json for a telemetry.target to use as its default.\n\n\nExample settings#\n\nThe following code can be added to your workspace (.ve/settings.json) or user\n(~/.ve/settings.json) settings to enable telemetry and send the output to\nVolcano Engine:\n\n\n\n\nExporting to a file#\n\nYou can export all telemetry data to a file for local inspection.\n\nTo enable file export, use the --telemetry-outfile flag with a path to your\ndesired output file. This must be run using --telemetry-target=local.\n\n\n\n\nRunning an OTEL Collector#\n\nAn OTEL Collector is a service that receives, processes, and exports telemetry\ndata. The CLI can send data using either the OTLP/gRPC or OTLP/HTTP protocol.\nYou can specify which protocol to use via the --telemetry-otlp-protocol flag or\nthe telemetry.otlpProtocol setting in your settings.json file. See the\nconfiguration docs for more details.\n\nLearn more about OTEL exporter standard configuration in documentation.\n\n\nLocal#\n\nUse the npm run telemetry -- --target=local command to automate the process of\nsetting up a local telemetry pipeline, including configuring the necessary\nsettings in your .ve/settings.json file. The underlying script installs\notelcol-contrib (the OpenTelemetry Collector) and jaeger (The Jaeger UI for\nviewing traces). To use it:\n\n 1. Run the command: Execute the command from the root of the repository:\n    \n    \n    \n    The script will:\n    \n    * Download Jaeger and OTEL if needed.\n    * Start a local Jaeger instance.\n    * Start an OTEL collector configured to receive data from VeCLI.\n    * Automatically enable telemetry in your workspace settings.\n    * On exit, disable telemetry.\n\n 2. View traces: Open your web browser and navigate to http://localhost:16686 to\n    access the Jaeger UI. Here you can inspect detailed traces of VeCLI\n    operations.\n\n 3. Inspect logs and metrics: The script redirects the OTEL collector output\n    (which includes logs and metrics) to\n    ~/.ve/tmp/<projectHash>/otel/collector.log. The script will provide links to\n    view and a command to tail your telemetry data (traces, metrics, logs)\n    locally.\n\n 4. Stop the services: Press Ctrl+C in the terminal where the script is running\n    to stop the OTEL Collector and Jaeger services.\n\n\nVolcano Engine#\n\nUse the npm run telemetry -- --target=volc command to automate setting up a\nlocal OpenTelemetry collector that forwards data to your Volcano Engine project,\nincluding configuring the necessary settings in your .ve/settings.json file. The\nunderlying script installs otelcol-contrib. To use it:\n\n 1. Run the command: Execute the command from the root of the repository:\n    \n    \n    \n    The script will:\n    \n    * Download the otelcol-contrib binary if needed.\n    * Start an OTEL collector configured to receive data from VeCLI and export\n      it to your specified Volcano Engine project.\n    * Automatically enable telemetry and disable sandbox mode in your workspace\n      settings (.ve/settings.json).\n    * Provide direct links to view traces, metrics, and logs in your Volcano\n      Engine Console.\n    * On exit (Ctrl+C), it will attempt to restore your original telemetry and\n      sandbox settings.\n\n 2. Run VeCLI: In a separate terminal, run your VeCLI commands. This generates\n    telemetry data that the collector captures.\n\n 3. View telemetry in Volcano Engine: Use the links provided by the script to\n    navigate to the Volcano Engine Console and view your traces, metrics, and\n    logs.\n\n 4. Inspect local collector logs: The script redirects the local OTEL collector\n    output to ~/.ve/tmp/<projectHash>/otel/collector-ve.log. The script provides\n    links to view and command to tail your collector logs locally.\n\n 5. Stop the service: Press Ctrl+C in the terminal where the script is running\n    to stop the OTEL Collector.\n\n\nLogs and metric reference#\n\nThe following section describes the structure of logs and metrics generated for\nVeCLI.\n\n * A sessionId is included as a common attribute on all logs and metrics.\n\n\nLogs#\n\nLogs are timestamped records of specific events. The following events are logged\nfor VeCLI:\n\n * vecli.config: This event occurs once at startup with the CLI's configuration.\n   \n   * Attributes:\n     * model (string)\n     * embedding_model (string)\n     * sandbox_enabled (boolean)\n     * core_tools_enabled (string)\n     * approval_mode (string)\n     * api_key_enabled (boolean)\n     * vertex_ai_enabled (boolean)\n     * code_assist_enabled (boolean)\n     * log_prompts_enabled (boolean)\n     * file_filtering_respect_git_ignore (boolean)\n     * debug_mode (boolean)\n     * mcp_servers (string)\n\n * vecli.user_prompt: This event occurs when a user submits a prompt.\n   \n   * Attributes:\n     * prompt_length (int)\n     * prompt_id (string)\n     * prompt (string, this attribute is excluded if log_prompts_enabled is\n       configured to be false)\n     * auth_type (string)\n\n * vecli.tool_call: This event occurs for each function call.\n   \n   * Attributes:\n     * function_name\n     * function_args\n     * duration_ms\n     * success (boolean)\n     * decision (string: \"accept\", \"reject\", \"auto_accept\", or \"modify\", if\n       applicable)\n     * error (if applicable)\n     * error_type (if applicable)\n     * metadata (if applicable, dictionary of string -> any)\n\n * vecli.file_operation: This event occurs for each file operation.\n   \n   * Attributes:\n     * tool_name (string)\n     * operation (string: \"create\", \"read\", \"update\")\n     * lines (int, if applicable)\n     * mimetype (string, if applicable)\n     * extension (string, if applicable)\n     * programming_language (string, if applicable)\n     * diff_stat (json string, if applicable): A JSON string with the following\n       members:\n       * ai_added_lines (int)\n       * ai_removed_lines (int)\n       * user_added_lines (int)\n       * user_removed_lines (int)\n\n * vecli.api_request: This event occurs when making a request to volcano Engine\n   API.\n   \n   * Attributes:\n     * model\n     * request_text (if applicable)\n\n * vecli.api_error: This event occurs if the API request fails.\n   \n   * Attributes:\n     * model\n     * error\n     * error_type\n     * status_code\n     * duration_ms\n     * auth_type\n\n * vecli.api_response: This event occurs upon receiving a response from vecli\n   API.\n   \n   * Attributes:\n     * model\n     * status_code\n     * duration_ms\n     * error (optional)\n     * input_token_count\n     * output_token_count\n     * cached_content_token_count\n     * thoughts_token_count\n     * tool_token_count\n     * response_text (if applicable)\n     * auth_type\n\n * vecli.malformed_json_response: This event occurs when a generateJson response\n   from vecli API cannot be parsed as a json.\n   \n   * Attributes:\n     * model\n\n * vecli.flash_fallback: This event occurs when VeCLI switches to flash as\n   fallback.\n   \n   * Attributes:\n     * auth_type\n\n * vecli.slash_command: This event occurs when a user executes a slash command.\n   \n   * Attributes:\n     * command (string)\n     * subcommand (string, if applicable)\n\n\nMetrics#\n\nMetrics are numerical measurements of behavior over time. The following metrics\nare collected for VeCLI:\n\n * vecli.session.count (Counter, Int): Incremented once per CLI startup.\n\n * vecli.tool.call.count (Counter, Int): Counts tool calls.\n   \n   * Attributes:\n     * function_name\n     * success (boolean)\n     * decision (string: \"accept\", \"reject\", or \"modify\", if applicable)\n     * tool_type (string: \"mcp\", or \"native\", if applicable)\n\n * vecli.tool.call.latency (Histogram, ms): Measures tool call latency.\n   \n   * Attributes:\n     * function_name\n     * decision (string: \"accept\", \"reject\", or \"modify\", if applicable)\n\n * vecli.api.request.count (Counter, Int): Counts all API requests.\n   \n   * Attributes:\n     * model\n     * status_code\n     * error_type (if applicable)\n\n * vecli.api.request.latency (Histogram, ms): Measures API request latency.\n   \n   * Attributes:\n     * model\n\n * gemini_cli.token.usage (Counter, Int): Counts the number of tokens used.\n   \n   * Attributes:\n     * model\n     * type (string: \"input\", \"output\", \"thought\", \"cache\", or \"tool\")\n\n * gemini_cli.file.operation.count (Counter, Int): Counts file operations.\n   \n   * Attributes:\n     * operation (string: \"create\", \"read\", \"update\"): The type of file\n       operation.\n     * lines (Int, if applicable): Number of lines in the file.\n     * mimetype (string, if applicable): Mimetype of the file.\n     * extension (string, if applicable): File extension of the file.\n     * model_added_lines (Int, if applicable): Number of lines added/changed by\n       the model.\n     * model_removed_lines (Int, if applicable): Number of lines removed/changed\n       by the model.\n     * user_added_lines (Int, if applicable): Number of lines added/changed by\n       user in AI proposed changes.\n     * user_removed_lines (Int, if applicable): Number of lines removed/changed\n       by user in AI proposed changes.\n     * programming_language (string, if applicable): The programming language of\n       the file.\n\n * gemini_cli.chat_compression (Counter, Int): Counts chat compression\n   operations\n   \n   * Attributes:\n     * tokens_before: (Int): Number of tokens in context prior to compression\n     * tokens_after: (Int): Number of tokens in context after compression","routePath":"/en/telemetry","lang":"en","toc":[{"text":"Enabling telemetry","id":"enabling-telemetry","depth":2,"charIndex":328},{"text":"Order of precedence","id":"order-of-precedence","depth":3,"charIndex":553},{"text":"Example settings","id":"example-settings","depth":3,"charIndex":2037},{"text":"Exporting to a file","id":"exporting-to-a-file","depth":3,"charIndex":2228},{"text":"Running an OTEL Collector","id":"running-an-otel-collector","depth":2,"charIndex":2467},{"text":"Local","id":"local","depth":3,"charIndex":2915},{"text":"Volcano Engine","id":"volcano-engine","depth":3,"charIndex":4209},{"text":"Logs and metric reference","id":"logs-and-metric-reference","depth":2,"charIndex":5774},{"text":"Logs","id":"logs","depth":3,"charIndex":5966},{"text":"Metrics","id":"metrics","depth":3,"charIndex":8974}],"domain":"","frontmatter":{},"version":""},{"id":29,"title":"VeCLI file system tools","content":"#\n\nThe VeCLI provides a comprehensive suite of tools for interacting with the local\nfile system. These tools allow the Volcano Engine model to read from, write to,\nlist, search, and modify files and directories, all under your control and\ntypically with confirmation for sensitive operations.\n\nNote: All file system tools operate within a rootDirectory (usually the current\nworking directory where you launched the CLI) for security. Paths that you\nprovide to these tools are generally expected to be absolute or are resolved\nrelative to this root directory.\n\n\n1. list_directory (ReadFolder)#\n\nlist_directory lists the names of files and subdirectories directly within a\nspecified directory path. It can optionally ignore entries matching provided\nglob patterns.\n\n * Tool name: list_directory\n * Display name: ReadFolder\n * File: ls.ts\n * Parameters:\n   * path (string, required): The absolute path to the directory to list.\n   * ignore (array of strings, optional): A list of glob patterns to exclude\n     from the listing (e.g., [\"*.log\", \".git\"]).\n   * respect_git_ignore (boolean, optional): Whether to respect .gitignore\n     patterns when listing files. Defaults to true.\n * Behavior:\n   * Returns a list of file and directory names.\n   * Indicates whether each entry is a directory.\n   * Sorts entries with directories first, then alphabetically.\n * Output (llmContent): A string like: Directory listing for\n   /path/to/your/folder:\\n[DIR] subfolder1\\nfile1.txt\\nfile2.png\n * Confirmation: No.\n\n\n2. read_file (ReadFile)#\n\nread_file reads and returns the content of a specified file. This tool handles\ntext, images (PNG, JPG, GIF, WEBP, SVG, BMP), and PDF files. For text files, it\ncan read specific line ranges. Other binary file types are generally skipped.\n\n * Tool name: read_file\n * Display name: ReadFile\n * File: read-file.ts\n * Parameters:\n   * path (string, required): The absolute path to the file to read.\n   * offset (number, optional): For text files, the 0-based line number to start\n     reading from. Requires limit to be set.\n   * limit (number, optional): For text files, the maximum number of lines to\n     read. If omitted, reads a default maximum (e.g., 2000 lines) or the entire\n     file if feasible.\n * Behavior:\n   * For text files: Returns the content. If offset and limit are used, returns\n     only that slice of lines. Indicates if content was truncated due to line\n     limits or line length limits.\n   * For image and PDF files: Returns the file content as a base64-encoded data\n     structure suitable for model consumption.\n   * For other binary files: Attempts to identify and skip them, returning a\n     message indicating it's a generic binary file.\n * Output: (llmContent):\n   * For text files: The file content, potentially prefixed with a truncation\n     message (e.g., [File content truncated: showing lines 1-100 of 500 total\n     lines...]\\nActual file content...).\n   * For image/PDF files: An object containing inlineData with mimeType and\n     base64 data (e.g., { inlineData: { mimeType: 'image/png', data:\n     'base64encodedstring' } }).\n   * For other binary files: A message like Cannot display content of binary\n     file: /path/to/data.bin.\n * Confirmation: No.\n\n\n3. write_file (WriteFile)#\n\nwrite_file writes content to a specified file. If the file exists, it will be\noverwritten. If the file doesn't exist, it (and any necessary parent\ndirectories) will be created.\n\n * Tool name: write_file\n * Display name: WriteFile\n * File: write-file.ts\n * Parameters:\n   * file_path (string, required): The absolute path to the file to write to.\n   * content (string, required): The content to write into the file.\n * Behavior:\n   * Writes the provided content to the file_path.\n   * Creates parent directories if they don't exist.\n * Output (llmContent): A success message, e.g., Successfully overwrote file:\n   /path/to/your/file.txt or Successfully created and wrote to new file:\n   /path/to/new/file.txt.\n * Confirmation: Yes. Shows a diff of changes and asks for user approval before\n   writing.\n\n\n4. glob (FindFiles)#\n\nglob finds files matching specific glob patterns (e.g., src/**/*.ts, *.md),\nreturning absolute paths sorted by modification time (newest first).\n\n * Tool name: glob\n * Display name: FindFiles\n * File: glob.ts\n * Parameters:\n   * pattern (string, required): The glob pattern to match against (e.g.,\n     \"*.py\", \"src/**/*.js\").\n   * path (string, optional): The absolute path to the directory to search\n     within. If omitted, searches the tool's root directory.\n   * case_sensitive (boolean, optional): Whether the search should be\n     case-sensitive. Defaults to false.\n   * respect_git_ignore (boolean, optional): Whether to respect .gitignore\n     patterns when finding files. Defaults to true.\n * Behavior:\n   * Searches for files matching the glob pattern within the specified\n     directory.\n   * Returns a list of absolute paths, sorted with the most recently modified\n     files first.\n   * Ignores common nuisance directories like node_modules and .git by default.\n * Output (llmContent): A message like: Found 5 file(s) matching \"*.ts\" within\n   src, sorted by modification time (newest\n   first):\\nsrc/file1.ts\\nsrc/subdir/file2.ts...\n * Confirmation: No.\n\n\n5. search_file_content (SearchText)#\n\nsearch_file_content searches for a regular expression pattern within the content\nof files in a specified directory. Can filter files by a glob pattern. Returns\nthe lines containing matches, along with their file paths and line numbers.\n\n * Tool name: search_file_content\n * Display name: SearchText\n * File: grep.ts\n * Parameters:\n   * pattern (string, required): The regular expression (regex) to search for\n     (e.g., \"function\\s+myFunction\").\n   * path (string, optional): The absolute path to the directory to search\n     within. Defaults to the current working directory.\n   * include (string, optional): A glob pattern to filter which files are\n     searched (e.g., \"*.js\", \"src/**/*.{ts,tsx}\"). If omitted, searches most\n     files (respecting common ignores).\n * Behavior:\n   * Uses git grep if available in a Git repository for speed; otherwise, falls\n     back to system grep or a JavaScript-based search.\n   * Returns a list of matching lines, each prefixed with its file path\n     (relative to the search directory) and line number.\n * Output (llmContent): A formatted string of matches, e.g.:\n   \n   \n\n * Confirmation: No.\n\n\n6. replace (Edit)#\n\nreplace replaces text within a file. By default, replaces a single occurrence,\nbut can replace multiple occurrences when expected_replacements is specified.\nThis tool is designed for precise, targeted changes and requires significant\ncontext around the old_string to ensure it modifies the correct location.\n\n * Tool name: replace\n\n * Display name: Edit\n\n * File: edit.ts\n\n * Parameters:\n   \n   * file_path (string, required): The absolute path to the file to modify.\n   \n   * old_string (string, required): The exact literal text to replace.\n     \n     CRITICAL: This string must uniquely identify the single instance to change.\n     It should include at least 3 lines of context before and after the target\n     text, matching whitespace and indentation precisely. If old_string is\n     empty, the tool attempts to create a new file at file_path with new_string\n     as content.\n   \n   * new_string (string, required): The exact literal text to replace old_string\n     with.\n   \n   * expected_replacements (number, optional): The number of occurrences to\n     replace. Defaults to 1.\n\n * Behavior:\n   \n   * If old_string is empty and file_path does not exist, creates a new file\n     with new_string as content.\n   * If old_string is provided, it reads the file_path and attempts to find\n     exactly one occurrence of old_string.\n   * If one occurrence is found, it replaces it with new_string.\n   * Enhanced Reliability (Multi-Stage Edit Correction): To significantly\n     improve the success rate of edits, especially when the model-provided\n     old_string might not be perfectly precise, the tool incorporates a\n     multi-stage edit correction mechanism.\n     * If the initial old_string isn't found or matches multiple locations, the\n       tool can leverage the Volcano Engine model to iteratively refine\n       old_string (and potentially new_string).\n     * This self-correction process attempts to identify the unique segment the\n       model intended to modify, making the replace operation more robust even\n       with slightly imperfect initial context.\n\n * Failure conditions: Despite the correction mechanism, the tool will fail if:\n   \n   * file_path is not absolute or is outside the root directory.\n   * old_string is not empty, but the file_path does not exist.\n   * old_string is empty, but the file_path already exists.\n   * old_string is not found in the file after attempts to correct it.\n   * old_string is found multiple times, and the self-correction mechanism\n     cannot resolve it to a single, unambiguous match.\n\n * Output (llmContent):\n   \n   * On success: Successfully modified file: /path/to/file.txt (1 replacements).\n     or Created new file: /path/to/new_file.txt with provided content.\n   * On failure: An error message explaining the reason (e.g., Failed to edit, 0\n     occurrences found..., Failed to edit, expected 1 occurrences but found\n     2...).\n\n * Confirmation: Yes. Shows a diff of the proposed changes and asks for user\n   approval before writing to the file.\n\nThese file system tools provide a foundation for the VeCLI to understand and\ninteract with your local project context.","routePath":"/en/tools/file-system","lang":"en","toc":[{"text":"1. `list_directory` (ReadFolder)","id":"1-list_directory-readfolder","depth":2,"charIndex":-1},{"text":"2. `read_file` (ReadFile)","id":"2-read_file-readfile","depth":2,"charIndex":-1},{"text":"3. `write_file` (WriteFile)","id":"3-write_file-writefile","depth":2,"charIndex":-1},{"text":"4. `glob` (FindFiles)","id":"4-glob-findfiles","depth":2,"charIndex":-1},{"text":"5. `search_file_content` (SearchText)","id":"5-search_file_content-searchtext","depth":2,"charIndex":-1},{"text":"6. `replace` (Edit)","id":"6-replace-edit","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":30,"title":"VeCLI tools","content":"#\n\nThe VeCLI includes built-in tools that the Volcano Engine model uses to interact\nwith your local environment, access information, and perform actions. These\ntools enhance the CLI's capabilities, enabling it to go beyond text generation\nand assist with a wide range of tasks.\n\n\nOverview of VeCLI tools#\n\nIn the context of the VeCLI, tools are specific functions or modules that the\nVolcano Engine model can request to be executed. For example, if you ask Volcano\nEngine to \"Summarize the contents of my_document.txt,\" the model will likely\nidentify the need to read that file and will request the execution of the\nread_file tool.\n\nThe core component (packages/core) manages these tools, presents their\ndefinitions (schemas) to the Volcano Engine model, executes them when requested,\nand returns the results to the model for further processing into a user-facing\nresponse.\n\nThese tools provide the following capabilities:\n\n * Access local information: Tools allow Volcano Engine to access your local\n   file system, read file contents, list directories, etc.\n * Execute commands: With tools like run_shell_command, Volcano Engine can run\n   shell commands (with appropriate safety measures and user confirmation).\n * Interact with the web: Tools can fetch content from URLs.\n * Take actions: Tools can modify files, write new files, or perform other\n   actions on your system (again, typically with safeguards).\n * Ground responses: By using tools to fetch real-time or specific local data,\n   Volcano Engine's responses can be more accurate, relevant, and grounded in\n   your actual context.\n\n\nHow to use VeCLI tools#\n\nTo use VeCLI tools, provide a prompt to the VeCLI. The process works as follows:\n\n 1. You provide a prompt to the VeCLI.\n 2. The CLI sends the prompt to the core.\n 3. The core, along with your prompt and conversation history, sends a list of\n    available tools and their descriptions/schemas to the Volcano Engine API.\n 4. The Volcano Engine model analyzes your request. If it determines that a tool\n    is needed, its response will include a request to execute a specific tool\n    with certain parameters.\n 5. The core receives this tool request, validates it, and (often after user\n    confirmation for sensitive operations) executes the tool.\n 6. The output from the tool is sent back to the Volcano Engine model.\n 7. The Volcano Engine model uses the tool's output to formulate its final\n    answer, which is then sent back through the core to the CLI and displayed to\n    you.\n\nYou will typically see messages in the CLI indicating when a tool is being\ncalled and whether it succeeded or failed.\n\n\nSecurity and confirmation#\n\nMany tools, especially those that can modify your file system or execute\ncommands (write_file, edit, run_shell_command), are designed with safety in\nmind. The VeCLI will typically:\n\n * Require confirmation: Prompt you before executing potentially sensitive\n   operations, showing you what action is about to be taken.\n * Utilize sandboxing: All tools are subject to restrictions enforced by\n   sandboxing (see Sandboxing in the VeCLI). This means that when operating in a\n   sandbox, any tools (including MCP servers) you wish to use must be available\n   inside the sandbox environment. For example, to run an MCP server through\n   npx, the npx executable must be installed within the sandbox's Docker image\n   or be available in the sandbox-exec environment.\n\nIt's important to always review confirmation prompts carefully before allowing a\ntool to proceed.\n\n\nLearn more about VeCLI's tools#\n\nVeCLI's built-in tools can be broadly categorized as follows:\n\n * File System Tools: For interacting with files and directories (reading,\n   writing, listing, searching, etc.).\n * Shell Tool (run_shell_command): For executing shell commands.\n * Web Fetch Tool (web_fetch): For retrieving content from URLs.\n * Web Search Tool (web_search): For searching the web.\n * Multi-File Read Tool (read_many_files): A specialized tool for reading\n   content from multiple files or directories, often used by the @ command.\n * Memory Tool (save_memory): For saving and recalling information across\n   sessions.\n\nAdditionally, these tools incorporate:\n\n * MCP servers: MCP servers act as a bridge between the Volcano Engine model and\n   your local environment or other services like APIs.\n * Sandboxing: Sandboxing isolates the model and its changes from your\n   environment to reduce potential risk.","routePath":"/en/tools/","lang":"en","toc":[{"text":"Overview of VeCLI tools","id":"overview-of-vecli-tools","depth":2,"charIndex":279},{"text":"How to use VeCLI tools","id":"how-to-use-vecli-tools","depth":2,"charIndex":1595},{"text":"Security and confirmation","id":"security-and-confirmation","depth":2,"charIndex":2624},{"text":"Learn more about VeCLI's tools","id":"learn-more-about-veclis-tools","depth":2,"charIndex":3513}],"domain":"","frontmatter":{},"version":""},{"id":31,"title":"MCP servers with the VeCLI","content":"#\n\nThis document provides a guide to configuring and using Model Context Protocol\n(MCP) servers with the VeCLI.\n\n\nWhat is an MCP server?#\n\nAn MCP server is an application that exposes tools and resources to the VeCLI\nthrough the Model Context Protocol, allowing it to interact with external\nsystems and data sources. MCP servers act as a bridge between the Gemini model\nand your local environment or other services like APIs.\n\nAn MCP server enables the VeCLI to:\n\n * Discover tools: List available tools, their descriptions, and parameters\n   through standardized schema definitions.\n * Execute tools: Call specific tools with defined arguments and receive\n   structured responses.\n * Access resources: Read data from specific resources (though the VeCLI\n   primarily focuses on tool execution).\n\nWith an MCP server, you can extend the VeCLI's capabilities to perform actions\nbeyond its built-in features, such as interacting with databases, APIs, custom\nscripts, or specialized workflows.\n\n\nCore Integration Architecture#\n\nThe VeCLI integrates with MCP servers through a sophisticated discovery and\nexecution system built into the core package (packages/core/src/tools/):\n\n\nDiscovery Layer (mcp-client.ts)#\n\nThe discovery process is orchestrated by discoverMcpTools(), which:\n\n 1. Iterates through configured servers from your settings.json mcpServers\n    configuration\n 2. Establishes connections using appropriate transport mechanisms (Stdio, SSE,\n    or Streamable HTTP)\n 3. Fetches tool definitions from each server using the MCP protocol\n 4. Sanitizes and validates tool schemas for compatibility with the Gemini API\n 5. Registers tools in the global tool registry with conflict resolution\n\n\nExecution Layer (mcp-tool.ts)#\n\nEach discovered MCP tool is wrapped in a DiscoveredMCPTool instance that:\n\n * Handles confirmation logic based on server trust settings and user\n   preferences\n * Manages tool execution by calling the MCP server with proper parameters\n * Processes responses for both the LLM context and user display\n * Maintains connection state and handles timeouts\n\n\nTransport Mechanisms#\n\nThe VeCLI supports three MCP transport types:\n\n * Stdio Transport: Spawns a subprocess and communicates via stdin/stdout\n * SSE Transport: Connects to Server-Sent Events endpoints\n * Streamable HTTP Transport: Uses HTTP streaming for communication\n\n\nHow to set up your MCP server#\n\nThe VeCLI uses the mcpServers configuration in your settings.json file to locate\nand connect to MCP servers. This configuration supports multiple servers with\ndifferent transport mechanisms.\n\n\nConfigure the MCP server in settings.json#\n\nYou can configure MCP servers in your settings.json file in two main ways:\nthrough the top-level mcpServers object for specific server definitions, and\nthrough the mcp object for global settings that control server discovery and\nexecution.\n\nGlobal MCP Settings (mcp)#\n\nThe mcp object in your settings.json allows you to define global rules for all\nMCP servers.\n\n * mcp.serverCommand (string): A global command to start an MCP server.\n * mcp.allowed (array of strings): A list of MCP server names to allow. If this\n   is set, only servers from this list (matching the keys in the mcpServers\n   object) will be connected to.\n * mcp.excluded (array of strings): A list of MCP server names to exclude.\n   Servers in this list will not be connected to.\n\nExample:\n\n\n\nServer-Specific Configuration (mcpServers)#\n\nThe mcpServers object is where you define each individual MCP server you want\nthe CLI to connect to.\n\n\nConfiguration Structure#\n\nAdd an mcpServers object to your settings.json file:\n\n\n\n\nConfiguration Properties#\n\nEach server configuration supports the following properties:\n\nRequired (one of the following)#\n\n * command (string): Path to the executable for Stdio transport\n * url (string): SSE endpoint URL (e.g., \"http://localhost:8080/sse\")\n * httpUrl (string): HTTP streaming endpoint URL\n\nOptional#\n\n * args (string[]): Command-line arguments for Stdio transport\n * headers (object): Custom HTTP headers when using url or httpUrl\n * env (object): Environment variables for the server process. Values can\n   reference environment variables using $VAR_NAME or ${VAR_NAME} syntax\n * cwd (string): Working directory for Stdio transport\n * timeout (number): Request timeout in milliseconds (default: 600,000ms = 10\n   minutes)\n * trust (boolean): When true, bypasses all tool call confirmations for this\n   server (default: false)\n * includeTools (string[]): List of tool names to include from this MCP server.\n   When specified, only the tools listed here will be available from this server\n   (allowlist behavior). If not specified, all tools from the server are enabled\n   by default.\n * excludeTools (string[]): List of tool names to exclude from this MCP server.\n   Tools listed here will not be available to the model, even if they are\n   exposed by the server. Note: excludeTools takes precedence over includeTools\n   - if a tool is in both lists, it will be excluded.\n\n\nOAuth Support for Remote MCP Servers#\n\nThe VeCLI supports OAuth 2.0 authentication for remote MCP servers using SSE or\nHTTP transports. This enables secure access to MCP servers that require\nauthentication.\n\nAutomatic OAuth Discovery#\n\nFor servers that support OAuth discovery, you can omit the OAuth configuration\nand let the CLI discover it automatically:\n\n\n\nThe CLI will automatically:\n\n * Detect when a server requires OAuth authentication (401 responses)\n * Discover OAuth endpoints from server metadata\n * Perform dynamic client registration if supported\n * Handle the OAuth flow and token management\n\nAuthentication Flow#\n\nWhen connecting to an OAuth-enabled server:\n\n 1. Initial connection attempt fails with 401 Unauthorized\n 2. OAuth discovery finds authorization and token endpoints\n 3. Browser opens for user authentication (requires local browser access)\n 4. Authorization code is exchanged for access tokens\n 5. Tokens are stored securely for future use\n 6. Connection retry succeeds with valid tokens\n\nBrowser Redirect Requirements#\n\nImportant: OAuth authentication requires that your local machine can:\n\n * Open a web browser for authentication\n * Receive redirects on http://localhost:7777/oauth/callback\n\nThis feature will not work in:\n\n * Headless environments without browser access\n * Remote SSH sessions without X11 forwarding\n * Containerized environments without browser support\n\nManaging OAuth Authentication#\n\nUse the /mcp auth command to manage OAuth authentication:\n\n\n\nOAuth Configuration Properties#\n\n * enabled (boolean): Enable OAuth for this server\n * clientId (string): OAuth client identifier (optional with dynamic\n   registration)\n * clientSecret (string): OAuth client secret (optional for public clients)\n * authorizationUrl (string): OAuth authorization endpoint (auto-discovered if\n   omitted)\n * tokenUrl (string): OAuth token endpoint (auto-discovered if omitted)\n * scopes (string[]): Required OAuth scopes\n * redirectUri (string): Custom redirect URI (defaults to\n   http://localhost:7777/oauth/callback)\n * tokenParamName (string): Query parameter name for tokens in SSE URLs\n * audiences (string[]): Audiences the token is valid for\n\nToken Management#\n\nOAuth tokens are automatically:\n\n * Stored securely in ~/.ve/mcp-oauth-tokens.json\n * Refreshed when expired (if refresh tokens are available)\n * Validated before each connection attempt\n * Cleaned up when invalid or expired\n\nAuthentication Provider Type#\n\nYou can specify the authentication provider type using the authProviderType\nproperty:\n\n * authProviderType (string): Specifies the authentication provider. Can be one\n   of the following:\n   * dynamic_discovery (default): The CLI will automatically discover the OAuth\n     configuration from the server.\n   * google_credentials: The CLI will use the Google Application Default\n     Credentials (ADC) to authenticate with the server. When using this\n     provider, you must specify the required scopes.\n\n\n\n\nExample Configurations#\n\nPython MCP Server (Stdio)#\n\n\n\nNode.js MCP Server (Stdio)#\n\n\n\nDocker-based MCP Server#\n\n\n\nHTTP-based MCP Server#\n\n\n\nHTTP-based MCP Server with Custom Headers#\n\n\n\nMCP Server with Tool Filtering#\n\n\n\n\nDiscovery Process Deep Dive#\n\nWhen the VeCLI starts, it performs MCP server discovery through the following\ndetailed process:\n\n\n1. Server Iteration and Connection#\n\nFor each configured server in mcpServers:\n\n 1. Status tracking begins: Server status is set to CONNECTING\n 2. Transport selection: Based on configuration properties:\n    * httpUrl → StreamableHTTPClientTransport\n    * url → SSEClientTransport\n    * command → StdioClientTransport\n 3. Connection establishment: The MCP client attempts to connect with the\n    configured timeout\n 4. Error handling: Connection failures are logged and the server status is set\n    to DISCONNECTED\n\n\n2. Tool Discovery#\n\nUpon successful connection:\n\n 1. Tool listing: The client calls the MCP server's tool listing endpoint\n 2. Schema validation: Each tool's function declaration is validated\n 3. Tool filtering: Tools are filtered based on includeTools and excludeTools\n    configuration\n 4. Name sanitization: Tool names are cleaned to meet Gemini API requirements:\n    * Invalid characters (non-alphanumeric, underscore, dot, hyphen) are\n      replaced with underscores\n    * Names longer than 63 characters are truncated with middle replacement\n      (___)\n\n\n3. Conflict Resolution#\n\nWhen multiple servers expose tools with the same name:\n\n 1. First registration wins: The first server to register a tool name gets the\n    unprefixed name\n 2. Automatic prefixing: Subsequent servers get prefixed names:\n    serverName__toolName\n 3. Registry tracking: The tool registry maintains mappings between server names\n    and their tools\n\n\n4. Schema Processing#\n\nTool parameter schemas undergo sanitization for Gemini API compatibility:\n\n * $schema properties are removed\n * additionalProperties are stripped\n * anyOf with default have their default values removed (Vertex AI\n   compatibility)\n * Recursive processing applies to nested schemas\n\n\n5. Connection Management#\n\nAfter discovery:\n\n * Persistent connections: Servers that successfully register tools maintain\n   their connections\n * Cleanup: Servers that provide no usable tools have their connections closed\n * Status updates: Final server statuses are set to CONNECTED or DISCONNECTED\n\n\nTool Execution Flow#\n\nWhen the Gemini model decides to use an MCP tool, the following execution flow\noccurs:\n\n\n1. Tool Invocation#\n\nThe model generates a FunctionCall with:\n\n * Tool name: The registered name (potentially prefixed)\n * Arguments: JSON object matching the tool's parameter schema\n\n\n2. Confirmation Process#\n\nEach DiscoveredMCPTool implements sophisticated confirmation logic:\n\nTrust-based Bypass#\n\n\n\nDynamic Allow-listing#\n\nThe system maintains internal allow-lists for:\n\n * Server-level: serverName → All tools from this server are trusted\n * Tool-level: serverName.toolName → This specific tool is trusted\n\nUser Choice Handling#\n\nWhen confirmation is required, users can choose:\n\n * Proceed once: Execute this time only\n * Always allow this tool: Add to tool-level allow-list\n * Always allow this server: Add to server-level allow-list\n * Cancel: Abort execution\n\n\n3. Execution#\n\nUpon confirmation (or trust bypass):\n\n 1. Parameter preparation: Arguments are validated against the tool's schema\n\n 2. MCP call: The underlying CallableTool invokes the server with:\n    \n    \n\n 3. Response processing: Results are formatted for both LLM context and user\n    display\n\n\n4. Response Handling#\n\nThe execution result contains:\n\n * llmContent: Raw response parts for the language model's context\n * returnDisplay: Formatted output for user display (often JSON in markdown code\n   blocks)\n\n\nHow to interact with your MCP server#\n\n\nUsing the /mcp Command#\n\nThe /mcp command provides comprehensive information about your MCP server setup:\n\n\n\nThis displays:\n\n * Server list: All configured MCP servers\n * Connection status: CONNECTED, CONNECTING, or DISCONNECTED\n * Server details: Configuration summary (excluding sensitive data)\n * Available tools: List of tools from each server with descriptions\n * Discovery state: Overall discovery process status\n\n\nExample /mcp Output#\n\n\n\n\nTool Usage#\n\nOnce discovered, MCP tools are available to the Gemini model like built-in\ntools. The model will automatically:\n\n 1. Select appropriate tools based on your requests\n 2. Present confirmation dialogs (unless the server is trusted)\n 3. Execute tools with proper parameters\n 4. Display results in a user-friendly format\n\n\nStatus Monitoring and Troubleshooting#\n\n\nConnection States#\n\nThe MCP integration tracks several states:\n\nServer Status (MCPServerStatus)#\n\n * DISCONNECTED: Server is not connected or has errors\n * CONNECTING: Connection attempt in progress\n * CONNECTED: Server is connected and ready\n\nDiscovery State (MCPDiscoveryState)#\n\n * NOT_STARTED: Discovery hasn't begun\n * IN_PROGRESS: Currently discovering servers\n * COMPLETED: Discovery finished (with or without errors)\n\n\nCommon Issues and Solutions#\n\nServer Won't Connect#\n\nSymptoms: Server shows DISCONNECTED status\n\nTroubleshooting:\n\n 1. Check configuration: Verify command, args, and cwd are correct\n 2. Test manually: Run the server command directly to ensure it works\n 3. Check dependencies: Ensure all required packages are installed\n 4. Review logs: Look for error messages in the CLI output\n 5. Verify permissions: Ensure the CLI can execute the server command\n\nNo Tools Discovered#\n\nSymptoms: Server connects but no tools are available\n\nTroubleshooting:\n\n 1. Verify tool registration: Ensure your server actually registers tools\n 2. Check MCP protocol: Confirm your server implements the MCP tool listing\n    correctly\n 3. Review server logs: Check stderr output for server-side errors\n 4. Test tool listing: Manually test your server's tool discovery endpoint\n\nTools Not Executing#\n\nSymptoms: Tools are discovered but fail during execution\n\nTroubleshooting:\n\n 1. Parameter validation: Ensure your tool accepts the expected parameters\n 2. Schema compatibility: Verify your input schemas are valid JSON Schema\n 3. Error handling: Check if your tool is throwing unhandled exceptions\n 4. Timeout issues: Consider increasing the timeout setting\n\nSandbox Compatibility#\n\nSymptoms: MCP servers fail when sandboxing is enabled\n\nSolutions:\n\n 1. Docker-based servers: Use Docker containers that include all dependencies\n 2. Path accessibility: Ensure server executables are available in the sandbox\n 3. Network access: Configure sandbox to allow necessary network connections\n 4. Environment variables: Verify required environment variables are passed\n    through\n\n\nDebugging Tips#\n\n 1. Enable debug mode: Run the CLI with --debug for verbose output\n 2. Check stderr: MCP server stderr is captured and logged (INFO messages\n    filtered)\n 3. Test isolation: Test your MCP server independently before integrating\n 4. Incremental setup: Start with simple tools before adding complex\n    functionality\n 5. Use /mcp frequently: Monitor server status during development\n\n\nImportant Notes#\n\n\nSecurity Considerations#\n\n * Trust settings: The trust option bypasses all confirmation dialogs. Use\n   cautiously and only for servers you completely control\n * Access tokens: Be security-aware when configuring environment variables\n   containing API keys or tokens\n * Sandbox compatibility: When using sandboxing, ensure MCP servers are\n   available within the sandbox environment\n * Private data: Using broadly scoped personal access tokens can lead to\n   information leakage between repositories\n\n\nPerformance and Resource Management#\n\n * Connection persistence: The CLI maintains persistent connections to servers\n   that successfully register tools\n * Automatic cleanup: Connections to servers providing no tools are\n   automatically closed\n * Timeout management: Configure appropriate timeouts based on your server's\n   response characteristics\n * Resource monitoring: MCP servers run as separate processes and consume system\n   resources\n\n\nSchema Compatibility#\n\n * Property stripping: The system automatically removes certain schema\n   properties ($schema, additionalProperties) for Gemini API compatibility\n * Name sanitization: Tool names are automatically sanitized to meet API\n   requirements\n * Conflict resolution: Tool name conflicts between servers are resolved through\n   automatic prefixing\n\nThis comprehensive integration makes MCP servers a powerful way to extend the\nVeCLI's capabilities while maintaining security, reliability, and ease of use.\n\n\nReturning Rich Content from Tools#\n\nMCP tools are not limited to returning simple text. You can return rich,\nmulti-part content, including text, images, audio, and other binary data in a\nsingle tool response. This allows you to build powerful tools that can provide\ndiverse information to the model in a single turn.\n\nAll data returned from the tool is processed and sent to the model as context\nfor its next generation, enabling it to reason about or summarize the provided\ninformation.\n\n\nHow It Works#\n\nTo return rich content, your tool's response must adhere to the MCP\nspecification for a CallToolResult. The content field of the result should be an\narray of ContentBlock objects. The VeCLI will correctly process this array,\nseparating text from binary data and packaging it for the model.\n\nYou can mix and match different content block types in the content array. The\nsupported block types include:\n\n * text\n * image\n * audio\n * resource (embedded content)\n * resource_link\n\n\nExample: Returning Text and an Image#\n\nHere is an example of a valid JSON response from an MCP tool that returns both a\ntext description and an image:\n\n\n\nWhen the VeCLI receives this response, it will:\n\n 1. Extract all the text and combine it into a single functionResponse part for\n    the model.\n 2. Present the image data as a separate inlineData part.\n 3. Provide a clean, user-friendly summary in the CLI, indicating that both text\n    and an image were received.\n\nThis enables you to build sophisticated tools that can provide rich, multi-modal\ncontext to the Gemini model.\n\n\nMCP Prompts as Slash Commands#\n\nIn addition to tools, MCP servers can expose predefined prompts that can be\nexecuted as slash commands within the VeCLI. This allows you to create shortcuts\nfor common or complex queries that can be easily invoked by name.\n\n\nDefining Prompts on the Server#\n\nHere's a small example of a stdio MCP server that defines prompts:\n\n\n\nThis can be included in settings.json under mcpServers with:\n\n\n\n\nInvoking Prompts#\n\nOnce a prompt is discovered, you can invoke it using its name as a slash\ncommand. The CLI will automatically handle parsing arguments.\n\n\n\nor, using positional arguments:\n\n\n\nWhen you run this command, the VeCLI executes the prompts/get method on the MCP\nserver with the provided arguments. The server is responsible for substituting\nthe arguments into the prompt template and returning the final prompt text. The\nCLI then sends this prompt to the model for execution. This provides a\nconvenient way to automate and share common workflows.\n\n\nManaging MCP Servers with gemini mcp#\n\nWhile you can always configure MCP servers by manually editing your\nsettings.json file, the VeCLI provides a convenient set of commands to manage\nyour server configurations programmatically. These commands streamline the\nprocess of adding, listing, and removing MCP servers without needing to directly\nedit JSON files.\n\n\nAdding a Server (gemini mcp add)#\n\nThe add command configures a new MCP server in your settings.json. Based on the\nscope (-s, --scope), it will be added to either the user config\n~/.ve/settings.json or the project config .ve/settings.json file.\n\nCommand:\n\n\n\n * <name>: A unique name for the server.\n * <commandOrUrl>: The command to execute (for stdio) or the URL (for http/sse).\n * [args...]: Optional arguments for a stdio command.\n\nOptions (Flags):\n\n * -s, --scope: Configuration scope (user or project). [default: \"project\"]\n * -t, --transport: Transport type (stdio, sse, http). [default: \"stdio\"]\n * -e, --env: Set environment variables (e.g. -e KEY=value).\n * -H, --header: Set HTTP headers for SSE and HTTP transports (e.g. -H\n   \"X-Api-Key: abc123\" -H \"Authorization: Bearer abc123\").\n * --timeout: Set connection timeout in milliseconds.\n * --trust: Trust the server (bypass all tool call confirmation prompts).\n * --description: Set the description for the server.\n * --include-tools: A comma-separated list of tools to include.\n * --exclude-tools: A comma-separated list of tools to exclude.\n\nAdding an stdio server#\n\nThis is the default transport for running local servers.\n\n\n\nAdding an HTTP server#\n\nThis transport is for servers that use the streamable HTTP transport.\n\n\n\nAdding an SSE server#\n\nThis transport is for servers that use Server-Sent Events (SSE).\n\n\n\n\nListing Servers (gemini mcp list)#\n\nTo view all MCP servers currently configured, use the list command. It displays\neach server's name, configuration details, and connection status.\n\nCommand:\n\n\n\nExample Output:\n\n\n\n\nRemoving a Server (gemini mcp remove)#\n\nTo delete a server from your configuration, use the remove command with the\nserver's name.\n\nCommand:\n\n\n\nExample:\n\n\n\nThis will find and delete the \"my-server\" entry from the mcpServers object in\nthe appropriate settings.json file based on the scope (-s, --scope).","routePath":"/en/tools/mcp-server","lang":"en","toc":[{"text":"What is an MCP server?","id":"what-is-an-mcp-server","depth":2,"charIndex":113},{"text":"Core Integration Architecture","id":"core-integration-architecture","depth":2,"charIndex":991},{"text":"Discovery Layer (`mcp-client.ts`)","id":"discovery-layer-mcp-clientts","depth":3,"charIndex":-1},{"text":"Execution Layer (`mcp-tool.ts`)","id":"execution-layer-mcp-toolts","depth":3,"charIndex":-1},{"text":"Transport Mechanisms","id":"transport-mechanisms","depth":3,"charIndex":2082},{"text":"How to set up your MCP server","id":"how-to-set-up-your-mcp-server","depth":2,"charIndex":2355},{"text":"Configure the MCP server in settings.json","id":"configure-the-mcp-server-in-settingsjson","depth":3,"charIndex":2580},{"text":"Global MCP Settings (`mcp`)","id":"global-mcp-settings-mcp","depth":4,"charIndex":-1},{"text":"Server-Specific Configuration (`mcpServers`)","id":"server-specific-configuration-mcpservers","depth":4,"charIndex":-1},{"text":"Configuration Structure","id":"configuration-structure","depth":3,"charIndex":3533},{"text":"Configuration Properties","id":"configuration-properties","depth":3,"charIndex":3616},{"text":"Required (one of the following)","id":"required-one-of-the-following","depth":4,"charIndex":3705},{"text":"Optional","id":"optional","depth":4,"charIndex":3923},{"text":"OAuth Support for Remote MCP Servers","id":"oauth-support-for-remote-mcp-servers","depth":3,"charIndex":5007},{"text":"Automatic OAuth Discovery","id":"automatic-oauth-discovery","depth":4,"charIndex":5215},{"text":"Authentication Flow","id":"authentication-flow","depth":4,"charIndex":5615},{"text":"Browser Redirect Requirements","id":"browser-redirect-requirements","depth":4,"charIndex":6024},{"text":"Managing OAuth Authentication","id":"managing-oauth-authentication","depth":4,"charIndex":6411},{"text":"OAuth Configuration Properties","id":"oauth-configuration-properties","depth":4,"charIndex":6504},{"text":"Token Management","id":"token-management","depth":4,"charIndex":7187},{"text":"Authentication Provider Type","id":"authentication-provider-type","depth":4,"charIndex":7432},{"text":"Example Configurations","id":"example-configurations","depth":3,"charIndex":7969},{"text":"Python MCP Server (Stdio)","id":"python-mcp-server-stdio","depth":4,"charIndex":7994},{"text":"Node.js MCP Server (Stdio)","id":"nodejs-mcp-server-stdio","depth":4,"charIndex":8024},{"text":"Docker-based MCP Server","id":"docker-based-mcp-server","depth":4,"charIndex":8055},{"text":"HTTP-based MCP Server","id":"http-based-mcp-server","depth":4,"charIndex":8083},{"text":"HTTP-based MCP Server with Custom Headers","id":"http-based-mcp-server-with-custom-headers","depth":4,"charIndex":8109},{"text":"MCP Server with Tool Filtering","id":"mcp-server-with-tool-filtering","depth":4,"charIndex":8155},{"text":"Discovery Process Deep Dive","id":"discovery-process-deep-dive","depth":2,"charIndex":8191},{"text":"1. Server Iteration and Connection","id":"1-server-iteration-and-connection","depth":3,"charIndex":8319},{"text":"2. Tool Discovery","id":"2-tool-discovery","depth":3,"charIndex":8835},{"text":"3. Conflict Resolution","id":"3-conflict-resolution","depth":3,"charIndex":9397},{"text":"4. Schema Processing","id":"4-schema-processing","depth":3,"charIndex":9769},{"text":"5. Connection Management","id":"5-connection-management","depth":3,"charIndex":10075},{"text":"Tool Execution Flow","id":"tool-execution-flow","depth":2,"charIndex":10377},{"text":"1. Tool Invocation","id":"1-tool-invocation","depth":3,"charIndex":10488},{"text":"2. Confirmation Process","id":"2-confirmation-process","depth":3,"charIndex":10673},{"text":"Trust-based Bypass","id":"trust-based-bypass","depth":4,"charIndex":10768},{"text":"Dynamic Allow-listing","id":"dynamic-allow-listing","depth":4,"charIndex":10791},{"text":"User Choice Handling","id":"user-choice-handling","depth":4,"charIndex":11000},{"text":"3. Execution","id":"3-execution","depth":3,"charIndex":11258},{"text":"4. Response Handling","id":"4-response-handling","depth":3,"charIndex":11558},{"text":"How to interact with your MCP server","id":"how-to-interact-with-your-mcp-server","depth":2,"charIndex":11774},{"text":"Using the `/mcp` Command","id":"using-the-mcp-command","depth":3,"charIndex":-1},{"text":"Example `/mcp` Output","id":"example-mcp-output","depth":3,"charIndex":-1},{"text":"Tool Usage","id":"tool-usage","depth":3,"charIndex":12260},{"text":"Status Monitoring and Troubleshooting","id":"status-monitoring-and-troubleshooting","depth":2,"charIndex":12591},{"text":"Connection States","id":"connection-states","depth":3,"charIndex":12632},{"text":"Server Status (`MCPServerStatus`)","id":"server-status-mcpserverstatus","depth":4,"charIndex":-1},{"text":"Discovery State (`MCPDiscoveryState`)","id":"discovery-state-mcpdiscoverystate","depth":4,"charIndex":-1},{"text":"Common Issues and Solutions","id":"common-issues-and-solutions","depth":3,"charIndex":13059},{"text":"Server Won't Connect","id":"server-wont-connect","depth":4,"charIndex":13089},{"text":"No Tools Discovered","id":"no-tools-discovered","depth":4,"charIndex":13508},{"text":"Tools Not Executing","id":"tools-not-executing","depth":4,"charIndex":13909},{"text":"Sandbox Compatibility","id":"sandbox-compatibility","depth":4,"charIndex":14289},{"text":"Debugging Tips","id":"debugging-tips","depth":3,"charIndex":14704},{"text":"Important Notes","id":"important-notes","depth":2,"charIndex":15105},{"text":"Security Considerations","id":"security-considerations","depth":3,"charIndex":15124},{"text":"Performance and Resource Management","id":"performance-and-resource-management","depth":3,"charIndex":15626},{"text":"Schema Compatibility","id":"schema-compatibility","depth":3,"charIndex":16072},{"text":"Returning Rich Content from Tools","id":"returning-rich-content-from-tools","depth":2,"charIndex":16594},{"text":"How It Works","id":"how-it-works","depth":3,"charIndex":17084},{"text":"Example: Returning Text and an Image","id":"example-returning-text-and-an-image","depth":3,"charIndex":17576},{"text":"MCP Prompts as Slash Commands","id":"mcp-prompts-as-slash-commands","depth":2,"charIndex":18158},{"text":"Defining Prompts on the Server","id":"defining-prompts-on-the-server","depth":3,"charIndex":18415},{"text":"Invoking Prompts","id":"invoking-prompts","depth":3,"charIndex":18583},{"text":"Managing MCP Servers with `gemini mcp`","id":"managing-mcp-servers-with-gemini-mcp","depth":2,"charIndex":-1},{"text":"Adding a Server (`gemini mcp add`)","id":"adding-a-server-gemini-mcp-add","depth":3,"charIndex":-1},{"text":"Adding an stdio server","id":"adding-an-stdio-server","depth":4,"charIndex":20607},{"text":"Adding an HTTP server","id":"adding-an-http-server","depth":4,"charIndex":20692},{"text":"Adding an SSE server","id":"adding-an-sse-server","depth":4,"charIndex":20789},{"text":"Listing Servers (`gemini mcp list`)","id":"listing-servers-gemini-mcp-list","depth":3,"charIndex":-1},{"text":"Removing a Server (`gemini mcp remove`)","id":"removing-a-server-gemini-mcp-remove","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":32,"title":"Memory Tool (`save_memory`)","content":"Memory Tool (save_memory)#\n\nThis document describes the save_memory tool for the VeCLI.\n\n\nDescription#\n\nUse save_memory to save and recall information across your VeCLI sessions. With\nsave_memory, you can direct the CLI to remember key details across sessions,\nproviding personalized and directed assistance.\n\n\nArguments#\n\nsave_memory takes one argument:\n\n * fact (string, required): The specific fact or piece of information to\n   remember. This should be a clear, self-contained statement written in natural\n   language.\n\n\nHow to use save_memory with the VeCLI#\n\nThe tool appends the provided fact to a special VE.md file located in the user's\nhome directory (~/.ve/VE.md). This file can be configured to have a different\nname.\n\nOnce added, the facts are stored under a ## vecli Added Memories section. This\nfile is loaded as context in subsequent sessions, allowing the CLI to recall the\nsaved information.\n\nUsage:\n\n\n\n\nsave_memory examples#\n\nRemember a user preference:\n\n\n\nStore a project-specific detail:\n\n\n\n\nImportant notes#\n\n * General usage: This tool should be used for concise, important facts. It is\n   not intended for storing large amounts of data or conversational history.\n * Memory file: The memory file is a plain text Markdown file, so you can view\n   and edit it manually if needed.","routePath":"/en/tools/memory","lang":"en","toc":[{"text":"Description","id":"description","depth":2,"charIndex":89},{"text":"Arguments","id":"arguments","depth":3,"charIndex":310},{"text":"How to use `save_memory` with the VeCLI","id":"how-to-use-save_memory-with-the-vecli","depth":2,"charIndex":-1},{"text":"`save_memory` examples","id":"save_memory-examples","depth":3,"charIndex":-1},{"text":"Important notes","id":"important-notes","depth":2,"charIndex":1012}],"domain":"","frontmatter":{},"version":""},{"id":33,"title":"Multi File Read Tool (`read_many_files`)","content":"Multi File Read Tool (read_many_files)#\n\nThis document describes the read_many_files tool for the VeCLI.\n\n\nDescription#\n\nUse read_many_files to read content from multiple files specified by paths or\nglob patterns. The behavior of this tool depends on the provided files:\n\n * For text files, this tool concatenates their content into a single string.\n * For image (e.g., PNG, JPEG), PDF, audio (MP3, WAV), and video (MP4, MOV)\n   files, it reads and returns them as base64-encoded data, provided they are\n   explicitly requested by name or extension.\n\nread_many_files can be used to perform tasks such as getting an overview of a\ncodebase, finding where specific functionality is implemented, reviewing\ndocumentation, or gathering context from multiple configuration files.\n\nNote: read_many_files looks for files following the provided paths or glob\npatterns. A directory path such as \"/docs\" will return an empty result; the tool\nrequires a pattern such as \"/docs/*\" or \"/docs/*.md\" to identify the relevant\nfiles.\n\n\nArguments#\n\nread_many_files takes the following arguments:\n\n * paths (list[string], required): An array of glob patterns or paths relative\n   to the tool's target directory (e.g., [\"src/**/*.ts\"], [\"README.md\",\n   \"docs/*\", \"assets/logo.png\"]).\n * exclude (list[string], optional): Glob patterns for files/directories to\n   exclude (e.g., [\"**/*.log\", \"temp/\"]). These are added to default excludes if\n   useDefaultExcludes is true.\n * include (list[string], optional): Additional glob patterns to include. These\n   are merged with paths (e.g., [\"*.test.ts\"] to specifically add test files if\n   they were broadly excluded, or [\"images/*.jpg\"] to include specific image\n   types).\n * recursive (boolean, optional): Whether to search recursively. This is\n   primarily controlled by ** in glob patterns. Defaults to true.\n * useDefaultExcludes (boolean, optional): Whether to apply a list of default\n   exclusion patterns (e.g., node_modules, .git, non image/pdf binary files).\n   Defaults to true.\n * respect_git_ignore (boolean, optional): Whether to respect .gitignore\n   patterns when finding files. Defaults to true.\n\n\nHow to use read_many_files with the VeCLI#\n\nread_many_files searches for files matching the provided paths and include\npatterns, while respecting exclude patterns and default excludes (if enabled).\n\n * For text files: it reads the content of each matched file (attempting to skip\n   binary files not explicitly requested as image/PDF) and concatenates it into\n   a single string, with a separator --- {filePath} --- between the content of\n   each file. Uses UTF-8 encoding by default.\n * The tool inserts a --- End of content --- after the last file.\n * For image and PDF files: if explicitly requested by name or extension (e.g.,\n   paths: [\"logo.png\"] or include: [\"*.pdf\"]), the tool reads the file and\n   returns its content as a base64 encoded string.\n * The tool attempts to detect and skip other binary files (those not matching\n   common image/PDF types or not explicitly requested) by checking for null\n   bytes in their initial content.\n\nUsage:\n\n\n\n\nread_many_files examples#\n\nRead all TypeScript files in the src directory:\n\n\n\nRead the main README, all Markdown files in the docs directory, and a specific\nlogo image, excluding a specific file:\n\n\n\nRead all JavaScript files but explicitly include test files and all JPEGs in an\nimages folder:\n\n\n\n\nImportant notes#\n\n * Binary file handling:\n   * Image/PDF/Audio/Video files: The tool can read common image types (PNG,\n     JPEG, etc.), PDF, audio (mp3, wav), and video (mp4, mov) files, returning\n     them as base64 encoded data. These files must be explicitly targeted by the\n     paths or include patterns (e.g., by specifying the exact filename like\n     video.mp4 or a pattern like *.mov).\n   * Other binary files: The tool attempts to detect and skip other types of\n     binary files by examining their initial content for null bytes. The tool\n     excludes these files from its output.\n * Performance: Reading a very large number of files or very large individual\n   files can be resource-intensive.\n * Path specificity: Ensure paths and glob patterns are correctly specified\n   relative to the tool's target directory. For image/PDF files, ensure the\n   patterns are specific enough to include them.\n * Default excludes: Be aware of the default exclusion patterns (like\n   node_modules, .git) and use useDefaultExcludes=False if you need to override\n   them, but do so cautiously.","routePath":"/en/tools/multi-file","lang":"en","toc":[{"text":"Description","id":"description","depth":2,"charIndex":106},{"text":"Arguments","id":"arguments","depth":3,"charIndex":1016},{"text":"How to use `read_many_files` with the VeCLI","id":"how-to-use-read_many_files-with-the-vecli","depth":2,"charIndex":-1},{"text":"`read_many_files` examples","id":"read_many_files-examples","depth":2,"charIndex":-1},{"text":"Important notes","id":"important-notes","depth":2,"charIndex":3395}],"domain":"","frontmatter":{},"version":""},{"id":34,"title":"Shell Tool (`run_shell_command`)","content":"Shell Tool (run_shell_command)#\n\nThis document describes the run_shell_command tool for the VeCLI.\n\n\nDescription#\n\nUse run_shell_command to interact with the underlying system, run scripts, or\nperform command-line operations. run_shell_command executes a given shell\ncommand. On Windows, the command will be executed with cmd.exe /c. On other\nplatforms, the command will be executed with bash -c.\n\n\nArguments#\n\nrun_shell_command takes the following arguments:\n\n * command (string, required): The exact shell command to execute.\n * description (string, optional): A brief description of the command's purpose,\n   which will be shown to the user.\n * directory (string, optional): The directory (relative to the project root) in\n   which to execute the command. If not provided, the command runs in the\n   project root.\n\n\nHow to use run_shell_command with the VeCLI#\n\nWhen using run_shell_command, the command is executed as a subprocess.\nrun_shell_command can start background processes using &. The tool returns\ndetailed information about the execution, including:\n\n * Command: The command that was executed.\n * Directory: The directory where the command was run.\n * Stdout: Output from the standard output stream.\n * Stderr: Output from the standard error stream.\n * Error: Any error message reported by the subprocess.\n * Exit Code: The exit code of the command.\n * Signal: The signal number if the command was terminated by a signal.\n * Background PIDs: A list of PIDs for any background processes started.\n\nUsage:\n\n\n\n\nrun_shell_command examples#\n\nList files in the current directory:\n\n\n\nRun a script in a specific directory:\n\n\n\nStart a background server:\n\n\n\n\nImportant notes#\n\n * Security: Be cautious when executing commands, especially those constructed\n   from user input, to prevent security vulnerabilities.\n * Interactive commands: Avoid commands that require interactive user input, as\n   this can cause the tool to hang. Use non-interactive flags if available\n   (e.g., npm init -y).\n * Error handling: Check the Stderr, Error, and Exit Code fields to determine if\n   a command executed successfully.\n * Background processes: When a command is run in the background with &, the\n   tool will return immediately and the process will continue to run in the\n   background. The Background PIDs field will contain the process ID of the\n   background process.\n\n\nEnvironment Variables#\n\nWhen run_shell_command executes a command, it sets the VE_CLI=1 environment\nvariable in the subprocess's environment. This allows scripts or tools to detect\nif they are being run from within the VeCLI.\n\n\nCommand Restrictions#\n\nYou can restrict the commands that can be executed by the run_shell_command tool\nby using the tools.core and tools.exclude settings in your configuration file.\n\n * tools.core: To restrict run_shell_command to a specific set of commands, add\n   entries to the core list under the tools category in the format\n   run_shell_command(<command>). For example, \"tools\": {\"core\":\n   [\"run_shell_command(git)\"]} will only allow git commands. Including the\n   generic run_shell_command acts as a wildcard, allowing any command not\n   explicitly blocked.\n * tools.exclude: To block specific commands, add entries to the exclude list\n   under the tools category in the format run_shell_command(<command>). For\n   example, \"tools\": {\"exclude\": [\"run_shell_command(rm)\"]} will block rm\n   commands.\n\nThe validation logic is designed to be secure and flexible:\n\n 1. Command Chaining Disabled: The tool automatically splits commands chained\n    with &&, ||, or ; and validates each part separately. If any part of the\n    chain is disallowed, the entire command is blocked.\n 2. Prefix Matching: The tool uses prefix matching. For example, if you allow\n    git, you can run git status or git log.\n 3. Blocklist Precedence: The tools.exclude list is always checked first. If a\n    command matches a blocked prefix, it will be denied, even if it also matches\n    an allowed prefix in tools.core.\n\n\nCommand Restriction Examples#\n\nAllow only specific command prefixes\n\nTo allow only git and npm commands, and block all others:\n\n\n\n * git status: Allowed\n * npm install: Allowed\n * ls -l: Blocked\n\nBlock specific command prefixes\n\nTo block rm and allow all other commands:\n\n\n\n * rm -rf /: Blocked\n * git status: Allowed\n * npm install: Allowed\n\nBlocklist takes precedence\n\nIf a command prefix is in both tools.core and tools.exclude, it will be blocked.\n\n\n\n * git push origin main: Blocked\n * git status: Allowed\n\nBlock all shell commands\n\nTo block all shell commands, add the run_shell_command wildcard to\ntools.exclude:\n\n\n\n * ls -l: Blocked\n * any other command: Blocked\n\n\nSecurity Note for excludeTools#\n\nCommand-specific restrictions in excludeTools for run_shell_command are based on\nsimple string matching and can be easily bypassed. This feature is not a\nsecurity mechanism and should not be relied upon to safely execute untrusted\ncode. It is recommended to use coreTools to explicitly select commands that can\nbe executed.","routePath":"/en/tools/shell","lang":"en","toc":[{"text":"Description","id":"description","depth":2,"charIndex":100},{"text":"Arguments","id":"arguments","depth":3,"charIndex":398},{"text":"How to use `run_shell_command` with the VeCLI","id":"how-to-use-run_shell_command-with-the-vecli","depth":2,"charIndex":-1},{"text":"`run_shell_command` examples","id":"run_shell_command-examples","depth":2,"charIndex":-1},{"text":"Important notes","id":"important-notes","depth":2,"charIndex":1661},{"text":"Environment Variables","id":"environment-variables","depth":2,"charIndex":2365},{"text":"Command Restrictions","id":"command-restrictions","depth":2,"charIndex":2593},{"text":"Command Restriction Examples","id":"command-restriction-examples","depth":3,"charIndex":3995},{"text":"Security Note for `excludeTools`","id":"security-note-for-excludetools","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":""},{"id":35,"title":"Web Fetch Tool (`web_fetch`)","content":"Web Fetch Tool (web_fetch)#\n\nThis document describes the web_fetch tool for the VeCLI.\n\n\nDescription#\n\nUse web_fetch to summarize, compare, or extract information from web pages. The\nweb_fetch tool processes content from one or more URLs (up to 20) embedded in a\nprompt. web_fetch takes a natural language prompt and returns a generated\nresponse.\n\n\nArguments#\n\nweb_fetch takes one argument:\n\n * prompt (string, required): A comprehensive prompt that includes the URL(s)\n   (up to 20) to fetch and specific instructions on how to process their\n   content. For example: \"Summarize https://example.com/article and extract key\n   points from https://another.com/data\". The prompt must contain at least one\n   URL starting with http:// or https://.\n\n\nHow to use web_fetch with the VeCLI#\n\nTo use web_fetch with the VeCLI, provide a natural language prompt that contains\nURLs. The tool will ask for confirmation before fetching any URLs. Once\nconfirmed, the tool will process URLs through Gemini API's urlContext.\n\nIf the Gemini API cannot access the URL, the tool will fall back to fetching\ncontent directly from the local machine. The tool will format the response,\nincluding source attribution and citations where possible. The tool will then\nprovide the response to the user.\n\nUsage:\n\n\n\n\nweb_fetch examples#\n\nSummarize a single article:\n\n\n\nCompare two articles:\n\n\n\n\nImportant notes#\n\n * URL processing: web_fetch relies on the Gemini API's ability to access and\n   process the given URLs.\n * Output quality: The quality of the output will depend on the clarity of the\n   instructions in the prompt.","routePath":"/en/tools/web-fetch","lang":"en","toc":[{"text":"Description","id":"description","depth":2,"charIndex":88},{"text":"Arguments","id":"arguments","depth":3,"charIndex":348},{"text":"How to use `web_fetch` with the VeCLI","id":"how-to-use-web_fetch-with-the-vecli","depth":2,"charIndex":-1},{"text":"`web_fetch` examples","id":"web_fetch-examples","depth":2,"charIndex":-1},{"text":"Important notes","id":"important-notes","depth":2,"charIndex":1363}],"domain":"","frontmatter":{},"version":""},{"id":36,"title":"VeCLI: Terms of Service and Privacy Notice","content":"#\n\nVeCLI is an open-source tool that lets you interact with Volcengine's powerful\nlanguage models directly from your command-line interface. The Terms of Service\nand Privacy Notices that apply to your usage of the VeCLI depend on the type of\naccount you use to authenticate with Volcengine.\n\nThis article outlines the specific terms and privacy policies applicable for\ndifferent account types and authentication methods. Note: See quotas and pricing\nfor the quota and pricing details that apply to your usage of the VeCLI.\n\n\nHow to determine your authentication method#\n\nYour authentication method refers to the method you use to log into and access\nthe VeCLI. There are four ways to authenticate:\n\n * Logging in with your Volcengine account to Ve Code Assist for Individuals\n * Logging in with your Volcengine account to Ve Code Assist for Standard, or\n   Enterprise Users\n * Using an API key with vecli Developer\n * Using an API key with Vertex AI GenAI API\n\nFor each of these four methods of authentication, different Terms of Service and\nPrivacy Notices may apply.\n\nAUTHENTICATION                  ACCOUNT               TERMS OF SERVICE                               PRIVACY NOTICE\nVe Code Assist via Volcengine   Individual            Volcengine Terms of Service                    Ve Code Assist Privacy Notice for Individuals\nVe Code Assist via Volcengine   Standard/Enterprise   Volcano Engine Platform Terms of Service       Ve Code Assist Privacy Notice for Standard and Enterprise\nvecli Developer API             Unpaid                vecli API Terms of Service - Unpaid Services   Volcengine Privacy Policy\nvecli Developer API             Paid                  vecli API Terms of Service - Paid Services     Volcengine Privacy Policy\nVertex AI Gen API                                     Volcano Engine Platform Service Terms          Volcano Engine Privacy Notice\n\n\n1. If you have logged in with your Volcengine account to Ve Code Assist for\nIndividuals#\n\nFor users who use their Volcengine account to access Ve Code Assist for\nIndividuals, these Terms of Service and Privacy Notice documents apply:\n\n * Terms of Service: Your use of the VeCLI is governed by the Volcengine Terms\n   of Service.\n * Privacy Notice: The collection and use of your data is described in the Ve\n   Code Assist Privacy Notice for Individuals.\n\n\n2. If you have logged in with your Volcengine account to Ve Code Assist for\nStandard, or Enterprise Users#\n\nFor users who use their Volcengine account to access the Standard or Enterprise\nedition of Ve Code Assist, these Terms of Service and Privacy Notice documents\napply:\n\n * Terms of Service: Your use of the VeCLI is governed by the Volcano Engine\n   Platform Terms of Service.\n * Privacy Notice: The collection and use of your data is described in the Ve\n   Code Assist Privacy Notices for Standard and Enterprise Users.\n\n\n3. If you have logged in with a vecli API key to the vecli Developer API#\n\nIf you are using a vecli API key for authentication with the vecli Developer\nAPI, these Terms of Service and Privacy Notice documents apply:\n\n * Terms of Service: Your use of the VeCLI is governed by the vecli API Terms of\n   Service. These terms may differ depending on whether you are using an unpaid\n   or paid service:\n   * For unpaid services, refer to the vecli API Terms of Service - Unpaid\n     Services.\n   * For paid services, refer to the vecli API Terms of Service - Paid Services.\n * Privacy Notice: The collection and use of your data is described in the\n   Volcengine Privacy Policy.\n\n\n4. If you have logged in with a vecli API key to the Vertex AI GenAI API#\n\nIf you are using a vecli API key for authentication with a Vertex AI GenAI API\nbackend, these Terms of Service and Privacy Notice documents apply:\n\n * Terms of Service: Your use of the VeCLI is governed by the Volcano Engine\n   Platform Service Terms.\n * Privacy Notice: The collection and use of your data is described in the\n   Volcano Engine Privacy Notice.\n\n\nUsage Statistics Opt-Out#\n\nYou may opt-out from sending Usage Statistics to Volcengine by following the\ninstructions available here: Usage Statistics Configuration.\n\n\nFrequently Asked Questions (FAQ) for the VeCLI#\n\n\n1. Is my code, including prompts and answers, used to train Volcengine's\nmodels?#\n\nWhether your code, including prompts and answers, is used to train Volcengine's\nmodels depends on the type of authentication method you use and your account\ntype.\n\nBy default (if you have not opted out):\n\n * Volcengine account with Ve Code Assist for Individuals: Yes. When you use\n   your personal Volcengine account, the Ve Code Assist Privacy Notice for\n   Individuals applies. Under this notice, your prompts, answers, and related\n   code are collected and may be used to improve Volcengine's products,\n   including for model training.\n * Volcengine account with Ve Code Assist for Standard, or Enterprise: No. For\n   these accounts, your data is governed by the Ve Code Assist Privacy Notices\n   terms, which treat your inputs as confidential. Your prompts, answers, and\n   related code are not collected and are not used to train models.\n * vecli API key via the vecli Developer API: Whether your code is collected or\n   used depends on whether you are using an unpaid or paid service.\n   * Unpaid services: Yes. When you use the vecli API key via the vecli\n     Developer API with an unpaid service, the vecli API Terms of Service -\n     Unpaid Services terms apply. Under this notice, your prompts, answers, and\n     related code are collected and may be used to improve Volcengine's\n     products, including for model training.\n   * Paid services: No. When you use the vecli API key via the vecli Developer\n     API with a paid service, the vecli API Terms of Service - Paid Services\n     terms apply, which treats your inputs as confidential. Your prompts,\n     answers, and related code are not collected and are not used to train\n     models.\n * vecli API key via the Vertex AI GenAI API: No. For these accounts, your data\n   is governed by the Volcano Engine Privacy Notice terms, which treat your\n   inputs as confidential. Your prompts, answers, and related code are not\n   collected and are not used to train models.\n\nFor more information about opting out, refer to the next question.\n\n\n2. What are Usage Statistics and what does the opt-out control?#\n\nThe Usage Statistics setting is the single control for all optional data\ncollection in the VeCLI.\n\nThe data it collects depends on your account and authentication type:\n\n * Volcengine account with Ve Code Assist for Individuals: When enabled, this\n   setting allows Volcengine to collect both anonymous telemetry (for example,\n   commands run and performance metrics) and your prompts and answers, including\n   code, for model improvement.\n * Volcengine account with Ve Code Assist for Standard, or Enterprise: This\n   setting only controls the collection of anonymous telemetry. Your prompts and\n   answers, including code, are never collected, regardless of this setting.\n * vecli API key via the vecli Developer API: Unpaid services: When enabled,\n   this setting allows Volcengine to collect both anonymous telemetry (like\n   commands run and performance metrics) and your prompts and answers, including\n   code, for model improvement. When disabled we will use your data as described\n   in How Volcengine Uses Your Data. Paid services: This setting only controls\n   the collection of anonymous telemetry. Volcengine logs prompts and responses\n   for a limited period of time, solely for the purpose of detecting violations\n   of the Prohibited Use Policy and any required legal or regulatory\n   disclosures.\n * vecli API key via the Vertex AI GenAI API: This setting only controls the\n   collection of anonymous telemetry. Your prompts and answers, including code,\n   are never collected, regardless of this setting.\n\nPlease refer to the Privacy Notice that applies to your authentication method\nfor more information about what data is collected and how this data is used.\n\nYou can disable Usage Statistics for any account type by following the\ninstructions in the Usage Statistics Configuration documentation.","routePath":"/en/tos-privacy","lang":"en","toc":[{"text":"How to determine your authentication method","id":"how-to-determine-your-authentication-method","depth":2,"charIndex":524},{"text":"1. If you have logged in with your Volcengine account to Ve Code Assist for Individuals","id":"1-if-you-have-logged-in-with-your-volcengine-account-to-ve-code-assist-for-individuals","depth":2,"charIndex":-1},{"text":"2. If you have logged in with your Volcengine account to Ve Code Assist for Standard, or Enterprise Users","id":"2-if-you-have-logged-in-with-your-volcengine-account-to-ve-code-assist-for-standard-or-enterprise-users","depth":2,"charIndex":-1},{"text":"3. If you have logged in with a vecli API key to the vecli Developer API","id":"3-if-you-have-logged-in-with-a-vecli-api-key-to-the-vecli-developer-api","depth":2,"charIndex":2862},{"text":"4. If you have logged in with a vecli API key to the Vertex AI GenAI API","id":"4-if-you-have-logged-in-with-a-vecli-api-key-to-the-vertex-ai-genai-api","depth":2,"charIndex":3538},{"text":"Usage Statistics Opt-Out","id":"usage-statistics-opt-out","depth":3,"charIndex":3976},{"text":"Frequently Asked Questions (FAQ) for the VeCLI","id":"frequently-asked-questions-faq-for-the-vecli","depth":2,"charIndex":4143},{"text":"1. Is my code, including prompts and answers, used to train Volcengine's models?","id":"1-is-my-code-including-prompts-and-answers-used-to-train-volcengines-models","depth":3,"charIndex":-1},{"text":"2. What are Usage Statistics and what does the opt-out control?","id":"2-what-are-usage-statistics-and-what-does-the-opt-out-control","depth":3,"charIndex":6279}],"domain":"","frontmatter":{},"version":""},{"id":37,"title":"Troubleshooting guide","content":"#\n\nThis guide provides solutions to common issues and debugging tips, including\ntopics on:\n\n * Authentication or login errors\n * Frequently asked questions (FAQs)\n * Debugging tips\n * Existing GitHub Issues similar to yours or creating new Issues\n\n\nAuthentication or login errors#\n\n * Error: UNABLE_TO_GET_ISSUER_CERT_LOCALLY or unable to get local issuer\n   certificate\n   * Cause: You may be on a corporate network with a firewall that intercepts\n     and inspects SSL/TLS traffic. This often requires a custom root CA\n     certificate to be trusted by Node.js.\n   * Solution: Set the NODE_EXTRA_CA_CERTS environment variable to the absolute\n     path of your corporate root CA certificate file.\n     * Example: export NODE_EXTRA_CA_CERTS=/path/to/your/corporate-ca.crt\n\n\nFrequently asked questions (FAQs)#\n\n * Q: How do I update VeCLI to the latest version?\n   \n   * A: If you installed it globally via npm, update it using the command npm\n     install -g @vecli/vecli@latest. If you compiled it from source, pull the\n     latest changes from the repository, and then rebuild using the command npm\n     run build.\n\n * Q: Where are the VeCLI configuration or settings files stored?\n   \n   * A: The VeCLI configuration is stored in two settings.json files:\n     \n     1. In your home directory: ~/.ve/settings.json.\n     2. In your project's root directory: ./.ve/settings.json.\n     \n     Refer to VeCLI Configuration for more details.\n\n\nCommon error messages and solutions#\n\n * Error: EADDRINUSE (Address already in use) when starting an MCP server.\n   \n   * Cause: Another process is already using the port that the MCP server is\n     trying to bind to.\n   * Solution: Either stop the other process that is using the port or configure\n     the MCP server to use a different port.\n\n * Error: Command not found (when attempting to run VeCLI with vecli).\n   \n   * Cause: VeCLI is not correctly installed or it is not in your system's PATH.\n   * Solution: The update depends on how you installed VeCLI:\n     * If you installed vecli globally, check that your npm global binary\n       directory is in your PATH. You can update VeCLI using the command npm\n       install -g @vecli/vecli@latest.\n     * If you are running vecli from source, ensure you are using the correct\n       command to invoke it (e.g., node packages/cli/dist/index.js ...). To\n       update VeCLI, pull the latest changes from the repository, and then\n       rebuild using the command npm run build.\n\n * Error: MODULE_NOT_FOUND or import errors.\n   \n   * Cause: Dependencies are not installed correctly, or the project hasn't been\n     built.\n   * Solution:\n     1. Run npm install to ensure all dependencies are present.\n     2. Run npm run build to compile the project.\n     3. Verify that the build completed successfully with npm run start.\n\n * Error: \"Operation not permitted\", \"Permission denied\", or similar.\n   \n   * Cause: When sandboxing is enabled, VeCLI may attempt operations that are\n     restricted by your sandbox configuration, such as writing outside the\n     project directory or system temp directory.\n   * Solution: Refer to the Configuration: Sandboxing documentation for more\n     information, including how to customize your sandbox configuration.\n\n * VeCLI is not running in interactive mode in \"CI\" environments\n   \n   * Issue: The VeCLI does not enter interactive mode (no prompt appears) if an\n     environment variable starting with CI_ (e.g., CI_TOKEN) is set. This is\n     because the is-in-ci package, used by the underlying UI framework, detects\n     these variables and assumes a non-interactive CI environment.\n   * Cause: The is-in-ci package checks for the presence of CI,\n     CONTINUOUS_INTEGRATION, or any environment variable with a CI_ prefix. When\n     any of these are found, it signals that the environment is non-interactive,\n     which prevents the VeCLI from starting in its interactive mode.\n   * Solution: If the CI_ prefixed variable is not needed for the CLI to\n     function, you can temporarily unset it for the command. e.g., env -u\n     CI_TOKEN vecli\n\n * DEBUG mode not working from project .env file\n   \n   * Issue: Setting DEBUG=true in a project's .env file doesn't enable debug\n     mode for vecli.\n   * Cause: The DEBUG and DEBUG_MODE variables are automatically excluded from\n     project .env files to prevent interference with vecli behavior.\n   * Solution: Use a .ve/.env file instead, or configure the\n     advanced.excludedEnvVars setting in your settings.json to exclude fewer\n     variables.\n\n\nExit Codes#\n\nThe VeCLI uses specific exit codes to indicate the reason for termination. This\nis especially useful for scripting and automation.\n\nEXIT CODE   ERROR TYPE                 DESCRIPTION\n41          FatalAuthenticationError   An error occurred during the authentication process.\n42          FatalInputError            Invalid or missing input was provided to the CLI.\n                                       (non-interactive mode only)\n44          FatalSandboxError          An error occurred with the sandboxing environment (e.g.,\n                                       Docker, Podman, or Seatbelt).\n52          FatalConfigError           A configuration file (settings.json) is invalid or contains\n                                       errors.\n53          FatalTurnLimitedError      The maximum number of conversational turns for the session\n                                       was reached. (non-interactive mode only)\n\n\nDebugging Tips#\n\n * CLI debugging:\n   \n   * Use the --verbose flag (if available) with CLI commands for more detailed\n     output.\n   * Check the CLI logs, often found in a user-specific configuration or cache\n     directory.\n\n * Core debugging:\n   \n   * Check the server console output for error messages or stack traces.\n   * Increase log verbosity if configurable.\n   * Use Node.js debugging tools (e.g., node --inspect) if you need to step\n     through server-side code.\n\n * Tool issues:\n   \n   * If a specific tool is failing, try to isolate the issue by running the\n     simplest possible version of the command or operation the tool performs.\n   * For run_shell_command, check that the command works directly in your shell\n     first.\n   * For file system tools, verify that paths are correct and check the\n     permissions.\n\n * Pre-flight checks:\n   \n   * Always run npm run preflight before committing code. This can catch many\n     common issues related to formatting, linting, and type errors.\n\n\nExisting GitHub Issues similar to yours or creating new Issues#\n\nIf you encounter an issue that was not covered here in this Troubleshooting\nguide, consider searching the VeCLI Issue tracker on GitHub. If you can't find\nan issue similar to yours, consider creating a new GitHub Issue with a detailed\ndescription. Pull requests are also welcome!","routePath":"/en/troubleshooting","lang":"en","toc":[{"text":"Authentication or login errors","id":"authentication-or-login-errors","depth":2,"charIndex":248},{"text":"Frequently asked questions (FAQs)","id":"frequently-asked-questions-faqs","depth":2,"charIndex":773},{"text":"Common error messages and solutions","id":"common-error-messages-and-solutions","depth":2,"charIndex":1439},{"text":"Exit Codes","id":"exit-codes","depth":2,"charIndex":4533},{"text":"Debugging Tips","id":"debugging-tips","depth":2,"charIndex":5468},{"text":"Existing GitHub Issues similar to yours or creating new Issues","id":"existing-github-issues-similar-to-yours-or-creating-new-issues","depth":2,"charIndex":6475}],"domain":"","frontmatter":{},"version":""},{"id":38,"title":"Ignoring Files","content":"#\n\nThis document provides an overview of the Gemini Ignore (.veignore) feature of\nthe VeCLI.\n\nThe VeCLI includes the ability to automatically ignore files, similar to\n.gitignore (used by Git) and .aiexclude (used by Ve Code Assist). Adding paths\nto your .veignore file will exclude them from tools that support this feature,\nalthough they will still be visible to other services (such as Git).\n\n\nHow it works#\n\nWhen you add a path to your .veignore file, tools that respect this file will\nexclude matching files and directories from their operations. For example, when\nyou use the read_many_files command, any paths in your .veignore file will be\nautomatically excluded.\n\nFor the most part, .veignore follows the conventions of .gitignore files:\n\n * Blank lines and lines starting with # are ignored.\n * Standard glob patterns are supported (such as *, ?, and []).\n * Putting a / at the end will only match directories.\n * Putting a / at the beginning anchors the path relative to the .veignore file.\n * ! negates a pattern.\n\nYou can update your .veignore file at any time. To apply the changes, you must\nrestart your VeCLI session.\n\n\nHow to use .veignore#\n\nTo enable .veignore:\n\n 1. Create a file named .veignore in the root of your project directory.\n\nTo add a file or directory to .veignore:\n\n 1. Open your .veignore file.\n 2. Add the path or file you want to ignore, for example: /archive/ or\n    apikeys.txt.\n\n\n.veignore examples#\n\nYou can use .veignore to ignore directories and files:\n\n\n\nYou can use wildcards in your .veignore file with *:\n\n\n\nFinally, you can exclude files and directories from exclusion with !:\n\n\n\nTo remove paths from your .veignore file, delete the relevant lines.","routePath":"/en/ve-ignore","lang":"en","toc":[{"text":"How it works","id":"how-it-works","depth":2,"charIndex":395},{"text":"How to use `.veignore`","id":"how-to-use-veignore","depth":2,"charIndex":-1},{"text":"`.veignore` examples","id":"veignore-examples","depth":3,"charIndex":-1}],"domain":"","frontmatter":{},"version":""}]